{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-08 13:54:25.853161: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-08 13:54:26.223416: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746687266.359370   33644 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746687266.400935   33644 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1746687266.716609   33644 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746687266.716670   33644 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746687266.716672   33644 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746687266.716673   33644 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-08 13:54:26.754272: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "# os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "import wandb\n",
    "import time\n",
    "import os\n",
    "from transformers import PegasusTokenizer, PegasusForConditionalGeneration, PegasusConfig, get_cosine_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from rouge import Rouge\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 4\n",
    "FRAC_SAMPLE = 0.001\n",
    "MAX_LENGTH_ARTICLE = 512\n",
    "MIN_LENGTH_ARTICLE = 50\n",
    "MAX_LENGTH_SUMMARY = 128\n",
    "MIN_LENGTH_SUMMARY = 20\n",
    "HIDDEN_DIM = 128\n",
    "LEARNING_RATE = 1e-4\n",
    "PATIENCE = 5  # For early stopping\n",
    "WEIGHT_DECAY = 1e-4\n",
    "NUM_CYCLES = 5\n",
    "\n",
    "\n",
    "model_dir = \"../Model\"\n",
    "datafilter = \"../dataft\"\n",
    "save_dir = \"fine_tuned_pegasus_custom\"\n",
    "output_path = os.path.join(datafilter, \"test_pred_5.csv\")\n",
    "os.makedirs(datafilter, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 96 entries, 144417 to 16783\n",
      "Data columns (total 5 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   id                  96 non-null     object\n",
      " 1   articles            96 non-null     object\n",
      " 2   summaries           96 non-null     object\n",
      " 3   article_word_count  96 non-null     int64 \n",
      " 4   summary_word_count  96 non-null     int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 4.5+ KB\n",
      "\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 5 entries, 8901 to 9955\n",
      "Data columns (total 5 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   id                  5 non-null      object\n",
      " 1   articles            5 non-null      object\n",
      " 2   summaries           5 non-null      object\n",
      " 3   article_word_count  5 non-null      int64 \n",
      " 4   summary_word_count  5 non-null      int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 240.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv(\"../dataset/train.csv\")\n",
    "validation_data = pd.read_csv(\"../dataset/validation.csv\")\n",
    "test_data = pd.read_csv(\"../dataset/test.csv\")\n",
    "\n",
    "# add col\n",
    "train_data.rename(columns={\"highlights\": \"summaries\", \"article\":\"articles\"}, inplace=True)\n",
    "validation_data.rename(columns={\"highlights\": \"summaries\",\"article\":\"articles\"}, inplace=True)\n",
    "test_data.rename(columns={\"highlights\": \"summaries\", \"article\":\"articles\"}, inplace=True)\n",
    "\n",
    "train_data[\"article_word_count\"] = train_data[\"articles\"].astype(str).apply(lambda x: len(x.split()))\n",
    "train_data[\"summary_word_count\"] = train_data[\"summaries\"].astype(str).apply(lambda x: len(x.split()))\n",
    "\n",
    "validation_data[\"article_word_count\"] = validation_data[\"articles\"].astype(str).apply(lambda x: len(x.split()))\n",
    "validation_data[\"summary_word_count\"] = validation_data[\"summaries\"].astype(str).apply(lambda x: len(x.split()))\n",
    "\n",
    "test_data[\"article_word_count\"] = test_data[\"articles\"].astype(str).apply(lambda x: len(x.split()))\n",
    "test_data[\"summary_word_count\"] = test_data[\"summaries\"].astype(str).apply(lambda x: len(x.split()))\n",
    "\n",
    "# filter range\n",
    "train_data = train_data[\n",
    "    (train_data[\"article_word_count\"] <= MAX_LENGTH_ARTICLE) & \n",
    "    (train_data[\"article_word_count\"] >= MIN_LENGTH_ARTICLE) &\n",
    "    (train_data[\"summary_word_count\"] <= MAX_LENGTH_SUMMARY) &\n",
    "    (train_data[\"summary_word_count\"] >= MIN_LENGTH_SUMMARY)\n",
    "]\n",
    "\n",
    "validation_data = validation_data[\n",
    "    (validation_data[\"article_word_count\"] <= MAX_LENGTH_ARTICLE) & \n",
    "    (validation_data[\"article_word_count\"] >= MIN_LENGTH_ARTICLE) &\n",
    "    (validation_data[\"summary_word_count\"] <= MAX_LENGTH_SUMMARY) &\n",
    "    (validation_data[\"summary_word_count\"] >= MIN_LENGTH_SUMMARY)\n",
    "]\n",
    "test_data = test_data[\n",
    "    (test_data[\"article_word_count\"] <= MAX_LENGTH_ARTICLE) & \n",
    "    (test_data[\"article_word_count\"] >= MIN_LENGTH_ARTICLE) &\n",
    "    (test_data[\"summary_word_count\"] <= MAX_LENGTH_SUMMARY) &\n",
    "    (test_data[\"summary_word_count\"] >= MIN_LENGTH_SUMMARY)\n",
    "]\n",
    "\n",
    "train_sample = train_data.sample(frac=FRAC_SAMPLE, random_state=1)\n",
    "validation_sample = validation_data.sample(frac=FRAC_SAMPLE*100, random_state=1)\n",
    "test_sample = test_data.sample(frac=0.1, random_state=1)\n",
    "train_sample.info()\n",
    "print(\"\\n\")\n",
    "validation_sample.info()\n",
    "train_sample.to_csv(os.path.join(datafilter,\"train_sample.csv\"), index=False)\n",
    "test_sample.to_csv(os.path.join(datafilter,\"test_sample.csv\"), index=False)\n",
    "validation_sample.to_csv(os.path.join(datafilter,\"validation_sample.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 96 entries, 0 to 95\n",
      "Data columns (total 5 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   id                  96 non-null     object\n",
      " 1   articles            96 non-null     object\n",
      " 2   summaries           96 non-null     object\n",
      " 3   article_word_count  96 non-null     int64 \n",
      " 4   summary_word_count  96 non-null     int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 3.9+ KB\n"
     ]
    }
   ],
   "source": [
    "train_sample = pd.read_csv(\"../dataft/train_sample.csv\")\n",
    "validation_sample = pd.read_csv(\"../dataft/validation_sample.csv\")\n",
    "test_sample = pd.read_csv(\"../dataft/test_sample.csv\")\n",
    "train_sample.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummarizationDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_input_length=MAX_LENGTH_ARTICLE, max_output_length=MAX_LENGTH_SUMMARY):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_input_length = max_input_length\n",
    "        self.max_output_length = max_output_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        article = self.data.iloc[index][\"articles\"]\n",
    "        summary = self.data.iloc[index][\"summaries\"]\n",
    "        input_ids = self.tokenizer.encode(article, max_length=self.max_input_length, truncation=True, padding=\"max_length\")\n",
    "        output_ids = self.tokenizer.encode(summary, max_length=self.max_output_length, truncation=True, padding=\"max_length\")\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": [int(token_id != 0) for token_id in input_ids],\n",
    "            \"decoder_input_ids\": output_ids[:-1],\n",
    "            \"decoder_attention_mask\": [1] * (len(output_ids) - 1),\n",
    "            \"labels\": output_ids[1:]\n",
    "        }\n",
    "        \n",
    "train_df = train_sample\n",
    "test_df = test_sample\n",
    "val_df = validation_sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-large and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-large and are newly initialized because the shapes did not match:\n",
      "- model.shared.weight: found shape torch.Size([96103, 1024]) in the checkpoint and torch.Size([96103, 512]) in the model instantiated\n",
      "- model.encoder.embed_tokens.weight: found shape torch.Size([96103, 1024]) in the checkpoint and torch.Size([96103, 512]) in the model instantiated\n",
      "- model.encoder.layers.0.self_attn.k_proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- model.encoder.layers.0.self_attn.k_proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.encoder.layers.0.self_attn.v_proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- model.encoder.layers.0.self_attn.v_proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.encoder.layers.0.self_attn.q_proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- model.encoder.layers.0.self_attn.q_proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.encoder.layers.0.self_attn.out_proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- model.encoder.layers.0.self_attn.out_proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.encoder.layers.0.self_attn_layer_norm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.encoder.layers.0.self_attn_layer_norm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.encoder.layers.0.fc1.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 512]) in the model instantiated\n",
      "- model.encoder.layers.0.fc2.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([512, 4096]) in the model instantiated\n",
      "- model.encoder.layers.0.fc2.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.encoder.layers.0.final_layer_norm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.encoder.layers.0.final_layer_norm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.encoder.layers.1.self_attn.k_proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- model.encoder.layers.1.self_attn.k_proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.encoder.layers.1.self_attn.v_proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- model.encoder.layers.1.self_attn.v_proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.encoder.layers.1.self_attn.q_proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- model.encoder.layers.1.self_attn.q_proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.encoder.layers.1.self_attn.out_proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- model.encoder.layers.1.self_attn.out_proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.encoder.layers.1.self_attn_layer_norm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.encoder.layers.1.self_attn_layer_norm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.encoder.layers.1.fc1.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 512]) in the model instantiated\n",
      "- model.encoder.layers.1.fc2.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([512, 4096]) in the model instantiated\n",
      "- model.encoder.layers.1.fc2.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.encoder.layers.1.final_layer_norm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.encoder.layers.1.final_layer_norm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.encoder.layers.2.self_attn.k_proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- model.encoder.layers.2.self_attn.k_proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.encoder.layers.2.self_attn.v_proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- model.encoder.layers.2.self_attn.v_proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.encoder.layers.2.self_attn.q_proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- model.encoder.layers.2.self_attn.q_proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.encoder.layers.2.self_attn.out_proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- model.encoder.layers.2.self_attn.out_proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.encoder.layers.2.self_attn_layer_norm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.encoder.layers.2.self_attn_layer_norm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.encoder.layers.2.fc1.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 512]) in the model instantiated\n",
      "- model.encoder.layers.2.fc2.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([512, 4096]) in the model instantiated\n",
      "- model.encoder.layers.2.fc2.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.encoder.layers.2.final_layer_norm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.encoder.layers.2.final_layer_norm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.encoder.layers.3.self_attn.k_proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- model.encoder.layers.3.self_attn.k_proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.encoder.layers.3.self_attn.v_proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- model.encoder.layers.3.self_attn.v_proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.encoder.layers.3.self_attn.q_proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- model.encoder.layers.3.self_attn.q_proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.encoder.layers.3.self_attn.out_proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- model.encoder.layers.3.self_attn.out_proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.encoder.layers.3.self_attn_layer_norm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.encoder.layers.3.self_attn_layer_norm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.encoder.layers.3.fc1.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 512]) in the model instantiated\n",
      "- model.encoder.layers.3.fc2.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([512, 4096]) in the model instantiated\n",
      "- model.encoder.layers.3.fc2.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.encoder.layers.3.final_layer_norm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.encoder.layers.3.final_layer_norm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.encoder.layers.4.self_attn.k_proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- model.encoder.layers.4.self_attn.k_proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.encoder.layers.4.self_attn.v_proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- model.encoder.layers.4.self_attn.v_proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.encoder.layers.4.self_attn.q_proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- model.encoder.layers.4.self_attn.q_proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.encoder.layers.4.self_attn.out_proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- model.encoder.layers.4.self_attn.out_proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.encoder.layers.4.self_attn_layer_norm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.encoder.layers.4.self_attn_layer_norm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.encoder.layers.4.fc1.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 512]) in the model instantiated\n",
      "- model.encoder.layers.4.fc2.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([512, 4096]) in the model instantiated\n",
      "- model.encoder.layers.4.fc2.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.encoder.layers.4.final_layer_norm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.encoder.layers.4.final_layer_norm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.encoder.layers.5.self_attn.k_proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- model.encoder.layers.5.self_attn.k_proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.encoder.layers.5.self_attn.v_proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- model.encoder.layers.5.self_attn.v_proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.encoder.layers.5.self_attn.q_proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- model.encoder.layers.5.self_attn.q_proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.encoder.layers.5.self_attn.out_proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- model.encoder.layers.5.self_attn.out_proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.encoder.layers.5.self_attn_layer_norm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.encoder.layers.5.self_attn_layer_norm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.encoder.layers.5.fc1.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 512]) in the model instantiated\n",
      "- model.encoder.layers.5.fc2.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([512, 4096]) in the model instantiated\n",
      "- model.encoder.layers.5.fc2.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.encoder.layers.5.final_layer_norm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.encoder.layers.5.final_layer_norm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.encoder.layer_norm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.encoder.layer_norm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.embed_tokens.weight: found shape torch.Size([96103, 1024]) in the checkpoint and torch.Size([96103, 512]) in the model instantiated\n",
      "- model.decoder.layers.0.self_attn.k_proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- model.decoder.layers.0.self_attn.k_proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.0.self_attn.v_proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- model.decoder.layers.0.self_attn.v_proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.0.self_attn.q_proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- model.decoder.layers.0.self_attn.q_proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.0.self_attn.out_proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- model.decoder.layers.0.self_attn.out_proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.0.self_attn_layer_norm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.0.self_attn_layer_norm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.0.encoder_attn.k_proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- model.decoder.layers.0.encoder_attn.k_proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.0.encoder_attn.v_proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- model.decoder.layers.0.encoder_attn.v_proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.0.encoder_attn.q_proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- model.decoder.layers.0.encoder_attn.q_proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.0.encoder_attn.out_proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- model.decoder.layers.0.encoder_attn.out_proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.0.encoder_attn_layer_norm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.0.encoder_attn_layer_norm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.0.fc1.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 512]) in the model instantiated\n",
      "- model.decoder.layers.0.fc2.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([512, 4096]) in the model instantiated\n",
      "- model.decoder.layers.0.fc2.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.0.final_layer_norm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.0.final_layer_norm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.1.self_attn.k_proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- model.decoder.layers.1.self_attn.k_proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.1.self_attn.v_proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- model.decoder.layers.1.self_attn.v_proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.1.self_attn.q_proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- model.decoder.layers.1.self_attn.q_proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.1.self_attn.out_proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- model.decoder.layers.1.self_attn.out_proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.1.self_attn_layer_norm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.1.self_attn_layer_norm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.1.encoder_attn.k_proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- model.decoder.layers.1.encoder_attn.k_proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.1.encoder_attn.v_proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- model.decoder.layers.1.encoder_attn.v_proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.1.encoder_attn.q_proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- model.decoder.layers.1.encoder_attn.q_proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.1.encoder_attn.out_proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- model.decoder.layers.1.encoder_attn.out_proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.1.encoder_attn_layer_norm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.1.encoder_attn_layer_norm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.1.fc1.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 512]) in the model instantiated\n",
      "- model.decoder.layers.1.fc2.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([512, 4096]) in the model instantiated\n",
      "- model.decoder.layers.1.fc2.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.1.final_layer_norm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.1.final_layer_norm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.2.self_attn.k_proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- model.decoder.layers.2.self_attn.k_proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.2.self_attn.v_proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- model.decoder.layers.2.self_attn.v_proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.2.self_attn.q_proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- model.decoder.layers.2.self_attn.q_proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.2.self_attn.out_proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- model.decoder.layers.2.self_attn.out_proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.2.self_attn_layer_norm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.2.self_attn_layer_norm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.2.encoder_attn.k_proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- model.decoder.layers.2.encoder_attn.k_proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.2.encoder_attn.v_proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- model.decoder.layers.2.encoder_attn.v_proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.2.encoder_attn.q_proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- model.decoder.layers.2.encoder_attn.q_proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.2.encoder_attn.out_proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- model.decoder.layers.2.encoder_attn.out_proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.2.encoder_attn_layer_norm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.2.encoder_attn_layer_norm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.2.fc1.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 512]) in the model instantiated\n",
      "- model.decoder.layers.2.fc2.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([512, 4096]) in the model instantiated\n",
      "- model.decoder.layers.2.fc2.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.2.final_layer_norm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.2.final_layer_norm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.3.self_attn.k_proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- model.decoder.layers.3.self_attn.k_proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.3.self_attn.v_proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- model.decoder.layers.3.self_attn.v_proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.3.self_attn.q_proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- model.decoder.layers.3.self_attn.q_proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.3.self_attn.out_proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- model.decoder.layers.3.self_attn.out_proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.3.self_attn_layer_norm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.3.self_attn_layer_norm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.3.encoder_attn.k_proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- model.decoder.layers.3.encoder_attn.k_proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.3.encoder_attn.v_proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- model.decoder.layers.3.encoder_attn.v_proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.3.encoder_attn.q_proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- model.decoder.layers.3.encoder_attn.q_proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.3.encoder_attn.out_proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- model.decoder.layers.3.encoder_attn.out_proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.3.encoder_attn_layer_norm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.3.encoder_attn_layer_norm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.3.fc1.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 512]) in the model instantiated\n",
      "- model.decoder.layers.3.fc2.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([512, 4096]) in the model instantiated\n",
      "- model.decoder.layers.3.fc2.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.3.final_layer_norm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.3.final_layer_norm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.4.self_attn.k_proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- model.decoder.layers.4.self_attn.k_proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.4.self_attn.v_proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- model.decoder.layers.4.self_attn.v_proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.4.self_attn.q_proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- model.decoder.layers.4.self_attn.q_proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.4.self_attn.out_proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- model.decoder.layers.4.self_attn.out_proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.4.self_attn_layer_norm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.4.self_attn_layer_norm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.4.encoder_attn.k_proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- model.decoder.layers.4.encoder_attn.k_proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.4.encoder_attn.v_proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- model.decoder.layers.4.encoder_attn.v_proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.4.encoder_attn.q_proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- model.decoder.layers.4.encoder_attn.q_proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.4.encoder_attn.out_proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- model.decoder.layers.4.encoder_attn.out_proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.4.encoder_attn_layer_norm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.4.encoder_attn_layer_norm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.4.fc1.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 512]) in the model instantiated\n",
      "- model.decoder.layers.4.fc2.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([512, 4096]) in the model instantiated\n",
      "- model.decoder.layers.4.fc2.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.4.final_layer_norm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.4.final_layer_norm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.5.self_attn.k_proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- model.decoder.layers.5.self_attn.k_proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.5.self_attn.v_proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- model.decoder.layers.5.self_attn.v_proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.5.self_attn.q_proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- model.decoder.layers.5.self_attn.q_proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.5.self_attn.out_proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- model.decoder.layers.5.self_attn.out_proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.5.self_attn_layer_norm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.5.self_attn_layer_norm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.5.encoder_attn.k_proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- model.decoder.layers.5.encoder_attn.k_proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.5.encoder_attn.v_proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- model.decoder.layers.5.encoder_attn.v_proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.5.encoder_attn.q_proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- model.decoder.layers.5.encoder_attn.q_proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.5.encoder_attn.out_proj.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- model.decoder.layers.5.encoder_attn.out_proj.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.5.encoder_attn_layer_norm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.5.encoder_attn_layer_norm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.5.fc1.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 512]) in the model instantiated\n",
      "- model.decoder.layers.5.fc2.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([512, 4096]) in the model instantiated\n",
      "- model.decoder.layers.5.fc2.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.5.final_layer_norm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layers.5.final_layer_norm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layer_norm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- model.decoder.layer_norm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initialize model and tokenizer\n",
    "tokenizer = PegasusTokenizer.from_pretrained(\"google/pegasus-large\")\n",
    "config = PegasusConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    d_model=512,  # Increased from 256\n",
    "    encoder_layers=6,  # Increased from 4\n",
    "    decoder_layers=6,  # Increased from 4\n",
    "    encoder_attention_heads=8,\n",
    "    decoder_attention_heads=8,\n",
    "    dropout=0.1,\n",
    "    max_position_embeddings=MAX_LENGTH_ARTICLE,\n",
    "    scale_embedding=True\n",
    ")\n",
    "model = PegasusForConditionalGeneration.from_pretrained(\"google/pegasus-large\", config=config).to(device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SummarizationDataset(train_df, tokenizer)\n",
    "val_dataset = SummarizationDataset(val_df, tokenizer)\n",
    "test_dataset = SummarizationDataset(test_df, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    input_ids = [item[\"input_ids\"] for item in batch]\n",
    "    attention_mask = [item[\"attention_mask\"] for item in batch]\n",
    "    decoder_input_ids = [item[\"decoder_input_ids\"] for item in batch]\n",
    "    decoder_attention_mask = [item[\"decoder_attention_mask\"] for item in batch]\n",
    "    labels = [item[\"labels\"] for item in batch]\n",
    "    max_input_length = max(len(ids) for ids in input_ids)\n",
    "    max_output_length = max(len(ids) for ids in decoder_input_ids)\n",
    "    input_ids = [ids + [0] * (max_input_length - len(ids)) for ids in input_ids]\n",
    "    attention_mask = [mask + [0] * (max_input_length - len(mask)) for mask in attention_mask]\n",
    "    decoder_input_ids = [ids + [0] * (max_output_length - len(ids)) for ids in decoder_input_ids]\n",
    "    decoder_attention_mask = [mask + [0] * (max_output_length - len(mask)) for mask in decoder_attention_mask]\n",
    "    labels = [ids + [-100] * (max_output_length - len(ids)) for ids in labels]\n",
    "    return {\n",
    "        \"input_ids\": torch.tensor(input_ids),\n",
    "        \"attention_mask\": torch.tensor(attention_mask),\n",
    "        \"decoder_input_ids\": torch.tensor(decoder_input_ids),\n",
    "        \"decoder_attention_mask\": torch.tensor(decoder_attention_mask),\n",
    "        \"labels\": torch.tensor(labels)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_decay = ['bias', 'LayerNorm.weight']    # cần ít regularization hơn weight\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        'weight_decay': WEIGHT_DECAY,\n",
    "    },\n",
    "    {\n",
    "        'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        'weight_decay': 0.0,\n",
    "    },\n",
    "]\n",
    "# optimizer = AdamW(optimizer_grouped_parameters, lr=LEARNING_RATE)\n",
    "\n",
    "# Scheduler\n",
    "num_training_steps = len(train_loader) * NUM_EPOCHS\n",
    "# scheduler = get_cosine_schedule_with_warmup(\n",
    "#     optimizer,\n",
    "#     num_warmup_steps=int(0.2 * num_training_steps),\n",
    "#     num_training_steps=num_training_steps,\n",
    "#     num_cycles=NUM_CYCLES\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add custom scheduler class (place this at the top of your script)\n",
    "class CosineWarmupWithBounds(_LRScheduler):\n",
    "    def __init__(self, optimizer, num_warmup_steps, num_training_steps, num_cycles=0.5, max_lr=1e-4, min_lr=1e-5, last_epoch=-1):\n",
    "        self.num_warmup_steps = num_warmup_steps\n",
    "        self.num_training_steps = num_training_steps\n",
    "        self.num_cycles = num_cycles\n",
    "        self.max_lr = max_lr\n",
    "        self.min_lr = min_lr\n",
    "        super(CosineWarmupWithBounds, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        step = self.last_epoch + 1\n",
    "        if step < self.num_warmup_steps:\n",
    "            lr = self.min_lr + (self.max_lr - self.min_lr) * step / self.num_warmup_steps\n",
    "        else:\n",
    "            progress = (step - self.num_warmup_steps) / (self.num_training_steps - self.num_warmup_steps)\n",
    "            cosine_factor = 0.5 * (1.0 + math.cos(math.pi * progress * self.num_cycles))\n",
    "            lr = self.min_lr + (self.max_lr - self.min_lr) * cosine_factor\n",
    "        lr = max(self.min_lr, min(self.max_lr, lr))\n",
    "        return [lr for _ in self.optimizer.param_groups]\n",
    "\n",
    "# Update optimizer and scheduler\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=1e-4)\n",
    "scheduler = CosineWarmupWithBounds(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(0.2 * num_training_steps),\n",
    "    num_training_steps=num_training_steps,\n",
    "    num_cycles=NUM_CYCLES,\n",
    "    max_lr=1e-4,\n",
    "    min_lr=1e-5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvubkk67\u001b[0m (\u001b[33mvubkk67-hanoi-university-of-science-and-technology\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/vuda/Text-Summarization/Abstractive/wandb/run-20250508_135512-u5l6qj3t</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vubkk67-hanoi-university-of-science-and-technology/Finetune-Summarization/runs/u5l6qj3t' target=\"_blank\">pegasus-custom-20250508-135511</a></strong> to <a href='https://wandb.ai/vubkk67-hanoi-university-of-science-and-technology/Finetune-Summarization' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vubkk67-hanoi-university-of-science-and-technology/Finetune-Summarization' target=\"_blank\">https://wandb.ai/vubkk67-hanoi-university-of-science-and-technology/Finetune-Summarization</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vubkk67-hanoi-university-of-science-and-technology/Finetune-Summarization/runs/u5l6qj3t' target=\"_blank\">https://wandb.ai/vubkk67-hanoi-university-of-science-and-technology/Finetune-Summarization/runs/u5l6qj3t</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/vubkk67-hanoi-university-of-science-and-technology/Finetune-Summarization/runs/u5l6qj3t?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fd9d09fe840>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Weights & Biases initialization\n",
    "wandb.init(\n",
    "    project=\"Finetune-Summarization\",\n",
    "    name=f\"pegasus-custom-{datetime.now().strftime('%Y%m%d-%H%M%S')}\",\n",
    "    config={\n",
    "        \"model\": \"Pegasus_custom\",\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"weight_decay\": WEIGHT_DECAY,\n",
    "        \"num_epochs\": NUM_EPOCHS,\n",
    "        \"num_cycles\": NUM_CYCLES,\n",
    "        \"data_ratio\": FRAC_SAMPLE,\n",
    "        \"warm_up\": \"Cosine\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e263eb8cb594eaeb002d6715e07f65a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1 [Train]:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6389fc3eae114c59a8f60adc82d48115",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1 [Val]:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved best model to `fine_tuned_pegasus_custom` at epoch 1\n",
      "Epoch 01 | Train Loss: 6.3165 | Val Loss: 8.2625 | LR: 0.000057 | Time: 9.17s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7887b48dd12f4052bd44adac0d8e4e23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2 [Train]:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "102f1ac03479401096f4d5099d5a8128",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2 [Val]:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved best model to `fine_tuned_pegasus_custom` at epoch 2\n",
      "Epoch 02 | Train Loss: 4.8005 | Val Loss: 7.1994 | LR: 0.000100 | Time: 8.22s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1d98f729f044ed98a069f58d8a61fe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3 [Train]:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eb424c6d5204ec5b689ab0d377a4c7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3 [Val]:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved best model to `fine_tuned_pegasus_custom` at epoch 3\n",
      "Epoch 03 | Train Loss: 3.9359 | Val Loss: 6.4831 | LR: 0.000034 | Time: 8.29s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b530924456af4a6aa845751a1fec594d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4 [Train]:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8de6532f9f144bd8a8e0ab12495b011d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4 [Val]:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved best model to `fine_tuned_pegasus_custom` at epoch 4\n",
      "Epoch 04 | Train Loss: 3.6124 | Val Loss: 6.3459 | LR: 0.000026 | Time: 8.39s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43bd605958bf4cdf848182313ed83387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5 [Train]:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d46fa6b20ba4494b14d53883193fbbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5 [Val]:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved best model to `fine_tuned_pegasus_custom` at epoch 5\n",
      "Epoch 05 | Train Loss: 3.4280 | Val Loss: 6.0042 | LR: 0.000098 | Time: 8.46s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca76dc9377d14dec8c3fe5d3cd353174",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6 [Train]:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5129748ff2c4714969cbd4b58c3f4a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6 [Val]:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved best model to `fine_tuned_pegasus_custom` at epoch 6\n",
      "Epoch 06 | Train Loss: 3.0785 | Val Loss: 5.7593 | LR: 0.000051 | Time: 8.25s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "626e67f35744429e9823f5fd75307335",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7 [Train]:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b0b93a8d24047fa90dcd2f7452999ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7 [Val]:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved best model to `fine_tuned_pegasus_custom` at epoch 7\n",
      "Epoch 07 | Train Loss: 2.9139 | Val Loss: 5.7260 | LR: 0.000015 | Time: 8.40s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02f9d941cf534f79b95aa8f9bf5ebfb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8 [Train]:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa1c40a3a5b74aa4b24341000b4593d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8 [Val]:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved best model to `fine_tuned_pegasus_custom` at epoch 8\n",
      "Epoch 08 | Train Loss: 2.8703 | Val Loss: 5.6894 | LR: 0.000089 | Time: 8.46s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c872369f17e9495abbdc39b78f745bdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9 [Train]:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "939ed3706d4a435bb9ce71b2c973e072",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9 [Val]:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved best model to `fine_tuned_pegasus_custom` at epoch 9\n",
      "Epoch 09 | Train Loss: 2.7737 | Val Loss: 5.6409 | LR: 0.000069 | Time: 8.39s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aac2794b33974e89ad4904bfb0b63f4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10 [Train]:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8e8e128d4f649c6bf168cd120b41051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10 [Val]:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved best model to `fine_tuned_pegasus_custom` at epoch 10\n",
      "Epoch 10 | Train Loss: 2.6835 | Val Loss: 5.6369 | LR: 0.000010 | Time: 8.28s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55b9285c2c714750af7eccb32e17fda2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.008 MB of 0.008 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_val_loss</td><td> █▅▃▃▂▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>lr</td><td>▅█▃▂█▄▁▇▆▁</td></tr><tr><td>train_loss</td><td>█▅▃▃▂▂▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▅▃▃▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_loss</td><td>5.64091</td></tr><tr><td>epoch</td><td>10</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>train_loss</td><td>2.68354</td></tr><tr><td>val_loss</td><td>5.63685</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">pegasus-custom-20250508-135511</strong> at: <a href='https://wandb.ai/vubkk67-hanoi-university-of-science-and-technology/Finetune-Summarization/runs/u5l6qj3t' target=\"_blank\">https://wandb.ai/vubkk67-hanoi-university-of-science-and-technology/Finetune-Summarization/runs/u5l6qj3t</a><br/> View project at: <a href='https://wandb.ai/vubkk67-hanoi-university-of-science-and-technology/Finetune-Summarization' target=\"_blank\">https://wandb.ai/vubkk67-hanoi-university-of-science-and-technology/Finetune-Summarization</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250508_135512-u5l6qj3t/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save best model and early stopping\n",
    "best_val_loss = float(\"inf\")\n",
    "epochs_no_improve = 0\n",
    "wandb.watch(model)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1} [Train]\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1} [Val]\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            val_loss += outputs.loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "\n",
    "    # W&B log\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"train_loss\": train_loss,\n",
    "        \"val_loss\": val_loss,\n",
    "        \"lr\": current_lr,\n",
    "        \"best_val_loss\": best_val_loss\n",
    "    })\n",
    "\n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        model.save_pretrained(save_dir)\n",
    "        tokenizer.save_pretrained(save_dir)\n",
    "        print(f\"Saved best model to `{save_dir}` at epoch {epoch+1}\")\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    # Early stopping\n",
    "    if epochs_no_improve >= PATIENCE:\n",
    "        print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch+1:02d} | \"\n",
    "        f\"Train Loss: {train_loss:.4f} | \"\n",
    "        f\"Val Loss: {val_loss:.4f} | \"\n",
    "        f\"LR: {current_lr:.6f} | \"\n",
    "        f\"Time: {time.time() - start_time:.2f}s\"\n",
    "    )\n",
    "\n",
    "# W&B end\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = PegasusTokenizer.from_pretrained(\"google/pegasus-large\")\n",
    "model = PegasusForConditionalGeneration.from_pretrained(save_dir).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ac484e3b5ac4b6488c4d22f1f124957",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating summaries:   0%|          | 0/106 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ File has been saved at: ../dataft/test_pred_5.csv\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for step, batch in enumerate(tqdm(test_loader, desc=\"Generating summaries\")):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        output_ids = model.generate(\n",
    "            input_ids=batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "            max_length=MAX_LENGTH_SUMMARY,\n",
    "            num_beams=4,\n",
    "            length_penalty=2.0,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        batch_preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "        predictions.extend(batch_preds)\n",
    "\n",
    "test_sample = test_df.iloc[:len(predictions)].copy()\n",
    "test_sample[\"predicted_summary\"] = predictions\n",
    "test_sample.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"✅ File has been saved at: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = pd.read_csv(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>articles</th>\n",
       "      <th>summaries</th>\n",
       "      <th>predicted_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Florida bus passenger was arrested for throw...</td>\n",
       "      <td>Joel Parker, 33, was riding the bus in St John...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aston Villa may be able to sign Cordoba strike...</td>\n",
       "      <td>Aston Villa have held talks over Cordoba strik...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A South Carolina mother of four died over the ...</td>\n",
       "      <td>Adam Leheup ran Fitness 535 in Columbia, South...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A playground in Tokyo has been found to have d...</td>\n",
       "      <td>Soil underneath a slide in the park showed ext...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A lonely shepherd has been found dead alongsid...</td>\n",
       "      <td>Body of Jose Alberto, 58, discovered at home i...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Four years after signing for Arsenal, Wellingt...</td>\n",
       "      <td>Wellington Silva signed for Arsenal in 2011 fo...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Joey Barton has urged QPR to use their win ove...</td>\n",
       "      <td>Queens Park Rangers strode to a 4-1 victory ag...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>An Indian woman who holds several awards for t...</td>\n",
       "      <td>Smita Srivastava currently holds record for lo...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Drilling threes on the buzzer is all good as f...</td>\n",
       "      <td>LeBron James posted an unhappy picture at the ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A teenager killed by police in Illinois on Sat...</td>\n",
       "      <td>Justus Howell, 17, was running from scene of a...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            articles  \\\n",
       "0  A Florida bus passenger was arrested for throw...   \n",
       "1  Aston Villa may be able to sign Cordoba strike...   \n",
       "2  A South Carolina mother of four died over the ...   \n",
       "3  A playground in Tokyo has been found to have d...   \n",
       "4  A lonely shepherd has been found dead alongsid...   \n",
       "5  Four years after signing for Arsenal, Wellingt...   \n",
       "6  Joey Barton has urged QPR to use their win ove...   \n",
       "7  An Indian woman who holds several awards for t...   \n",
       "8  Drilling threes on the buzzer is all good as f...   \n",
       "9  A teenager killed by police in Illinois on Sat...   \n",
       "\n",
       "                                           summaries  predicted_summary  \n",
       "0  Joel Parker, 33, was riding the bus in St John...                NaN  \n",
       "1  Aston Villa have held talks over Cordoba strik...                NaN  \n",
       "2  Adam Leheup ran Fitness 535 in Columbia, South...                NaN  \n",
       "3  Soil underneath a slide in the park showed ext...                NaN  \n",
       "4  Body of Jose Alberto, 58, discovered at home i...                NaN  \n",
       "5  Wellington Silva signed for Arsenal in 2011 fo...                NaN  \n",
       "6  Queens Park Rangers strode to a 4-1 victory ag...                NaN  \n",
       "7  Smita Srivastava currently holds record for lo...                NaN  \n",
       "8  LeBron James posted an unhappy picture at the ...                NaN  \n",
       "9  Justus Howell, 17, was running from scene of a...                NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(test_pred[[\"articles\",\"summaries\", \"predicted_summary\"]].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Hypothesis is empty.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummaries\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m test_pred\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m      3\u001b[0m     rouge \u001b[38;5;241m=\u001b[39m Rouge()\n\u001b[0;32m----> 4\u001b[0m     scores \u001b[38;5;241m=\u001b[39m \u001b[43mrouge\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_sample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msummaries\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mavg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mROUGE scores:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mROUGE-1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscores[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrouge-1\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/rouge/rouge.py:108\u001b[0m, in \u001b[0;36mRouge.get_scores\u001b[0;34m(self, hyps, refs, avg, ignore_empty)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m avg:\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_scores(hyps, refs)\n\u001b[0;32m--> 108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_avg_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhyps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/rouge/rouge.py:148\u001b[0m, in \u001b[0;36mRouge._get_avg_scores\u001b[0;34m(self, hyps, refs)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics:\n\u001b[1;32m    147\u001b[0m     fn \u001b[38;5;241m=\u001b[39m Rouge\u001b[38;5;241m.\u001b[39mAVAILABLE_METRICS[m]\n\u001b[0;32m--> 148\u001b[0m     sc \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclusive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexclusive\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m     scores[m] \u001b[38;5;241m=\u001b[39m {s: scores[m][s] \u001b[38;5;241m+\u001b[39m sc[s] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstats}\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_lengths:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/rouge/rouge.py:53\u001b[0m, in \u001b[0;36mRouge.<lambda>\u001b[0;34m(hyp, ref, **k)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mRouge\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m     DEFAULT_METRICS \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrouge-1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrouge-2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrouge-l\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     52\u001b[0m     AVAILABLE_METRICS \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m---> 53\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrouge-1\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m hyp, ref, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mk: \u001b[43mrouge_score\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrouge_n\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     54\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrouge-2\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m hyp, ref, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mk: rouge_score\u001b[38;5;241m.\u001b[39mrouge_n(hyp, ref, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mk),\n\u001b[1;32m     55\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrouge-3\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m hyp, ref, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mk: rouge_score\u001b[38;5;241m.\u001b[39mrouge_n(hyp, ref, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mk),\n\u001b[1;32m     56\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrouge-4\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m hyp, ref, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mk: rouge_score\u001b[38;5;241m.\u001b[39mrouge_n(hyp, ref, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mk),\n\u001b[1;32m     57\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrouge-5\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m hyp, ref, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mk: rouge_score\u001b[38;5;241m.\u001b[39mrouge_n(hyp, ref, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mk),\n\u001b[1;32m     58\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrouge-l\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m hyp, ref, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mk:\n\u001b[1;32m     59\u001b[0m             rouge_score\u001b[38;5;241m.\u001b[39mrouge_l_summary_level(hyp, ref, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mk),\n\u001b[1;32m     60\u001b[0m     }\n\u001b[1;32m     61\u001b[0m     DEFAULT_STATS \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     62\u001b[0m     AVAILABLE_STATS \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/rouge/rouge_score.py:253\u001b[0m, in \u001b[0;36mrouge_n\u001b[0;34m(evaluated_sentences, reference_sentences, n, raw_results, exclusive)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;124;03mComputes ROUGE-N of two text collections of sentences.\u001b[39;00m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;124;03mSourece: http://research.microsoft.com/en-us/um/people/cyl/download/\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;124;03m  ValueError: raises exception if a param has len <= 0\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(evaluated_sentences) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 253\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHypothesis is empty.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(reference_sentences) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReference is empty.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Hypothesis is empty."
     ]
    }
   ],
   "source": [
    "# Tính điểm ROUGE\n",
    "if \"summaries\" in test_pred.columns:\n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(predictions, test_sample[\"summaries\"].tolist(), avg=True)\n",
    "\n",
    "    print(\"ROUGE scores:\")\n",
    "    print(f\"ROUGE-1: {scores['rouge-1']['f']:.4f}\")\n",
    "    print(f\"ROUGE-2: {scores['rouge-2']['f']:.4f}\")\n",
    "    print(f\"ROUGE-L: {scores['rouge-l']['f']:.4f}\")\n",
    "else:\n",
    "    print(\"⚠️ Không tìm thấy cột 'summaries' để tính ROUGE.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
