{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted Averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-11 12:09:08.029441: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-11 12:09:08.176261: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746940148.294715   14567 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746940148.339106   14567 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1746940148.465545   14567 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746940148.465617   14567 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746940148.465619   14567 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746940148.465621   14567 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-11 12:09:08.495528: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, BartTokenizer, BartForConditionalGeneration, PegasusTokenizer, PegasusForConditionalGeneration\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "from rouge import Rouge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 50\n",
    "BATCH_SIZE = 8\n",
    "FRAC_SAMPLE = 0.01\n",
    "MAX_LENGTH_ARTICLE = 512\n",
    "MIN_LENGTH_ARTICLE = 50\n",
    "MAX_LENGTH_SUMMARY = 128\n",
    "MIN_LENGTH_SUMMARY = 20\n",
    "HIDDEN_DIM = 128\n",
    "LEARNING_RATE = 1e-5\n",
    "PATIENCE = 5  # For early stopping\n",
    "WEIGHT_DECAY = 1e-4\n",
    "NUM_CYCLES = 5\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "datafilter = \"../dataft\"\n",
    "output_path = os.path.join(datafilter, \"test_pred_ensemble.csv\")\n",
    "save_dir_bart = \"fine_tuned_bart_cosine_3\"\n",
    "save_dir_t5 = \"fine_tuned_t5_small\"\n",
    "save_dir_pegasus = \"fine_tuned_pegasus_custom\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 960 entries, 144417 to 108633\n",
      "Data columns (total 5 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   id                  960 non-null    object\n",
      " 1   articles            960 non-null    object\n",
      " 2   summaries           960 non-null    object\n",
      " 3   article_word_count  960 non-null    int64 \n",
      " 4   summary_word_count  960 non-null    int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 45.0+ KB\n",
      "\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 50 entries, 8901 to 12116\n",
      "Data columns (total 5 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   id                  50 non-null     object\n",
      " 1   articles            50 non-null     object\n",
      " 2   summaries           50 non-null     object\n",
      " 3   article_word_count  50 non-null     int64 \n",
      " 4   summary_word_count  50 non-null     int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 2.3+ KB\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv(\"../dataset/train.csv\")\n",
    "validation_data = pd.read_csv(\"../dataset/validation.csv\")\n",
    "test_data = pd.read_csv(\"../dataset/test.csv\")\n",
    "\n",
    "# add col\n",
    "train_data.rename(columns={\"highlights\": \"summaries\", \"article\":\"articles\"}, inplace=True)\n",
    "validation_data.rename(columns={\"highlights\": \"summaries\",\"article\":\"articles\"}, inplace=True)\n",
    "test_data.rename(columns={\"highlights\": \"summaries\", \"article\":\"articles\"}, inplace=True)\n",
    "\n",
    "train_data[\"article_word_count\"] = train_data[\"articles\"].astype(str).apply(lambda x: len(x.split()))\n",
    "train_data[\"summary_word_count\"] = train_data[\"summaries\"].astype(str).apply(lambda x: len(x.split()))\n",
    "\n",
    "validation_data[\"article_word_count\"] = validation_data[\"articles\"].astype(str).apply(lambda x: len(x.split()))\n",
    "validation_data[\"summary_word_count\"] = validation_data[\"summaries\"].astype(str).apply(lambda x: len(x.split()))\n",
    "\n",
    "test_data[\"article_word_count\"] = test_data[\"articles\"].astype(str).apply(lambda x: len(x.split()))\n",
    "test_data[\"summary_word_count\"] = test_data[\"summaries\"].astype(str).apply(lambda x: len(x.split()))\n",
    "\n",
    "# filter range\n",
    "train_data = train_data[\n",
    "    (train_data[\"article_word_count\"] <= MAX_LENGTH_ARTICLE) & \n",
    "    (train_data[\"article_word_count\"] >= MIN_LENGTH_ARTICLE) &\n",
    "    (train_data[\"summary_word_count\"] <= MAX_LENGTH_SUMMARY) &\n",
    "    (train_data[\"summary_word_count\"] >= MIN_LENGTH_SUMMARY)\n",
    "]\n",
    "\n",
    "validation_data = validation_data[\n",
    "    (validation_data[\"article_word_count\"] <= MAX_LENGTH_ARTICLE) & \n",
    "    (validation_data[\"article_word_count\"] >= MIN_LENGTH_ARTICLE) &\n",
    "    (validation_data[\"summary_word_count\"] <= MAX_LENGTH_SUMMARY) &\n",
    "    (validation_data[\"summary_word_count\"] >= MIN_LENGTH_SUMMARY)\n",
    "]\n",
    "test_data = test_data[\n",
    "    (test_data[\"article_word_count\"] <= MAX_LENGTH_ARTICLE) & \n",
    "    (test_data[\"article_word_count\"] >= MIN_LENGTH_ARTICLE) &\n",
    "    (test_data[\"summary_word_count\"] <= MAX_LENGTH_SUMMARY) &\n",
    "    (test_data[\"summary_word_count\"] >= MIN_LENGTH_SUMMARY)\n",
    "]\n",
    "\n",
    "train_sample = train_data.sample(frac=FRAC_SAMPLE, random_state=1)\n",
    "validation_sample = validation_data.sample(frac=FRAC_SAMPLE, random_state=1)\n",
    "test_sample = test_data.sample(frac=0.1, random_state=1)\n",
    "train_sample.info()\n",
    "print(\"\\n\")\n",
    "validation_sample.info()\n",
    "# train_sample.to_csv(os.path.join(datafilter,\"train_sample.csv\"), index=False)\n",
    "# test_sample.to_csv(os.path.join(datafilter,\"test_sample.csv\"), index=False)\n",
    "# validation_sample.to_csv(os.path.join(datafilter,\"validation_sample.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BartBaseDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_input_length=MAX_LENGTH_ARTICLE, max_output_length=MAX_LENGTH_SUMMARY):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_input_length = max_input_length\n",
    "        self.max_output_length = max_output_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        article = self.data.iloc[index][\"articles\"]\n",
    "        summary = self.data.iloc[index][\"summaries\"]\n",
    "        input_ids = self.tokenizer.encode(article, max_length=self.max_input_length, truncation=True, padding=\"max_length\")\n",
    "        output_ids = self.tokenizer.encode(summary, max_length=self.max_output_length, truncation=True, padding=\"max_length\")\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": [int(token_id != 0) for token_id in input_ids], \"decoder_input_ids\": output_ids[:-1], \"decoder_attention_mask\": [1] * (len(output_ids) - 1), \"labels\": output_ids[1:]}\n",
    "\n",
    "def collate_bart(batch):\n",
    "    input_ids = [item[\"input_ids\"] for item in batch]\n",
    "    attention_mask = [item[\"attention_mask\"] for item in batch]\n",
    "    decoder_input_ids = [item[\"decoder_input_ids\"] for item in batch]\n",
    "    decoder_attention_mask = [item[\"decoder_attention_mask\"] for item in batch]\n",
    "    labels = [item[\"labels\"] for item in batch]\n",
    "    max_input_length = max(len(ids) for ids in input_ids)\n",
    "    max_output_length = max(len(ids) for ids in decoder_input_ids)\n",
    "    input_ids = [ids + [0] * (max_input_length - len(ids)) for ids in input_ids]\n",
    "    attention_mask = [mask + [0] * (max_input_length - len(mask)) for mask in attention_mask]\n",
    "    decoder_input_ids = [ids + [0] * (max_output_length - len(ids)) for ids in decoder_input_ids]\n",
    "    decoder_attention_mask = [mask + [0] * (max_output_length - len(mask)) for mask in decoder_attention_mask]\n",
    "    labels = [ids + [-100] * (max_output_length - len(ids)) for ids in labels]\n",
    "    return {\"input_ids\": torch.tensor(input_ids), \"attention_mask\": torch.tensor(attention_mask), \"decoder_input_ids\": torch.tensor(decoder_input_ids), \"decoder_attention_mask\": torch.tensor(decoder_attention_mask), \"labels\": torch.tensor(labels)}\n",
    "\n",
    "bart_tokenizer = BartTokenizer.from_pretrained(save_dir_bart)\n",
    "bart_model = BartForConditionalGeneration.from_pretrained(save_dir_bart).to(device)\n",
    "\n",
    "test_dataset_bart = BartBaseDataset(test_df, bart_tokenizer)\n",
    "test_loader_bart = DataLoader(test_dataset_bart, batch_size=8, collate_fn=collate_bart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class T5SmallDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_input_length=MAX_LENGTH_ARTICLE, max_output_length=MAX_LENGTH_SUMMARY):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_input_length = max_input_length\n",
    "        self.max_output_length = max_output_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        article = self.data.iloc[index][\"articles\"]\n",
    "        summary = self.data.iloc[index][\"summaries\"]\n",
    "        \n",
    "        # T5 need prefix:\n",
    "        input_text = \"summarize: \" + article\n",
    "        inputs = self.tokenizer(\n",
    "            input_text,\n",
    "            max_length=self.max_input_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        outputs = self.tokenizer(\n",
    "            summary,\n",
    "            max_length=self.max_output_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": inputs.input_ids.squeeze(),\n",
    "            \"attention_mask\": inputs.attention_mask.squeeze(),\n",
    "            \"labels\": outputs.input_ids.squeeze()\n",
    "        }\n",
    "def collate_t5(batch):\n",
    "    input_ids = torch.stack([item[\"input_ids\"] for item in batch])\n",
    "    attention_mask = torch.stack([item[\"attention_mask\"] for item in batch])\n",
    "    labels = torch.stack([item[\"labels\"] for item in batch])\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "    \n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(save_dir_t5)\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained(save_dir_t5).to(device)\n",
    "\n",
    "test_dataset_t5 = T5SmallDataset(test_df, t5_tokenizer)\n",
    "test_loader_t5 = DataLoader(test_dataset_t5, batch_size=8, collate_fn=collate_t5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pegasus_tokenizer = PegasusTokenizer.from_pretrained(save_dir_pegasus)\n",
    "pegasus_model = PegasusForConditionalGeneration.from_pretrained(save_dir_pegasus).to(device)\n",
    "\n",
    "class PegaCustomDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_input_length=MAX_LENGTH_ARTICLE, max_output_length=MAX_LENGTH_SUMMARY):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_input_length = max_input_length\n",
    "        self.max_output_length = max_output_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        article = self.data.iloc[index][\"articles\"]\n",
    "        summary = self.data.iloc[index][\"summaries\"]\n",
    "        input_ids = self.tokenizer.encode(article, max_length=self.max_input_length, truncation=True, padding=\"max_length\")\n",
    "        output_ids = self.tokenizer.encode(summary, max_length=self.max_output_length, truncation=True, padding=\"max_length\")\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": [int(token_id != 0) for token_id in input_ids], \"decoder_input_ids\": output_ids[:-1], \"decoder_attention_mask\": [1] * (len(output_ids) - 1), \"labels\": output_ids[1:]}\n",
    "\n",
    "def collate_pega(batch):\n",
    "    input_ids = [item[\"input_ids\"] for item in batch]\n",
    "    attention_mask = [item[\"attention_mask\"] for item in batch]\n",
    "    decoder_input_ids = [item[\"decoder_input_ids\"] for item in batch]\n",
    "    decoder_attention_mask = [item[\"decoder_attention_mask\"] for item in batch]\n",
    "    labels = [item[\"labels\"] for item in batch]\n",
    "    max_input_length = max(len(ids) for ids in input_ids)\n",
    "    max_output_length = max(len(ids) for ids in decoder_input_ids)\n",
    "    input_ids = [ids + [0] * (max_input_length - len(ids)) for ids in input_ids]\n",
    "    attention_mask = [mask + [0] * (max_input_length - len(mask)) for mask in attention_mask]\n",
    "    decoder_input_ids = [ids + [0] * (max_output_length - len(ids)) for ids in decoder_input_ids]\n",
    "    decoder_attention_mask = [mask + [0] * (max_output_length - len(mask)) for mask in decoder_attention_mask]\n",
    "    labels = [ids + [-100] * (max_output_length - len(ids)) for ids in labels]\n",
    "    \n",
    "    labels = torch.tensor(labels)\n",
    "    labels[labels == pegasus_tokenizer.pad_token_id] = -100\n",
    "    \n",
    "    return {\"input_ids\": torch.tensor(input_ids), \"attention_mask\": torch.tensor(attention_mask), \"decoder_input_ids\": torch.tensor(decoder_input_ids), \"decoder_attention_mask\": torch.tensor(decoder_attention_mask), \"labels\": torch.tensor(labels)}\n",
    "\n",
    "test_dataset_pegasus = PegaCustomDataset(test_df, pegasus_tokenizer)\n",
    "test_loader_pegasus = DataLoader(test_dataset_pegasus, batch_size=8, collate_fn=collate_pega)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summaries(model, tokenizer, data_loader, model_type=\"bart\"):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=f\"Generating {model_type} summaries\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            if model_type == \"t5\":\n",
    "                output_ids = model.generate(\n",
    "                    input_ids=batch[\"input_ids\"],\n",
    "                    attention_mask=batch[\"attention_mask\"],\n",
    "                    max_length=MAX_LENGTH_SUMMARY,\n",
    "                    num_beams=4,\n",
    "                    length_penalty=2.0,\n",
    "                    early_stopping=True\n",
    "                )\n",
    "            else:  # BART hoặc Pegasus\n",
    "                output_ids = model.generate(\n",
    "                    input_ids=batch[\"input_ids\"],\n",
    "                    attention_mask=batch[\"attention_mask\"],\n",
    "                    max_length=MAX_LENGTH_SUMMARY,\n",
    "                    num_beams=4,\n",
    "                    length_penalty=2.0,\n",
    "                    early_stopping=True,\n",
    "                    decoder_start_token_id=tokenizer.pad_token_id\n",
    "                )\n",
    "\n",
    "            batch_preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "            predictions.extend(batch_preds)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating bart summaries: 100%|██████████| 53/53 [03:05<00:00,  3.49s/it]\n",
      "Generating t5 summaries: 100%|██████████| 53/53 [01:55<00:00,  2.18s/it]\n",
      "Generating pegasus summaries:   0%|          | 0/53 [00:00<?, ?it/s]/tmp/ipykernel_14567/2490203540.py:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {\"input_ids\": torch.tensor(input_ids), \"attention_mask\": torch.tensor(attention_mask), \"decoder_input_ids\": torch.tensor(decoder_input_ids), \"decoder_attention_mask\": torch.tensor(decoder_attention_mask), \"labels\": torch.tensor(labels)}\n",
      "Generating pegasus summaries: 100%|██████████| 53/53 [00:48<00:00,  1.09it/s]\n"
     ]
    }
   ],
   "source": [
    "bart_predictions = generate_summaries(bart_model, bart_tokenizer, test_loader_bart, \"bart\")\n",
    "t5_predictions = generate_summaries(t5_model, t5_tokenizer, test_loader_t5, \"t5\")\n",
    "pegasus_predictions = generate_summaries(pegasus_model, pegasus_tokenizer, test_loader_pegasus, \"pegasus\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lưu dự đoán vào DataFrame\n",
    "test_df[\"bart_summary\"] = bart_predictions\n",
    "test_df[\"t5_summary\"] = t5_predictions\n",
    "test_df[\"pegasus_summary\"] = pegasus_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>articles</th>\n",
       "      <th>summaries</th>\n",
       "      <th>article_word_count</th>\n",
       "      <th>summary_word_count</th>\n",
       "      <th>bart_summary</th>\n",
       "      <th>t5_summary</th>\n",
       "      <th>pegasus_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9204</th>\n",
       "      <td>fc8f37cb5bc8fe97794175fae6b876f07cf3fda4</td>\n",
       "      <td>A Florida bus passenger was arrested for throw...</td>\n",
       "      <td>Joel Parker, 33, was riding the bus in St John...</td>\n",
       "      <td>143</td>\n",
       "      <td>58</td>\n",
       "      <td>Joel Parker, 33, was arrested for throwing a ...</td>\n",
       "      <td>Joel Parker, 33, was about to get off the Suns...</td>\n",
       "      <td>the driver, 33, was arrested at St James Parke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10729</th>\n",
       "      <td>8d1da9b0197d9c733db56bdfa62332d04144398d</td>\n",
       "      <td>Aston Villa may be able to sign Cordoba strike...</td>\n",
       "      <td>Aston Villa have held talks over Cordoba strik...</td>\n",
       "      <td>189</td>\n",
       "      <td>38</td>\n",
       "      <td>Aston Villa could sign Cordoba striker Florin...</td>\n",
       "      <td>Aston Villa could sign Cordoba striker Florin ...</td>\n",
       "      <td>d to raise £2.5million for the Spanish side . ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             id  \\\n",
       "9204   fc8f37cb5bc8fe97794175fae6b876f07cf3fda4   \n",
       "10729  8d1da9b0197d9c733db56bdfa62332d04144398d   \n",
       "\n",
       "                                                articles  \\\n",
       "9204   A Florida bus passenger was arrested for throw...   \n",
       "10729  Aston Villa may be able to sign Cordoba strike...   \n",
       "\n",
       "                                               summaries  article_word_count  \\\n",
       "9204   Joel Parker, 33, was riding the bus in St John...                 143   \n",
       "10729  Aston Villa have held talks over Cordoba strik...                 189   \n",
       "\n",
       "       summary_word_count                                       bart_summary  \\\n",
       "9204                   58   Joel Parker, 33, was arrested for throwing a ...   \n",
       "10729                  38   Aston Villa could sign Cordoba striker Florin...   \n",
       "\n",
       "                                              t5_summary  \\\n",
       "9204   Joel Parker, 33, was about to get off the Suns...   \n",
       "10729  Aston Villa could sign Cordoba striker Florin ...   \n",
       "\n",
       "                                         pegasus_summary  \n",
       "9204   the driver, 33, was arrested at St James Parke...  \n",
       "10729  d to raise £2.5million for the Spanish side . ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(test_df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bart-base\n",
      "ROUGE scores:\n",
      "ROUGE-1: 0.3843\n",
      "ROUGE-2: 0.1697\n",
      "ROUGE-L: 0.3613\n",
      "t5\n",
      "ROUGE scores:\n",
      "ROUGE-1: 0.4071\n",
      "ROUGE-2: 0.1924\n",
      "ROUGE-L: 0.3867\n",
      "pegasus\n",
      "ROUGE scores:\n",
      "ROUGE-1: 0.2602\n",
      "ROUGE-2: 0.0602\n",
      "ROUGE-L: 0.2446\n"
     ]
    }
   ],
   "source": [
    "# test_pred = pd.read_csv(output_path)\n",
    "print(\"bart-base\")\n",
    "# Tính điểm ROUGE\n",
    "if \"summaries\" in test_df.columns:\n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(test_df[\"bart_summary\"].tolist(), test_df[\"summaries\"].tolist(), avg=True)\n",
    "\n",
    "    print(\"ROUGE scores:\")\n",
    "    print(f\"ROUGE-1: {scores['rouge-1']['f']:.4f}\")\n",
    "    print(f\"ROUGE-2: {scores['rouge-2']['f']:.4f}\")\n",
    "    print(f\"ROUGE-L: {scores['rouge-l']['f']:.4f}\")\n",
    "else:\n",
    "    print(\"⚠️ Không tìm thấy cột 'summaries' để tính ROUGE.\")\n",
    "print(\"t5\")    \n",
    "# Tính điểm ROUGE\n",
    "if \"summaries\" in test_df.columns:\n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(test_df[\"t5_summary\"].tolist(), test_df[\"summaries\"].tolist(), avg=True)\n",
    "\n",
    "    print(\"ROUGE scores:\")\n",
    "    print(f\"ROUGE-1: {scores['rouge-1']['f']:.4f}\")\n",
    "    print(f\"ROUGE-2: {scores['rouge-2']['f']:.4f}\")\n",
    "    print(f\"ROUGE-L: {scores['rouge-l']['f']:.4f}\")\n",
    "else:\n",
    "    print(\"⚠️ Không tìm thấy cột 'summaries' để tính ROUGE.\")\n",
    "\n",
    "print(\"pegasus\")    \n",
    "# Tính điểm ROUGE\n",
    "if \"summaries\" in test_df.columns:\n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(test_df[\"pegasus_summary\"].tolist(), test_df[\"summaries\"].tolist(), avg=True)\n",
    "\n",
    "    print(\"ROUGE scores:\")\n",
    "    print(f\"ROUGE-1: {scores['rouge-1']['f']:.4f}\")\n",
    "    print(f\"ROUGE-2: {scores['rouge-2']['f']:.4f}\")\n",
    "    print(f\"ROUGE-L: {scores['rouge-l']['f']:.4f}\")\n",
    "else:\n",
    "    print(\"⚠️ Không tìm thấy cột 'summaries' để tính ROUGE.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tóm tắt ensemble:\n",
      "Aesop’s Fables, Roald Dahl’s The Fantastic Mr. Fox, Rudyard Kipling’s The Jungle Book, and the Panchatantra are exciting adventure stories on animals. Children can be taught about various aspects of life through storytelling.\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "WEIGHTS = {\"bart\": 0.40, \"t5\": 0.50, \"pegasus\": 0.10}  # Trọng số mới\n",
    "\n",
    "# Hàm xử lý input mới và tạo tóm tắt từ từng mô hình\n",
    "def generate_single_summary(model, tokenizer, article, model_type=\"bart\"):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        if model_type == \"t5\":\n",
    "            input_text = \"summarize: \" + article\n",
    "            inputs = t5_tokenizer(\n",
    "                input_text,\n",
    "                max_length=MAX_LENGTH_ARTICLE,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(device)\n",
    "            output_ids = model.generate(\n",
    "                input_ids=inputs[\"input_ids\"],\n",
    "                attention_mask=inputs[\"attention_mask\"],\n",
    "                max_length=MAX_LENGTH_SUMMARY,\n",
    "                num_beams=4,\n",
    "                length_penalty=2.0,\n",
    "                early_stopping=True\n",
    "            )\n",
    "        else:  # BART hoặc Pegasus\n",
    "            inputs = tokenizer(\n",
    "                article,\n",
    "                max_length=MAX_LENGTH_ARTICLE,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(device)\n",
    "            output_ids = model.generate(\n",
    "                input_ids=inputs[\"input_ids\"],\n",
    "                attention_mask=inputs[\"attention_mask\"],\n",
    "                max_length=MAX_LENGTH_SUMMARY,\n",
    "                num_beams=4,\n",
    "                length_penalty=2.0,\n",
    "                early_stopping=True,\n",
    "                decoder_start_token_id=tokenizer.pad_token_id\n",
    "            )\n",
    "        summary = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "# Hàm hậu xử lý tóm tắt\n",
    "def post_process_summary(summary, max_length=MAX_LENGTH_SUMMARY):\n",
    "    sentences = sent_tokenize(summary)\n",
    "    unique_sentences = list(dict.fromkeys(sentences))  # Loại bỏ trùng lặp\n",
    "    tokenized = bart_tokenizer.encode(\" \".join(unique_sentences), truncation=True, max_length=max_length)\n",
    "    return bart_tokenizer.decode(tokenized, skip_special_tokens=True)\n",
    "\n",
    "# Hàm ensemble dựa trên ROUGE-L\n",
    "def ensemble_summaries(article):\n",
    "    # Tạo tóm tắt từ từng mô hình\n",
    "    bart_summary = generate_single_summary(bart_model, bart_tokenizer, article, \"bart\")\n",
    "    t5_summary = generate_single_summary(t5_model, t5_tokenizer, article, \"t5\")\n",
    "    pegasus_summary = generate_single_summary(pegasus_model, pegasus_tokenizer, article, \"pegasus\")\n",
    "    \n",
    "    # Danh sách tóm tắt\n",
    "    summaries = {\n",
    "        \"bart\": bart_summary,\n",
    "        \"t5\": t5_summary,\n",
    "        \"pegasus\": pegasus_summary\n",
    "    }\n",
    "    \n",
    "    # Tính ROUGE-L để chọn tóm tắt tốt nhất\n",
    "    candidates = [summaries[\"bart\"], summaries[\"t5\"], summaries[\"pegasus\"]]\n",
    "    rouge = Rouge()\n",
    "    scores = []\n",
    "    \n",
    "    for i, cand in enumerate(candidates):\n",
    "        others = [c for j, c in enumerate(candidates) if j != i]\n",
    "        # Tính điểm ROUGE-L trung bình so với các tóm tắt khác\n",
    "        if others:  # Đảm bảo có tóm tắt khác để so sánh\n",
    "            rouge_scores = rouge.get_scores([cand] * len(others), others, avg=True)\n",
    "            rouge_l_score = rouge_scores[\"rouge-l\"][\"f\"]\n",
    "            weighted_score = rouge_l_score * WEIGHTS[list(summaries.keys())[i]]\n",
    "            scores.append(weighted_score)\n",
    "        else:\n",
    "            scores.append(0.0)  # Nếu không có tóm tắt khác, gán điểm 0\n",
    "    \n",
    "    # Chọn tóm tắt có điểm cao nhất\n",
    "    best_idx = np.argmax(scores)\n",
    "    best_summary = candidates[best_idx]\n",
    "    \n",
    "    # Hậu xử lý\n",
    "    final_summary = post_process_summary(best_summary)\n",
    "    return final_summary\n",
    "\n",
    "# Hàm chính để gọi từ ngoài\n",
    "def summarize_article(article):\n",
    "    \"\"\"\n",
    "    Nhận một bài viết mới và trả về tóm tắt ensemble.\n",
    "    \n",
    "    Args:\n",
    "        article (str): Bài viết cần tóm tắt.\n",
    "    \n",
    "    Returns:\n",
    "        str: Tóm tắt ensemble.\n",
    "    \"\"\"\n",
    "    return ensemble_summaries(article)\n",
    "\n",
    "# Ví dụ sử dụng\n",
    "if __name__ == \"__main__\":\n",
    "    # Input mới\n",
    "    new_article = \"\"\"\n",
    "Animal stories for kids fascinate and intrigue their curious minds. These tales do more than just entertain – they plant seeds of wisdom that help little ones learn about right and wrong, caring for others, and how to be a good person in the world. Children learn important life lessons in a fun and memorable way through the adventures of furry and feathered friends. Aesop’s Fables, Roald Dahl’s The Fantastic Mr. Fox, Rudyard Kipling’s The Jungle Book, and the Panchatantra are exciting adventure stories on animals that kids may enjoy.\n",
    "\n",
    "Children can be taught about various aspects of life through storytelling. While some messages from the moral stories in English are simple and easy to follow, others may be intense and cannot be delivered directly. Science has proven that using animals enables authors to tell a powerful story while also maintaining emotional distance (1).\n",
    "\n",
    "Here is a compilation of some of the best short stories for kids that they may enjoy hearing. Encourage your child to explore these stories together and discuss the morals they convey.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Tạo tóm tắt\n",
    "    summary = summarize_article(new_article)\n",
    "    print(\"Tóm tắt ensemble:\")\n",
    "    print(summary)\n",
    "    print(summary == new_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3299cec81dd645e8ae9b88abff049932",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas() \n",
    "test_df[\"predicted_ensemble\"] = test_df[\"articles\"].progress_apply(summarize_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE scores:\n",
      "ROUGE-1: 0.4078\n",
      "ROUGE-2: 0.1930\n",
      "ROUGE-L: 0.3878\n"
     ]
    }
   ],
   "source": [
    "# Tính điểm ROUGE\n",
    "if \"summaries\" in test_df.columns:\n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(test_df[\"predicted_ensemble\"].tolist(), test_df[\"summaries\"].tolist(), avg=True)\n",
    "\n",
    "    print(\"ROUGE scores:\")\n",
    "    print(f\"ROUGE-1: {scores['rouge-1']['f']:.4f}\")\n",
    "    print(f\"ROUGE-2: {scores['rouge-2']['f']:.4f}\")\n",
    "    print(f\"ROUGE-L: {scores['rouge-l']['f']:.4f}\")\n",
    "else:\n",
    "    print(\"⚠️ Không tìm thấy cột 'summaries' để tính ROUGE.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv(output_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
