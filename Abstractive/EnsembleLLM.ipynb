{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted Averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, BartTokenizer, BartForConditionalGeneration, PegasusTokenizer, PegasusForConditionalGeneration\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "from rouge_score import rouge_scorer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 50\n",
    "BATCH_SIZE = 8\n",
    "FRAC_SAMPLE = 0.01\n",
    "MAX_LENGTH_ARTICLE = 512\n",
    "MIN_LENGTH_ARTICLE = 50\n",
    "MAX_LENGTH_SUMMARY = 128\n",
    "MIN_LENGTH_SUMMARY = 20\n",
    "HIDDEN_DIM = 128\n",
    "LEARNING_RATE = 1e-5\n",
    "PATIENCE = 5  # For early stopping\n",
    "WEIGHT_DECAY = 1e-4\n",
    "NUM_CYCLES = 5\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "datafilter = \"../dataft\"\n",
    "output_path = os.path.join(datafilter, \"test_pred_ensemble.csv\")\n",
    "save_dir_bart = \"fine_tuned_bart_cosine_3\"\n",
    "save_dir_t5 = \"fine_tuned_t5_small\"\n",
    "save_dir_pegasus = \"fine_tuned_pegasus_custom\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"../dataset/train.csv\")\n",
    "validation_data = pd.read_csv(\"../dataset/validation.csv\")\n",
    "test_data = pd.read_csv(\"../dataset/test.csv\")\n",
    "\n",
    "# add col\n",
    "train_data.rename(columns={\"highlights\": \"summaries\", \"article\":\"articles\"}, inplace=True)\n",
    "validation_data.rename(columns={\"highlights\": \"summaries\",\"article\":\"articles\"}, inplace=True)\n",
    "test_data.rename(columns={\"highlights\": \"summaries\", \"article\":\"articles\"}, inplace=True)\n",
    "\n",
    "train_data[\"article_word_count\"] = train_data[\"articles\"].astype(str).apply(lambda x: len(x.split()))\n",
    "train_data[\"summary_word_count\"] = train_data[\"summaries\"].astype(str).apply(lambda x: len(x.split()))\n",
    "\n",
    "validation_data[\"article_word_count\"] = validation_data[\"articles\"].astype(str).apply(lambda x: len(x.split()))\n",
    "validation_data[\"summary_word_count\"] = validation_data[\"summaries\"].astype(str).apply(lambda x: len(x.split()))\n",
    "\n",
    "test_data[\"article_word_count\"] = test_data[\"articles\"].astype(str).apply(lambda x: len(x.split()))\n",
    "test_data[\"summary_word_count\"] = test_data[\"summaries\"].astype(str).apply(lambda x: len(x.split()))\n",
    "\n",
    "# filter range\n",
    "train_data = train_data[\n",
    "    (train_data[\"article_word_count\"] <= MAX_LENGTH_ARTICLE) & \n",
    "    (train_data[\"article_word_count\"] >= MIN_LENGTH_ARTICLE) &\n",
    "    (train_data[\"summary_word_count\"] <= MAX_LENGTH_SUMMARY) &\n",
    "    (train_data[\"summary_word_count\"] >= MIN_LENGTH_SUMMARY)\n",
    "]\n",
    "\n",
    "validation_data = validation_data[\n",
    "    (validation_data[\"article_word_count\"] <= MAX_LENGTH_ARTICLE) & \n",
    "    (validation_data[\"article_word_count\"] >= MIN_LENGTH_ARTICLE) &\n",
    "    (validation_data[\"summary_word_count\"] <= MAX_LENGTH_SUMMARY) &\n",
    "    (validation_data[\"summary_word_count\"] >= MIN_LENGTH_SUMMARY)\n",
    "]\n",
    "test_data = test_data[\n",
    "    (test_data[\"article_word_count\"] <= MAX_LENGTH_ARTICLE) & \n",
    "    (test_data[\"article_word_count\"] >= MIN_LENGTH_ARTICLE) &\n",
    "    (test_data[\"summary_word_count\"] <= MAX_LENGTH_SUMMARY) &\n",
    "    (test_data[\"summary_word_count\"] >= MIN_LENGTH_SUMMARY)\n",
    "]\n",
    "\n",
    "train_sample = train_data.sample(frac=FRAC_SAMPLE, random_state=1)\n",
    "validation_sample = validation_data.sample(frac=FRAC_SAMPLE, random_state=1)\n",
    "test_sample = test_data.sample(frac=1, random_state=1)\n",
    "train_sample.info()\n",
    "print(\"\\n\")\n",
    "validation_sample.info()\n",
    "train_sample.to_csv(os.path.join(datafilter,\"train_sample.csv\"), index=False)\n",
    "test_sample.to_csv(os.path.join(datafilter,\"test_sample.csv\"), index=False)\n",
    "validation_sample.to_csv(os.path.join(datafilter,\"validation_sample.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample = pd.read_csv(\"../dataft/train_sample.csv\")\n",
    "validation_sample = pd.read_csv(\"../dataft/validation_sample.csv\")\n",
    "test_sample = pd.read_csv(\"../dataft/test_sample.csv\")\n",
    "train_sample.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bart_tokenizer = BartTokenizer.from_pretrained(save_dir_bart)\n",
    "bart_model = BartForConditionalGeneration.from_pretrained(save_dir_bart).to(device)\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(save_dir_t5)\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained(save_dir_t5).to(device)\n",
    "pegasus_tokenizer = PegasusTokenizer.from_pretrained(save_dir_pegasus)\n",
    "pegasus_model = PegasusForConditionalGeneration.from_pretrained(save_dir_pegasus).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummarizationDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_input_length=MAX_LENGTH_ARTICLE, max_output_length=MAX_LENGTH_SUMMARY):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_input_length = max_input_length\n",
    "        self.max_output_length = max_output_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        article = self.data.iloc[index][\"articles\"]\n",
    "        summary = self.data.iloc[index][\"summaries\"]\n",
    "        input_text = \"summarize: \" + article if isinstance(self.tokenizer, T5Tokenizer) else article\n",
    "        inputs = self.tokenizer(\n",
    "            input_text,\n",
    "            max_length=self.max_input_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        outputs = self.tokenizer(\n",
    "            summary,\n",
    "            max_length=self.max_output_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": inputs.input_ids.squeeze(),\n",
    "            \"attention_mask\": inputs.attention_mask.squeeze(),\n",
    "            \"labels\": outputs.input_ids.squeeze(),\n",
    "            \"article\": article,\n",
    "            \"summary\": summary\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_sample\n",
    "# Create test dataset (use T5 tokenizer for simplicity, as it works for all models)\n",
    "test_dataset = SummarizationDataset(test_df, t5_tokenizer)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summaries(model, tokenizer, loader, is_t5=False):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Generating summaries\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            output_ids = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=MAX_LENGTH_SUMMARY,\n",
    "                min_length=MIN_LENGTH_SUMMARY,\n",
    "                num_beams=4,\n",
    "                length_penalty=2.0,\n",
    "                early_stopping=True\n",
    "            )\n",
    "            batch_preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "            predictions.extend(batch_preds)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate summaries for each model\n",
    "bart_preds = generate_summaries(bart_model, bart_tokenizer, test_loader)\n",
    "t5_preds = generate_summaries(t5_model, t5_tokenizer, test_loader, is_t5=True)\n",
    "pegasus_preds = generate_summaries(pegasus_model, pegasus_tokenizer, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble function (Weighted Averaging)\n",
    "def ensemble_summaries(bart_preds, t5_preds, pegasus_preds, weights=[0.4, 0.2, 0.4]):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    ensemble_preds = []\n",
    "    \n",
    "    for bart_pred, t5_pred, pegasus_pred in zip(bart_preds, t5_preds, pegasus_preds):\n",
    "        candidates = [bart_pred, t5_pred, pegasus_pred]\n",
    "        # Split into sentences (simple split for demo)\n",
    "        candidate_sentences = [sent.split('. ') for sent in candidates]\n",
    "        # Flatten and collect unique sentences\n",
    "        all_sentences = []\n",
    "        for sents in candidate_sentences:\n",
    "            all_sentences.extend(sents)\n",
    "        all_sentences = list(set([s.strip() for s in all_sentences if s.strip()]))\n",
    "        \n",
    "        if not all_sentences:\n",
    "            ensemble_preds.append(bart_pred)  # Fallback to BART if no sentences\n",
    "            continue\n",
    "        \n",
    "        # Score each sentence based on ROUGE with reference to all candidates\n",
    "        sentence_scores = []\n",
    "        for sent in all_sentences:\n",
    "            scores = []\n",
    "            for cand in candidates:\n",
    "                if sent in cand:\n",
    "                    score = scorer.score(cand, sent)['rouge1'].fmeasure\n",
    "                    scores.append(score)\n",
    "                else:\n",
    "                    scores.append(0.0)\n",
    "            # Weighted score\n",
    "            weighted_score = sum(w * s for w, s in zip(weights, scores))\n",
    "            sentence_scores.append((sent, weighted_score))\n",
    "        \n",
    "        # Select top sentences\n",
    "        sentence_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        selected_sentences = sentence_scores[:3]  # Select top 3 sentences\n",
    "        ensemble_summary = '. '.join([sent for sent, _ in selected_sentences if sent])\n",
    "        ensemble_preds.append(ensemble_summary)\n",
    "    \n",
    "    return ensemble_preds\n",
    "\n",
    "# Ensemble predictions\n",
    "weights = [0.5, 0.3, 0.2]  # Higher weights for BART, t5\n",
    "ensemble_preds = ensemble_summaries(bart_preds, t5_preds, pegasus_preds, weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions\n",
    "test_sample = test_df.iloc[:len(ensemble_preds)].copy()\n",
    "test_sample[\"bart_summary\"] = bart_preds\n",
    "test_sample[\"t5_summary\"] = t5_preds\n",
    "test_sample[\"pegasus_summary\"] = pegasus_preds\n",
    "test_sample[\"ensemble_summary\"] = ensemble_preds\n",
    "test_sample.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"✅ File has been saved at: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = pd.read_csv(output_path)\n",
    "# Tính điểm ROUGE\n",
    "if \"summaries\" in test_pred.columns:\n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(predictions, test_sample[\"summaries\"].tolist(), avg=True)\n",
    "\n",
    "    print(\"ROUGE scores:\")\n",
    "    print(f\"ROUGE-1: {scores['rouge-1']['f']:.4f}\")\n",
    "    print(f\"ROUGE-2: {scores['rouge-2']['f']:.4f}\")\n",
    "    print(f\"ROUGE-L: {scores['rouge-l']['f']:.4f}\")\n",
    "else:\n",
    "    print(\"⚠️ Không tìm thấy cột 'summaries' để tính ROUGE.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
