{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted Averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, BartTokenizer, BartForConditionalGeneration, PegasusTokenizer, PegasusForConditionalGeneration\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "from rouge import Rouge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 1\n",
    "BATCH_SIZE = 8\n",
    "FRAC_SAMPLE = 0.01\n",
    "MAX_LENGTH_ARTICLE = 512\n",
    "MIN_LENGTH_ARTICLE = 50\n",
    "MAX_LENGTH_SUMMARY = 128\n",
    "MIN_LENGTH_SUMMARY = 20\n",
    "HIDDEN_DIM = 128\n",
    "LEARNING_RATE = 1e-5\n",
    "PATIENCE = 5  # For early stopping\n",
    "WEIGHT_DECAY = 1e-4\n",
    "NUM_CYCLES = 5\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "datafilter = \"../dataft\"\n",
    "output_path = os.path.join(datafilter, \"test_pred_ensemble.csv\")\n",
    "save_dir_bart = \"../Model/fine_tuned_bart_base\"\n",
    "save_dir_t5 = \"../Model/fine_tuned_t5_small\"\n",
    "save_dir_pegasus = \"../Model/fine_tuned_pegasus_custom\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 960 entries, 144417 to 108633\n",
      "Data columns (total 5 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   id                  960 non-null    object\n",
      " 1   articles            960 non-null    object\n",
      " 2   summaries           960 non-null    object\n",
      " 3   article_word_count  960 non-null    int64 \n",
      " 4   summary_word_count  960 non-null    int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 45.0+ KB\n",
      "\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 50 entries, 8901 to 12116\n",
      "Data columns (total 5 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   id                  50 non-null     object\n",
      " 1   articles            50 non-null     object\n",
      " 2   summaries           50 non-null     object\n",
      " 3   article_word_count  50 non-null     int64 \n",
      " 4   summary_word_count  50 non-null     int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 2.3+ KB\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv(\"../dataset/train.csv\")\n",
    "validation_data = pd.read_csv(\"../dataset/validation.csv\")\n",
    "test_data = pd.read_csv(\"../dataset/test.csv\")\n",
    "\n",
    "# add col\n",
    "train_data.rename(columns={\"highlights\": \"summaries\", \"article\":\"articles\"}, inplace=True)\n",
    "validation_data.rename(columns={\"highlights\": \"summaries\",\"article\":\"articles\"}, inplace=True)\n",
    "test_data.rename(columns={\"highlights\": \"summaries\", \"article\":\"articles\"}, inplace=True)\n",
    "\n",
    "train_data[\"article_word_count\"] = train_data[\"articles\"].astype(str).apply(lambda x: len(x.split()))\n",
    "train_data[\"summary_word_count\"] = train_data[\"summaries\"].astype(str).apply(lambda x: len(x.split()))\n",
    "\n",
    "validation_data[\"article_word_count\"] = validation_data[\"articles\"].astype(str).apply(lambda x: len(x.split()))\n",
    "validation_data[\"summary_word_count\"] = validation_data[\"summaries\"].astype(str).apply(lambda x: len(x.split()))\n",
    "\n",
    "test_data[\"article_word_count\"] = test_data[\"articles\"].astype(str).apply(lambda x: len(x.split()))\n",
    "test_data[\"summary_word_count\"] = test_data[\"summaries\"].astype(str).apply(lambda x: len(x.split()))\n",
    "\n",
    "# filter range\n",
    "train_data = train_data[\n",
    "    (train_data[\"article_word_count\"] <= MAX_LENGTH_ARTICLE) & \n",
    "    (train_data[\"article_word_count\"] >= MIN_LENGTH_ARTICLE) &\n",
    "    (train_data[\"summary_word_count\"] <= MAX_LENGTH_SUMMARY) &\n",
    "    (train_data[\"summary_word_count\"] >= MIN_LENGTH_SUMMARY)\n",
    "]\n",
    "\n",
    "validation_data = validation_data[\n",
    "    (validation_data[\"article_word_count\"] <= MAX_LENGTH_ARTICLE) & \n",
    "    (validation_data[\"article_word_count\"] >= MIN_LENGTH_ARTICLE) &\n",
    "    (validation_data[\"summary_word_count\"] <= MAX_LENGTH_SUMMARY) &\n",
    "    (validation_data[\"summary_word_count\"] >= MIN_LENGTH_SUMMARY)\n",
    "]\n",
    "test_data = test_data[\n",
    "    (test_data[\"article_word_count\"] <= MAX_LENGTH_ARTICLE) & \n",
    "    (test_data[\"article_word_count\"] >= MIN_LENGTH_ARTICLE) &\n",
    "    (test_data[\"summary_word_count\"] <= MAX_LENGTH_SUMMARY) &\n",
    "    (test_data[\"summary_word_count\"] >= MIN_LENGTH_SUMMARY)\n",
    "]\n",
    "\n",
    "train_sample = train_data.sample(frac=FRAC_SAMPLE, random_state=1)\n",
    "validation_sample = validation_data.sample(frac=FRAC_SAMPLE, random_state=1)\n",
    "test_sample = test_data.sample(frac=0.01, random_state=1)\n",
    "train_sample.info()\n",
    "print(\"\\n\")\n",
    "validation_sample.info()\n",
    "# train_sample.to_csv(os.path.join(datafilter,\"train_sample.csv\"), index=False)\n",
    "# test_sample.to_csv(os.path.join(datafilter,\"test_sample.csv\"), index=False)\n",
    "# validation_sample.to_csv(os.path.join(datafilter,\"validation_sample.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BartBaseDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_input_length=MAX_LENGTH_ARTICLE, max_output_length=MAX_LENGTH_SUMMARY):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_input_length = max_input_length\n",
    "        self.max_output_length = max_output_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        article = self.data.iloc[index][\"articles\"]\n",
    "        summary = self.data.iloc[index][\"summaries\"]\n",
    "        input_ids = self.tokenizer.encode(article, max_length=self.max_input_length, truncation=True, padding=\"max_length\")\n",
    "        output_ids = self.tokenizer.encode(summary, max_length=self.max_output_length, truncation=True, padding=\"max_length\")\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": [int(token_id != 0) for token_id in input_ids], \"decoder_input_ids\": output_ids[:-1], \"decoder_attention_mask\": [1] * (len(output_ids) - 1), \"labels\": output_ids[1:]}\n",
    "\n",
    "def collate_bart(batch):\n",
    "    input_ids = [item[\"input_ids\"] for item in batch]\n",
    "    attention_mask = [item[\"attention_mask\"] for item in batch]\n",
    "    decoder_input_ids = [item[\"decoder_input_ids\"] for item in batch]\n",
    "    decoder_attention_mask = [item[\"decoder_attention_mask\"] for item in batch]\n",
    "    labels = [item[\"labels\"] for item in batch]\n",
    "    max_input_length = max(len(ids) for ids in input_ids)\n",
    "    max_output_length = max(len(ids) for ids in decoder_input_ids)\n",
    "    input_ids = [ids + [0] * (max_input_length - len(ids)) for ids in input_ids]\n",
    "    attention_mask = [mask + [0] * (max_input_length - len(mask)) for mask in attention_mask]\n",
    "    decoder_input_ids = [ids + [0] * (max_output_length - len(ids)) for ids in decoder_input_ids]\n",
    "    decoder_attention_mask = [mask + [0] * (max_output_length - len(mask)) for mask in decoder_attention_mask]\n",
    "    labels = [ids + [-100] * (max_output_length - len(ids)) for ids in labels]\n",
    "    return {\"input_ids\": torch.tensor(input_ids), \"attention_mask\": torch.tensor(attention_mask), \"decoder_input_ids\": torch.tensor(decoder_input_ids), \"decoder_attention_mask\": torch.tensor(decoder_attention_mask), \"labels\": torch.tensor(labels)}\n",
    "\n",
    "bart_tokenizer = BartTokenizer.from_pretrained(save_dir_bart)\n",
    "bart_model = BartForConditionalGeneration.from_pretrained(save_dir_bart).to(device)\n",
    "\n",
    "test_dataset_bart = BartBaseDataset(test_df, bart_tokenizer)\n",
    "test_loader_bart = DataLoader(test_dataset_bart, batch_size=8, collate_fn=collate_bart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class T5SmallDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_input_length=MAX_LENGTH_ARTICLE, max_output_length=MAX_LENGTH_SUMMARY):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_input_length = max_input_length\n",
    "        self.max_output_length = max_output_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        article = self.data.iloc[index][\"articles\"]\n",
    "        summary = self.data.iloc[index][\"summaries\"]\n",
    "        \n",
    "        # T5 need prefix:\n",
    "        input_text = \"summarize: \" + article\n",
    "        inputs = self.tokenizer(\n",
    "            input_text,\n",
    "            max_length=self.max_input_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        outputs = self.tokenizer(\n",
    "            summary,\n",
    "            max_length=self.max_output_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": inputs.input_ids.squeeze(),\n",
    "            \"attention_mask\": inputs.attention_mask.squeeze(),\n",
    "            \"labels\": outputs.input_ids.squeeze()\n",
    "        }\n",
    "def collate_t5(batch):\n",
    "    input_ids = torch.stack([item[\"input_ids\"] for item in batch])\n",
    "    attention_mask = torch.stack([item[\"attention_mask\"] for item in batch])\n",
    "    labels = torch.stack([item[\"labels\"] for item in batch])\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "    \n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(save_dir_t5)\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained(save_dir_t5).to(device)\n",
    "\n",
    "test_dataset_t5 = T5SmallDataset(test_df, t5_tokenizer)\n",
    "test_loader_t5 = DataLoader(test_dataset_t5, batch_size=8, collate_fn=collate_t5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pegasus_tokenizer = PegasusTokenizer.from_pretrained(save_dir_pegasus)\n",
    "pegasus_model = PegasusForConditionalGeneration.from_pretrained(save_dir_pegasus).to(device)\n",
    "\n",
    "class PegaCustomDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_input_length=MAX_LENGTH_ARTICLE, max_output_length=MAX_LENGTH_SUMMARY):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_input_length = max_input_length\n",
    "        self.max_output_length = max_output_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        article = self.data.iloc[index][\"articles\"]\n",
    "        summary = self.data.iloc[index][\"summaries\"]\n",
    "        input_ids = self.tokenizer.encode(article, max_length=self.max_input_length, truncation=True, padding=\"max_length\")\n",
    "        output_ids = self.tokenizer.encode(summary, max_length=self.max_output_length, truncation=True, padding=\"max_length\")\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": [int(token_id != 0) for token_id in input_ids], \"decoder_input_ids\": output_ids[:-1], \"decoder_attention_mask\": [1] * (len(output_ids) - 1), \"labels\": output_ids[1:]}\n",
    "\n",
    "def collate_pega(batch):\n",
    "    input_ids = [item[\"input_ids\"] for item in batch]\n",
    "    attention_mask = [item[\"attention_mask\"] for item in batch]\n",
    "    decoder_input_ids = [item[\"decoder_input_ids\"] for item in batch]\n",
    "    decoder_attention_mask = [item[\"decoder_attention_mask\"] for item in batch]\n",
    "    labels = [item[\"labels\"] for item in batch]\n",
    "    max_input_length = max(len(ids) for ids in input_ids)\n",
    "    max_output_length = max(len(ids) for ids in decoder_input_ids)\n",
    "    input_ids = [ids + [0] * (max_input_length - len(ids)) for ids in input_ids]\n",
    "    attention_mask = [mask + [0] * (max_input_length - len(mask)) for mask in attention_mask]\n",
    "    decoder_input_ids = [ids + [0] * (max_output_length - len(ids)) for ids in decoder_input_ids]\n",
    "    decoder_attention_mask = [mask + [0] * (max_output_length - len(mask)) for mask in decoder_attention_mask]\n",
    "    labels = [ids + [-100] * (max_output_length - len(ids)) for ids in labels]\n",
    "    \n",
    "    labels = torch.tensor(labels)\n",
    "    labels[labels == pegasus_tokenizer.pad_token_id] = -100\n",
    "    \n",
    "    return {\"input_ids\": torch.tensor(input_ids), \"attention_mask\": torch.tensor(attention_mask), \"decoder_input_ids\": torch.tensor(decoder_input_ids), \"decoder_attention_mask\": torch.tensor(decoder_attention_mask), \"labels\": torch.tensor(labels)}\n",
    "\n",
    "test_dataset_pegasus = PegaCustomDataset(test_df, pegasus_tokenizer)\n",
    "test_loader_pegasus = DataLoader(test_dataset_pegasus, batch_size=8, collate_fn=collate_pega)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summaries(model, tokenizer, data_loader, model_type=\"bart\"):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=f\"Generating {model_type} summaries\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            if model_type == \"t5\":\n",
    "                output_ids = model.generate(\n",
    "                    input_ids=batch[\"input_ids\"],\n",
    "                    attention_mask=batch[\"attention_mask\"],\n",
    "                    max_length=MAX_LENGTH_SUMMARY,\n",
    "                    num_beams=4,\n",
    "                    length_penalty=2.0,\n",
    "                    early_stopping=True\n",
    "                )\n",
    "            else:  # BART ho·∫∑c Pegasus\n",
    "                output_ids = model.generate(\n",
    "                    input_ids=batch[\"input_ids\"],\n",
    "                    attention_mask=batch[\"attention_mask\"],\n",
    "                    max_length=MAX_LENGTH_SUMMARY,\n",
    "                    num_beams=4,\n",
    "                    length_penalty=2.0,\n",
    "                    early_stopping=True,\n",
    "                    decoder_start_token_id=tokenizer.pad_token_id\n",
    "                )\n",
    "\n",
    "            batch_preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "            predictions.extend(batch_preds)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating bart summaries: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [05:57<00:00, 59.60s/it]\n",
      "Generating t5 summaries: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:30<00:00, 25.12s/it]\n",
      "Generating pegasus summaries:   0%|          | 0/6 [00:00<?, ?it/s]/tmp/ipykernel_3616/2490203540.py:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {\"input_ids\": torch.tensor(input_ids), \"attention_mask\": torch.tensor(attention_mask), \"decoder_input_ids\": torch.tensor(decoder_input_ids), \"decoder_attention_mask\": torch.tensor(decoder_attention_mask), \"labels\": torch.tensor(labels)}\n",
      "Generating pegasus summaries: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:50<00:00,  8.47s/it]\n"
     ]
    }
   ],
   "source": [
    "bart_predictions = generate_summaries(bart_model, bart_tokenizer, test_loader_bart, \"bart\")\n",
    "t5_predictions = generate_summaries(t5_model, t5_tokenizer, test_loader_t5, \"t5\")\n",
    "pegasus_predictions = generate_summaries(pegasus_model, pegasus_tokenizer, test_loader_pegasus, \"pegasus\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L∆∞u d·ª± ƒëo√°n v√†o DataFrame\n",
    "test_df[\"bart_summary\"] = bart_predictions\n",
    "test_df[\"t5_summary\"] = t5_predictions\n",
    "test_df[\"pegasus_summary\"] = pegasus_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>articles</th>\n",
       "      <th>summaries</th>\n",
       "      <th>article_word_count</th>\n",
       "      <th>summary_word_count</th>\n",
       "      <th>bart_summary</th>\n",
       "      <th>t5_summary</th>\n",
       "      <th>pegasus_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9204</th>\n",
       "      <td>fc8f37cb5bc8fe97794175fae6b876f07cf3fda4</td>\n",
       "      <td>A Florida bus passenger was arrested for throw...</td>\n",
       "      <td>Joel Parker, 33, was riding the bus in St John...</td>\n",
       "      <td>143</td>\n",
       "      <td>58</td>\n",
       "      <td>Joel Parker, 33, was about to get off the Sun...</td>\n",
       "      <td>Joel Parker, 33, was about to get off the Suns...</td>\n",
       "      <td>the driver, 33, was arrested at St James Parke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10729</th>\n",
       "      <td>8d1da9b0197d9c733db56bdfa62332d04144398d</td>\n",
       "      <td>Aston Villa may be able to sign Cordoba strike...</td>\n",
       "      <td>Aston Villa have held talks over Cordoba strik...</td>\n",
       "      <td>189</td>\n",
       "      <td>38</td>\n",
       "      <td>ston Villa may be able to sign Cordoba striker...</td>\n",
       "      <td>Aston Villa could sign Cordoba striker Florin ...</td>\n",
       "      <td>d to raise ¬£2.5million for the Spanish side . ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             id  \\\n",
       "9204   fc8f37cb5bc8fe97794175fae6b876f07cf3fda4   \n",
       "10729  8d1da9b0197d9c733db56bdfa62332d04144398d   \n",
       "\n",
       "                                                articles  \\\n",
       "9204   A Florida bus passenger was arrested for throw...   \n",
       "10729  Aston Villa may be able to sign Cordoba strike...   \n",
       "\n",
       "                                               summaries  article_word_count  \\\n",
       "9204   Joel Parker, 33, was riding the bus in St John...                 143   \n",
       "10729  Aston Villa have held talks over Cordoba strik...                 189   \n",
       "\n",
       "       summary_word_count                                       bart_summary  \\\n",
       "9204                   58   Joel Parker, 33, was about to get off the Sun...   \n",
       "10729                  38  ston Villa may be able to sign Cordoba striker...   \n",
       "\n",
       "                                              t5_summary  \\\n",
       "9204   Joel Parker, 33, was about to get off the Suns...   \n",
       "10729  Aston Villa could sign Cordoba striker Florin ...   \n",
       "\n",
       "                                         pegasus_summary  \n",
       "9204   the driver, 33, was arrested at St James Parke...  \n",
       "10729  d to raise ¬£2.5million for the Spanish side . ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(test_df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bart-base\n",
      "ROUGE scores:\n",
      "ROUGE-1: 0.3967\n",
      "ROUGE-2: 0.1817\n",
      "ROUGE-L: 0.3723\n",
      "t5\n",
      "ROUGE scores:\n",
      "ROUGE-1: 0.3891\n",
      "ROUGE-2: 0.1803\n",
      "ROUGE-L: 0.3706\n",
      "pegasus\n",
      "ROUGE scores:\n",
      "ROUGE-1: 0.2474\n",
      "ROUGE-2: 0.0573\n",
      "ROUGE-L: 0.2303\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"bart-base\")\n",
    "# T√≠nh ƒëi·ªÉm ROUGE\n",
    "if \"summaries\" in test_df.columns:\n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(test_df[\"bart_summary\"].tolist(), test_df[\"summaries\"].tolist(), avg=True)\n",
    "\n",
    "    print(\"ROUGE scores:\")\n",
    "    print(f\"ROUGE-1: {scores['rouge-1']['f']:.4f}\")\n",
    "    print(f\"ROUGE-2: {scores['rouge-2']['f']:.4f}\")\n",
    "    print(f\"ROUGE-L: {scores['rouge-l']['f']:.4f}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y c·ªôt 'summaries' ƒë·ªÉ t√≠nh ROUGE.\")\n",
    "print(\"t5\")    \n",
    "# T√≠nh ƒëi·ªÉm ROUGE\n",
    "if \"summaries\" in test_df.columns:\n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(test_df[\"t5_summary\"].tolist(), test_df[\"summaries\"].tolist(), avg=True)\n",
    "\n",
    "    print(\"ROUGE scores:\")\n",
    "    print(f\"ROUGE-1: {scores['rouge-1']['f']:.4f}\")\n",
    "    print(f\"ROUGE-2: {scores['rouge-2']['f']:.4f}\")\n",
    "    print(f\"ROUGE-L: {scores['rouge-l']['f']:.4f}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y c·ªôt 'summaries' ƒë·ªÉ t√≠nh ROUGE.\")\n",
    "\n",
    "print(\"pegasus\")    \n",
    "# T√≠nh ƒëi·ªÉm ROUGE\n",
    "if \"summaries\" in test_df.columns:\n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(test_df[\"pegasus_summary\"].tolist(), test_df[\"summaries\"].tolist(), avg=True)\n",
    "\n",
    "    print(\"ROUGE scores:\")\n",
    "    print(f\"ROUGE-1: {scores['rouge-1']['f']:.4f}\")\n",
    "    print(f\"ROUGE-2: {scores['rouge-2']['f']:.4f}\")\n",
    "    print(f\"ROUGE-L: {scores['rouge-l']['f']:.4f}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y c·ªôt 'summaries' ƒë·ªÉ t√≠nh ROUGE.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble summarization:\n",
      "Animal stories for kids fascinate and intrigue their curious minds . They plant seeds of wisdom that help little ones learn about right and wrong, caring for others, and how to be a good person in the world . Children can be taught about various aspects of life through storytelling .\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "WEIGHTS = {\"bart\": 0.40, \"t5\": 0.50, \"pegasus\": 0.10}  # Tr·ªçng s·ªë m·ªõi\n",
    "\n",
    "def generate_single_summary(model, tokenizer, article, model_type=\"bart\"):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        if model_type == \"t5\":\n",
    "            input_text = \"summarize: \" + article\n",
    "            inputs = t5_tokenizer(\n",
    "                input_text,\n",
    "                max_length=MAX_LENGTH_ARTICLE,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(device)\n",
    "            output_ids = model.generate(\n",
    "                input_ids=inputs[\"input_ids\"],\n",
    "                attention_mask=inputs[\"attention_mask\"],\n",
    "                max_length=MAX_LENGTH_SUMMARY,\n",
    "                num_beams=4,\n",
    "                length_penalty=2.0,\n",
    "                early_stopping=True\n",
    "            )\n",
    "        else:  # BART ho·∫∑c Pegasus\n",
    "            inputs = tokenizer(\n",
    "                article,\n",
    "                max_length=MAX_LENGTH_ARTICLE,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(device)\n",
    "            output_ids = model.generate(\n",
    "                input_ids=inputs[\"input_ids\"],\n",
    "                attention_mask=inputs[\"attention_mask\"],\n",
    "                max_length=MAX_LENGTH_SUMMARY,\n",
    "                num_beams=4,\n",
    "                length_penalty=2.0,\n",
    "                early_stopping=True,\n",
    "                decoder_start_token_id=tokenizer.pad_token_id\n",
    "            )\n",
    "        summary = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "\n",
    "def post_process_summary(summary, max_length=MAX_LENGTH_SUMMARY):\n",
    "    sentences = sent_tokenize(summary)\n",
    "    unique_sentences = list(dict.fromkeys(sentences))  # Lo·∫°i b·ªè tr√πng l·∫∑p\n",
    "    tokenized = bart_tokenizer.encode(\" \".join(unique_sentences), truncation=True, max_length=max_length)\n",
    "    return bart_tokenizer.decode(tokenized, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "def ensemble_summaries(article):\n",
    "    bart_summary = generate_single_summary(bart_model, bart_tokenizer, article, \"bart\")\n",
    "    t5_summary = generate_single_summary(t5_model, t5_tokenizer, article, \"t5\")\n",
    "    pegasus_summary = generate_single_summary(pegasus_model, pegasus_tokenizer, article, \"pegasus\")\n",
    "    \n",
    "\n",
    "    summaries = {\n",
    "        \"bart\": bart_summary,\n",
    "        \"t5\": t5_summary,\n",
    "        \"pegasus\": pegasus_summary\n",
    "    }\n",
    "    \n",
    "    # Ch√≥ose the best base on rouge l\n",
    "    candidates = [summaries[\"bart\"], summaries[\"t5\"], summaries[\"pegasus\"]]\n",
    "    rouge = Rouge()\n",
    "    scores = []\n",
    "    \n",
    "    for i, cand in enumerate(candidates):\n",
    "        others = [c for j, c in enumerate(candidates) if j != i]\n",
    "        if others:  \n",
    "            rouge_scores = rouge.get_scores([cand] * len(others), others, avg=True)\n",
    "            rouge_l_score = rouge_scores[\"rouge-l\"][\"f\"]\n",
    "            weighted_score = rouge_l_score * WEIGHTS[list(summaries.keys())[i]]\n",
    "            scores.append(weighted_score)\n",
    "        else:\n",
    "            scores.append(0.0)  # if no summ, attach 0\n",
    "    best_idx = np.argmax(scores)\n",
    "    best_summary = candidates[best_idx]\n",
    "\n",
    "    final_summary = post_process_summary(best_summary)\n",
    "    return final_summary\n",
    "\n",
    "# H√†m ch√≠nh ƒë·ªÉ g·ªçi t·ª´ ngo√†i\n",
    "def summarize_article(article):\n",
    "    \"\"\"\n",
    "    Generate an ensemble summary for a given article using multiple models.\n",
    "    \"\"\"\n",
    "    return ensemble_summaries(article)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    new_article = \"\"\"\n",
    "Animal stories for kids fascinate and intrigue their curious minds. These tales do more than just entertain ‚Äì they plant seeds of wisdom that help little ones learn about right and wrong, caring for others, and how to be a good person in the world. Children learn important life lessons in a fun and memorable way through the adventures of furry and feathered friends. Aesop‚Äôs Fables, Roald Dahl‚Äôs The Fantastic Mr. Fox, Rudyard Kipling‚Äôs The Jungle Book, and the Panchatantra are exciting adventure stories on animals that kids may enjoy.\n",
    "\n",
    "Children can be taught about various aspects of life through storytelling. While some messages from the moral stories in English are simple and easy to follow, others may be intense and cannot be delivered directly. Science has proven that using animals enables authors to tell a powerful story while also maintaining emotional distance (1).\n",
    "\n",
    "Here is a compilation of some of the best short stories for kids that they may enjoy hearing. Encourage your child to explore these stories together and discuss the morals they convey.\n",
    "    \"\"\"\n",
    "\n",
    "    summary = summarize_article(new_article)\n",
    "    print(\"Ensemble summarization:\")\n",
    "    print(summary)\n",
    "    print(summary == new_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "351d9c4cf3004d64a48b469c4a6eb05a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas() \n",
    "test_df[\"predicted_ensemble\"] = test_df[\"articles\"].progress_apply(summarize_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv(output_path,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE scores:\n",
      "ROUGE-1: 0.3881\n",
      "ROUGE-2: 0.1803\n",
      "ROUGE-L: 0.3691\n"
     ]
    }
   ],
   "source": [
    "# T√≠nh ƒëi·ªÉm ROUGE\n",
    "if \"summaries\" in test_df.columns:\n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(test_df[\"predicted_ensemble\"].tolist(), test_df[\"summaries\"].tolist(), avg=True)\n",
    "\n",
    "    print(\"ROUGE scores:\")\n",
    "    print(f\"ROUGE-1: {scores['rouge-1']['f']:.4f}\")\n",
    "    print(f\"ROUGE-2: {scores['rouge-2']['f']:.4f}\")\n",
    "    print(f\"ROUGE-L: {scores['rouge-l']['f']:.4f}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y c·ªôt 'summaries' ƒë·ªÉ t√≠nh ROUGE.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî∏ ROUGE scores:\n",
      "ROUGE-1: 0.3881\n",
      "ROUGE-2: 0.1803\n",
      "ROUGE-L: 0.3691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üî∏ BERTScore:\n",
      "Precision: 0.8839\n",
      "Recall:    0.8736\n",
      "F1:        0.8786\n",
      "\n",
      "üî∏ METEOR (avg): 0.2998\n"
     ]
    }
   ],
   "source": [
    "from rouge import Rouge\n",
    "from bert_score import score as bert_score\n",
    "from nltk.translate.meteor_score import single_meteor_score\n",
    "import pandas as pd\n",
    "\n",
    "if \"summaries\" in test_df.columns and \"predicted_ensemble\" in test_df.columns:\n",
    "    references = test_df[\"summaries\"].fillna(\"<empty>\").astype(str).tolist()\n",
    "    predictions = test_df[\"predicted_ensemble\"].fillna(\"<empty>\").astype(str).tolist()\n",
    "\n",
    "    valid_pairs = [\n",
    "        (pred, ref) for pred, ref in zip(predictions, references)\n",
    "        if pred.strip() and pred != \"<empty>\" and ref.strip()\n",
    "    ]\n",
    "\n",
    "    if not valid_pairs:\n",
    "        print(\"‚ö†Ô∏è Kh√¥ng c√≥ c·∫∑p h·ª£p l·ªá ƒë·ªÉ t√≠nh ƒëi·ªÉm.\")\n",
    "    else:\n",
    "        filtered_preds, filtered_refs = zip(*valid_pairs)\n",
    "\n",
    "        # ROUGE\n",
    "        rouge = Rouge()\n",
    "        rouge_scores = rouge.get_scores(filtered_preds, filtered_refs, avg=True)\n",
    "        print(\"üî∏ ROUGE scores:\")\n",
    "        print(f\"ROUGE-1: {rouge_scores['rouge-1']['f']:.4f}\")\n",
    "        print(f\"ROUGE-2: {rouge_scores['rouge-2']['f']:.4f}\")\n",
    "        print(f\"ROUGE-L: {rouge_scores['rouge-l']['f']:.4f}\")\n",
    "\n",
    "        # BERTScore\n",
    "        P, R, F1 = bert_score(filtered_preds, filtered_refs, lang=\"en\", verbose=False)\n",
    "        print(\"\\nüî∏ BERTScore:\")\n",
    "        print(f\"Precision: {P.mean().item():.4f}\")\n",
    "        print(f\"Recall:    {R.mean().item():.4f}\")\n",
    "        print(f\"F1:        {F1.mean().item():.4f}\")\n",
    "\n",
    "        # METEOR\n",
    "        meteor_scores = [single_meteor_score(ref.split(), pred.split())\n",
    "                         for pred, ref in zip(filtered_preds, filtered_refs)]\n",
    "        print(f\"\\nüî∏ METEOR (avg): {sum(meteor_scores)/len(meteor_scores):.4f}\")\n",
    "\n",
    "else:\n",
    "    print(\"No column 'summaries' and 'predicted_ensemble' for calculation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
