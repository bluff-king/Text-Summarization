{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "import wandb\n",
    "import time\n",
    "import os\n",
    "\n",
    "from rouge import Rouge\n",
    "from bert_score import score as bert_score\n",
    "from nltk.translate.meteor_score import single_meteor_score\n",
    "# Import Hugging Face tokenizer\n",
    "from transformers import BartTokenizer\n",
    "\n",
    "# nltk.download('punkt') # No longer needed with HF tokenizer\n",
    "\n",
    "NUM_EPOCHS = 1\n",
    "BATCH_SIZE = 4\n",
    "FRAC_SAMPLE = 0.01\n",
    "MAX_LENGTH_ARTICLE = 512\n",
    "MIN_LENGTH_ARTICLE = 50 \n",
    "MAX_LENGTH_SUMMARY = 128\n",
    "MIN_LENGTH_SUMMARY = 20 \n",
    "HIDDEN_DIM = 512\n",
    "LEARNING_RATE = 0.00005 \n",
    "NUM_CYCLES = 3\n",
    "MAX_PLATEAU_COUNT = 5\n",
    "WEIGHT_DECAY = 1e-4\n",
    "CLIP = 1\n",
    "USE_PRETRAINED_EMB = False \n",
    "USE_SCHEDULER = True\n",
    "SCHEDULER_TYPE = \"warmup_cosine_with_restarts\"\n",
    "TEACHER_FORCING_RATIO = 0.75\n",
    "NUM_CYCLES = 3\n",
    "MAX_PLATEAU_COUNT = 5\n",
    "\n",
    "\n",
    "# model_dir = \"../Model\"\n",
    "datafilter = \"../dataft\"\n",
    "os.makedirs(datafilter, exist_ok=True)\n",
    "# os.makedirs(model_dir, exist_ok=True)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"../dataset/train.csv\")\n",
    "validation_data = pd.read_csv(\"../dataset/validation.csv\")\n",
    "test_data = pd.read_csv(\"../dataset/test.csv\")\n",
    "\n",
    "# add col\n",
    "train_data.rename(columns={\"highlights\": \"summaries\", \"article\":\"articles\"}, inplace=True)\n",
    "validation_data.rename(columns={\"highlights\": \"summaries\",\"article\":\"articles\"}, inplace=True)\n",
    "test_data.rename(columns={\"highlights\": \"summaries\", \"article\":\"articles\"}, inplace=True)\n",
    "\n",
    "train_data[\"article_word_count\"] = train_data[\"articles\"].astype(str).apply(lambda x: len(x.split()))\n",
    "train_data[\"summary_word_count\"] = train_data[\"summaries\"].astype(str).apply(lambda x: len(x.split()))\n",
    "\n",
    "validation_data[\"article_word_count\"] = validation_data[\"articles\"].astype(str).apply(lambda x: len(x.split()))\n",
    "validation_data[\"summary_word_count\"] = validation_data[\"summaries\"].astype(str).apply(lambda x: len(x.split()))\n",
    "\n",
    "test_data[\"article_word_count\"] = test_data[\"articles\"].astype(str).apply(lambda x: len(x.split()))\n",
    "test_data[\"summary_word_count\"] = test_data[\"summaries\"].astype(str).apply(lambda x: len(x.split()))\n",
    "\n",
    "# filter range\n",
    "train_data = train_data[\n",
    "    (train_data[\"article_word_count\"] <= MAX_LENGTH_ARTICLE) &\n",
    "    (train_data[\"article_word_count\"] >= MIN_LENGTH_ARTICLE) &\n",
    "    (train_data[\"summary_word_count\"] <= MAX_LENGTH_SUMMARY) &\n",
    "    (train_data[\"summary_word_count\"] >= MIN_LENGTH_SUMMARY)\n",
    "]\n",
    "\n",
    "validation_data = validation_data[\n",
    "    (validation_data[\"article_word_count\"] <= MAX_LENGTH_ARTICLE) &\n",
    "    (validation_data[\"article_word_count\"] >= MIN_LENGTH_ARTICLE) &\n",
    "    (validation_data[\"summary_word_count\"] <= MAX_LENGTH_SUMMARY) &\n",
    "    (validation_data[\"summary_word_count\"] >= MIN_LENGTH_SUMMARY)\n",
    "]\n",
    "test_data = test_data[\n",
    "    (test_data[\"article_word_count\"] <= MAX_LENGTH_ARTICLE) &\n",
    "    (test_data[\"article_word_count\"] >= MIN_LENGTH_ARTICLE) &\n",
    "    (test_data[\"summary_word_count\"] <= MAX_LENGTH_SUMMARY) &\n",
    "    (test_data[\"summary_word_count\"] >= MIN_LENGTH_SUMMARY)\n",
    "]\n",
    "\n",
    "train_sample = train_data.sample(frac=FRAC_SAMPLE, random_state=1)\n",
    "validation_sample = validation_data.sample(frac=FRAC_SAMPLE, random_state=1)\n",
    "test_sample = test_data.sample(frac=0.01, random_state=1)\n",
    "\n",
    "train_sample.to_csv(os.path.join(datafilter,\"train_sample.csv\"), index=False)\n",
    "test_sample.to_csv(os.path.join(datafilter,\"test_sample.csv\"), index=False)\n",
    "validation_sample.to_csv(os.path.join(datafilter,\"validation_sample.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 960 entries, 144417 to 108633\n",
      "Data columns (total 5 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   id                  960 non-null    object\n",
      " 1   articles            960 non-null    object\n",
      " 2   summaries           960 non-null    object\n",
      " 3   article_word_count  960 non-null    int64 \n",
      " 4   summary_word_count  960 non-null    int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 45.0+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 42 entries, 9204 to 2968\n",
      "Data columns (total 5 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   id                  42 non-null     object\n",
      " 1   articles            42 non-null     object\n",
      " 2   summaries           42 non-null     object\n",
      " 3   article_word_count  42 non-null     int64 \n",
      " 4   summary_word_count  42 non-null     int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 2.0+ KB\n"
     ]
    }
   ],
   "source": [
    "train_sample.info()\n",
    "test_sample.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained tokenizer\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
    "\n",
    "vocab_size = len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 50265\n",
      "PAD token ID: 1\n",
      "UNK token ID: 3\n",
      "SOS token ID: 0\n",
      "EOS token ID: 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "unk_token_id = tokenizer.unk_token_id\n",
    "sos_token_id = tokenizer.bos_token_id\n",
    "eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"PAD token ID: {pad_token_id}\")\n",
    "print(f\"UNK token ID: {unk_token_id}\")\n",
    "print(f\"SOS token ID: {sos_token_id}\")\n",
    "print(f\"EOS token ID: {eos_token_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqDataset(Dataset):\n",
    "    def __init__(self, articles, summaries, tokenizer, max_len_article=MAX_LENGTH_ARTICLE, max_len_summary=MAX_LENGTH_SUMMARY):\n",
    "        self.articles = articles\n",
    "        self.summaries = summaries\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len_article = max_len_article\n",
    "        self.max_len_summary = max_len_summary\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.articles)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        article = str(self.articles[idx])\n",
    "        summary = str(self.summaries[idx])\n",
    "\n",
    "        article_encoding = self.tokenizer(\n",
    "            article,\n",
    "            max_length=self.max_len_article,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        summary_encoding = self.tokenizer(\n",
    "            summary,\n",
    "            max_length=self.max_len_summary,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'article': article_encoding['input_ids'].squeeze(0), # Remove batch dimension\n",
    "            'article_attention_mask': article_encoding['attention_mask'].squeeze(0), # Remove batch dimension\n",
    "            'summary': summary_encoding['input_ids'].squeeze(0), # Remove batch dimension\n",
    "            'summary_attention_mask': summary_encoding['attention_mask'].squeeze(0) # Remove batch dimension\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    # Batch is a list of dictionaries from __getitem__\n",
    "    return {\n",
    "        'article': torch.stack([item['article'] for item in batch]),\n",
    "        'article_attention_mask': torch.stack([item['article_attention_mask'] for item in batch]),\n",
    "        'summary': torch.stack([item['summary'] for item in batch]),\n",
    "        'summary_attention_mask': torch.stack([item['summary_attention_mask'] for item in batch])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader setup\n",
    "\n",
    "train_dataset = Seq2SeqDataset(train_sample['articles'].tolist(), train_sample['summaries'].tolist(), tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "validation_dataset= Seq2SeqDataset(validation_sample['articles'].tolist(), validation_sample['summaries'].tolist(), tokenizer)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = [seq_len, batch_size, d_model]\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hid_dim, n_layers, n_heads, pf_dim, dropout, device, max_length=512):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, hid_dim)\n",
    "        self.pos_encoder = PositionalEncoding(hid_dim, dropout, max_length)\n",
    "\n",
    "        encoder_layers = nn.TransformerEncoderLayer(\n",
    "            d_model=hid_dim, \n",
    "            nhead=n_heads, \n",
    "            dim_feedforward=pf_dim, \n",
    "            dropout=dropout,\n",
    "            batch_first=False\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, n_layers)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "\n",
    "    def forward(self, src, src_mask=None):\n",
    "        # src = [batch_size, src_len]\n",
    "        # src_mask = [batch_size, src_len] (boolean mask, True for padding positions)\n",
    "        \n",
    "        # Convert to [src_len, batch_size]\n",
    "        src = src.transpose(0, 1)  \n",
    "        \n",
    "        # Embedding and positional encoding\n",
    "        # [src_len, batch_size, hid_dim]\n",
    "        embedded = self.dropout(self.embedding(src) * self.scale)\n",
    "        src = self.pos_encoder(embedded)\n",
    "        \n",
    "        # Create src_key_padding_mask for transformer\n",
    "        src_key_padding_mask = src_mask if src_mask is not None else None\n",
    "        \n",
    "        # Pass through transformer encoder\n",
    "        # [src_len, batch_size, hid_dim]\n",
    "        encoder_output = self.transformer_encoder(src, src_key_padding_mask=src_key_padding_mask)\n",
    "        \n",
    "        # Return to [batch_size, src_len, hid_dim]\n",
    "        return encoder_output.transpose(0, 1)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hid_dim, n_layers, n_heads, pf_dim, dropout, device, max_length=128):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, hid_dim)\n",
    "        self.pos_encoder = PositionalEncoding(hid_dim, dropout, max_length)\n",
    "\n",
    "        decoder_layers = nn.TransformerDecoderLayer(\n",
    "            d_model=hid_dim, \n",
    "            nhead=n_heads, \n",
    "            dim_feedforward=pf_dim, \n",
    "            dropout=dropout,\n",
    "            batch_first=False\n",
    "        )\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layers, n_layers)\n",
    "\n",
    "        self.fc_out = nn.Linear(hid_dim, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "\n",
    "    def forward(self, trg, memory, tgt_mask=None, memory_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "        # trg = [batch_size, trg_len]\n",
    "        # memory = [batch_size, src_len, hid_dim]\n",
    "        \n",
    "        # Convert to [trg_len, batch_size]\n",
    "        trg = trg.transpose(0, 1)\n",
    "        \n",
    "        # Convert memory to [src_len, batch_size, hid_dim]\n",
    "        memory = memory.transpose(0, 1)\n",
    "        \n",
    "        # Embedding and positional encoding\n",
    "        # [trg_len, batch_size, hid_dim]\n",
    "        embedded = self.dropout(self.embedding(trg) * self.scale)\n",
    "        trg = self.pos_encoder(embedded)\n",
    "        \n",
    "        # Pass through transformer decoder\n",
    "        # [trg_len, batch_size, hid_dim]\n",
    "        decoder_output = self.transformer_decoder(\n",
    "            trg, \n",
    "            memory, \n",
    "            tgt_mask=tgt_mask,\n",
    "            memory_mask=memory_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "            memory_key_padding_mask=memory_key_padding_mask\n",
    "        )\n",
    "        \n",
    "        # Output projection\n",
    "        # [trg_len, batch_size, vocab_size]\n",
    "        output = self.fc_out(decoder_output)\n",
    "        \n",
    "        # Return to [batch_size, trg_len, vocab_size]\n",
    "        return output.transpose(0, 1)\n",
    "\n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self, encoder, decoder, pad_idx, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.pad_idx = pad_idx\n",
    "        self.device = device\n",
    "        self.sos_idx = tokenizer.bos_token_id\n",
    "        self.eos_idx = tokenizer.eos_token_id\n",
    "\n",
    "    def make_src_mask(self, src):\n",
    "        src_mask = (src == self.pad_idx)\n",
    "        return src_mask\n",
    "    \n",
    "    def make_trg_mask(self, trg):\n",
    "        trg_key_padding_mask = (trg == self.pad_idx)\n",
    "        trg_len = trg.shape[1]\n",
    "        tgt_mask = torch.triu(torch.ones((trg_len, trg_len), device=self.device), diagonal=1).bool()\n",
    "        return tgt_mask, trg_key_padding_mask\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        tgt_mask, tgt_key_padding_mask = self.make_trg_mask(trg)\n",
    "        \n",
    "        encoder_output = self.encoder(src, src_mask)\n",
    "        output = self.decoder(\n",
    "            trg=trg, \n",
    "            memory=encoder_output, \n",
    "            tgt_mask=tgt_mask, \n",
    "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "            memory_key_padding_mask=src_mask\n",
    "        )\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def generate(self, input_ids, attention_mask=None, max_length=128, num_beams=4, length_penalty=2.0, early_stopping=True):\n",
    "        batch_size = input_ids.shape[0]\n",
    "        \n",
    "        # Encoder pass\n",
    "        encoder_output = self.encoder(input_ids, (input_ids == self.pad_idx))\n",
    "        \n",
    "        # Initialize decoder input with SOS tokens\n",
    "        decoder_input = torch.ones((batch_size, 1), dtype=torch.long, device=self.device) * self.sos_idx\n",
    "        \n",
    "        # Use beam search for generation\n",
    "        if num_beams > 1:\n",
    "            return self._generate_beam_search(\n",
    "                encoder_output=encoder_output,\n",
    "                encoder_mask=(input_ids == self.pad_idx),\n",
    "                start_token_id=self.sos_idx,\n",
    "                end_token_id=self.eos_idx,\n",
    "                max_length=max_length,\n",
    "                num_beams=num_beams,\n",
    "                length_penalty=length_penalty,\n",
    "                early_stopping=early_stopping\n",
    "            )\n",
    "        \n",
    "        # Greedy search as fallback\n",
    "        return self._generate_greedy(\n",
    "            encoder_output=encoder_output,\n",
    "            encoder_mask=(input_ids == self.pad_idx),\n",
    "            start_token_id=self.sos_idx,\n",
    "            end_token_id=self.eos_idx,\n",
    "            max_length=max_length\n",
    "        )\n",
    "    \n",
    "    def _generate_greedy(self, encoder_output, encoder_mask, start_token_id, end_token_id, max_length):\n",
    "        batch_size = encoder_output.shape[0]\n",
    "        \n",
    "        # Initialize decoder input with start token\n",
    "        decoder_input = torch.ones((batch_size, 1), dtype=torch.long, device=self.device) * start_token_id\n",
    "        \n",
    "        # Track which sequences are completed\n",
    "        completed_sequences = torch.zeros(batch_size, dtype=torch.bool, device=self.device)\n",
    "        \n",
    "        for _ in range(max_length - 1):\n",
    "            # Make target mask for current decoder input\n",
    "            tgt_mask, tgt_key_padding_mask = self.make_trg_mask(decoder_input)\n",
    "            \n",
    "            # Decode one step\n",
    "            decoder_output = self.decoder(\n",
    "                trg=decoder_input,\n",
    "                memory=encoder_output,\n",
    "                tgt_mask=tgt_mask,\n",
    "                tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                memory_key_padding_mask=encoder_mask\n",
    "            )\n",
    "            \n",
    "            # Get next token prediction\n",
    "            next_token_logits = decoder_output[:, -1, :]\n",
    "            next_token = next_token_logits.argmax(dim=-1, keepdim=True)\n",
    "            \n",
    "            # Concatenate to decoder input\n",
    "            decoder_input = torch.cat([decoder_input, next_token], dim=1)\n",
    "            \n",
    "            # Check for EOS\n",
    "            completed_sequences = completed_sequences | (next_token.squeeze(-1) == end_token_id)\n",
    "            if completed_sequences.all():\n",
    "                break\n",
    "        \n",
    "        return decoder_input\n",
    "    \n",
    "    def _generate_beam_search(self, encoder_output, encoder_mask, start_token_id, end_token_id, max_length, num_beams, length_penalty, early_stopping):\n",
    "        batch_size = encoder_output.shape[0]\n",
    "        # Expand encoder output for beam search\n",
    "        # [batch_size, seq_len, hidden] -> [batch_size * num_beams, seq_len, hidden]\n",
    "        encoder_output = encoder_output.unsqueeze(1).expand(-1, num_beams, -1, -1).reshape(batch_size * num_beams, -1, encoder_output.shape[-1])\n",
    "\n",
    "        # [batch_size, seq_len] -> [batch_size * num_beams, seq_len]\n",
    "        encoder_mask = encoder_mask.unsqueeze(1).expand(-1, num_beams, -1).reshape(batch_size * num_beams, -1)\n",
    "        \n",
    "        # Start with beams of SOS tokens\n",
    "        current_tokens = torch.ones((batch_size * num_beams, 1), dtype=torch.long, device=self.device) * start_token_id\n",
    "        \n",
    "        # Track beam scores\n",
    "        beam_scores = torch.zeros((batch_size, num_beams), device=self.device)\n",
    "        beam_scores[:, 1:] = float('-inf')  # Only first beam is active initially\n",
    "        beam_scores = beam_scores.view(-1)  # [batch_size * num_beams]\n",
    "        \n",
    "        # Track completed sequences and scores\n",
    "        done_sequences = []\n",
    "        done_scores = []\n",
    "        done = [False for _ in range(batch_size)]\n",
    "        \n",
    "        for step in range(max_length - 1):\n",
    "            # Make target mask for current decoder input\n",
    "            tgt_mask, tgt_key_padding_mask = self.make_trg_mask(current_tokens)\n",
    "            \n",
    "            # Get decoder output\n",
    "            decoder_output = self.decoder(\n",
    "                trg=current_tokens,\n",
    "                memory=encoder_output,\n",
    "                tgt_mask=tgt_mask,\n",
    "                tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                memory_key_padding_mask=encoder_mask\n",
    "            )\n",
    "            \n",
    "            # Get log probabilities for next token\n",
    "            vocab_size = decoder_output.shape[-1]\n",
    "            next_token_logits = decoder_output[:, -1, :]\n",
    "            logprobs = torch.nn.functional.log_softmax(next_token_logits, dim=-1)  # [batch_size * num_beams, vocab_size]\n",
    "            \n",
    "            # Calculate new scores\n",
    "            # [batch_size * num_beams, vocab_size]\n",
    "            next_scores = beam_scores.unsqueeze(1) + logprobs\n",
    "            next_scores = next_scores.view(batch_size, num_beams * vocab_size)\n",
    "            \n",
    "            # Get the best 2*num_beams candidates\n",
    "            # [batch_size, 2*num_beams]\n",
    "            topk_scores, topk_indices = next_scores.topk(2 * num_beams, dim=1, largest=True, sorted=True)\n",
    "            \n",
    "            # Process each batch item\n",
    "            next_tokens = []\n",
    "            next_scores = []\n",
    "            \n",
    "            for batch_idx in range(batch_size):\n",
    "                # Skip if this batch item is already done\n",
    "                if done[batch_idx]:\n",
    "                    next_tokens.extend([current_tokens[batch_idx * num_beams] for _ in range(num_beams)])\n",
    "                    next_scores.extend([beam_scores[batch_idx * num_beams] for _ in range(num_beams)])\n",
    "                    continue\n",
    "                \n",
    "                # Find which beams and tokens to keep\n",
    "                beam_indices = []\n",
    "                token_indices = []\n",
    "                \n",
    "                for score_idx, (token_idx, score) in enumerate(zip(topk_indices[batch_idx], topk_scores[batch_idx])):\n",
    "                    beam_idx = token_idx // vocab_size\n",
    "                    token = token_idx % vocab_size\n",
    "                    \n",
    "                    if len(beam_indices) < num_beams:\n",
    "                        beam_indices.append(beam_idx)\n",
    "                        token_indices.append(token)\n",
    "                \n",
    "                # Create new token sequences\n",
    "                for beam_idx, token in zip(beam_indices, token_indices):\n",
    "                    token_seq = current_tokens[batch_idx * num_beams + beam_idx].clone()\n",
    "                    # Add new token\n",
    "                    new_seq = torch.cat([token_seq, token.unsqueeze(0)], dim=0)\n",
    "                    next_tokens.append(new_seq)\n",
    "                    \n",
    "                    # Apply length penalty\n",
    "                    lp = ((5 + len(new_seq)) / 6) ** length_penalty\n",
    "                    next_scores.append(beam_scores[batch_idx * num_beams + beam_idx] + logprobs[batch_idx * num_beams + beam_idx, token] / lp)\n",
    "                    \n",
    "                    # Check if sequence is done\n",
    "                    if token == end_token_id:\n",
    "                        done_sequences.append(new_seq)\n",
    "                        done_scores.append(next_scores[-1])\n",
    "                        \n",
    "                # Check if all sequences for this batch are done\n",
    "                if len(done_sequences) >= num_beams and early_stopping:\n",
    "                    done[batch_idx] = True\n",
    "\n",
    "            current_tokens = torch.stack(next_tokens).view(batch_size * num_beams, -1)\n",
    "            beam_scores = torch.tensor(next_scores, device=self.device).view(batch_size * num_beams)\n",
    "\n",
    "            if all(done):\n",
    "                break\n",
    "            \n",
    "        if not done_sequences:\n",
    "            done_sequences = [current_tokens[i] for i in range(batch_size * num_beams)]\n",
    "            done_scores = beam_scores.tolist()\n",
    "\n",
    "        result = []\n",
    "        for i in range(batch_size):\n",
    "            # Find best sequence for this batch\n",
    "            best_seq = None\n",
    "            best_score = float('-inf')\n",
    "            \n",
    "            for seq, score in zip(done_sequences, done_scores):\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_seq = seq\n",
    "            \n",
    "            result.append(best_seq)\n",
    "        \n",
    "        return torch.stack(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vuda/miniconda3/lib/python3.12/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 89,880,153 trainable parameters\n",
      "GPU Memory allocated: 344.12 MB\n",
      "GPU Memory reserved: 364.00 MB\n",
      "Model architecture:\n",
      "Seq2SeqTransformer(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(50265, 512)\n",
      "    (pos_encoder): PositionalEncoding(\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer_encoder): TransformerEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-2): 3 x TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (embedding): Embedding(50265, 512)\n",
      "    (pos_encoder): PositionalEncoding(\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer_decoder): TransformerDecoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-2): 3 x TransformerDecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fc_out): Linear(in_features=512, out_features=50265, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "PAD_IDX = tokenizer.pad_token_id\n",
    "UNK_IDX = tokenizer.unk_token_id\n",
    "SOS_IDX = tokenizer.bos_token_id\n",
    "EOS_IDX = tokenizer.eos_token_id\n",
    "\n",
    "ENC_LAYERS = 3\n",
    "DEC_LAYERS = 3\n",
    "ENC_HEADS = 8\n",
    "DEC_HEADS = 8\n",
    "ENC_PF_DIM = 512 \n",
    "DEC_PF_DIM = 512\n",
    "ENC_DROPOUT = 0.1\n",
    "DEC_DROPOUT = 0.1\n",
    "\n",
    "\n",
    "encoder = Encoder(vocab_size, HIDDEN_DIM, ENC_LAYERS, ENC_HEADS, ENC_PF_DIM, ENC_DROPOUT, device, MAX_LENGTH_ARTICLE)\n",
    "decoder = Decoder(vocab_size, HIDDEN_DIM, DEC_LAYERS, DEC_HEADS, DEC_PF_DIM, DEC_DROPOUT, device, MAX_LENGTH_SUMMARY)\n",
    "\n",
    "model = Seq2SeqTransformer(encoder, decoder, PAD_IDX, device).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX)\n",
    "\n",
    "#schesuler\n",
    "total_steps = len(train_loader) * NUM_EPOCHS\n",
    "warmup_steps = int(0.1 * total_steps) # 10% of total steps for warmup\n",
    "\n",
    "def linear_warmup_decay(step, warmup_steps, total_steps):\n",
    "    if step < warmup_steps:\n",
    "        return (step + 1) / (warmup_steps + 1)\n",
    "    else:\n",
    "        return max(1e-7, (total_steps - step) / (total_steps - warmup_steps))\n",
    "\n",
    "def warmup_cosine_with_restarts(step, warmup_steps, total_steps, num_cycles=1):\n",
    "    if step < warmup_steps:\n",
    "        return (step + 1) / (warmup_steps + 1)\n",
    "    else:\n",
    "        progress = (step - warmup_steps) / (total_steps - warmup_steps)\n",
    "        cycle_progress = progress * num_cycles % 1\n",
    "        return max(1e-7, 0.5 * (1 + math.cos(math.pi * cycle_progress)))\n",
    "\n",
    "def get_scheduler(optimizer, total_steps, warmup_steps, num_cycles=None, types='warmup_cosine_with_restarts'):\n",
    "    if types == 'warmup_cosine_with_restarts':\n",
    "        assert num_cycles != None, 'must specify num_cycles when types=\"warmup_cosine_with_restarts\"'\n",
    "        return torch.optim.lr_scheduler.LambdaLR(\n",
    "            optimizer,\n",
    "            lr_lambda=lambda step: warmup_cosine_with_restarts(\n",
    "                step, warmup_steps, total_steps, num_cycles=num_cycles)\n",
    "        )\n",
    "    elif types == 'linear_warmup_decay':\n",
    "        return torch.optim.lr_scheduler.LambdaLR(\n",
    "            optimizer,\n",
    "            lr_lambda=lambda step: linear_warmup_decay(step, warmup_steps, total_steps)\n",
    "        )\n",
    "    else:\n",
    "        raise Exception('not implemented')\n",
    "\n",
    "scheduler = get_scheduler(\n",
    "    optimizer,\n",
    "    total_steps=total_steps,\n",
    "    warmup_steps=warmup_steps,\n",
    "    num_cycles=NUM_CYCLES, # Used for warmup_cosine_with_restarts\n",
    "    types=SCHEDULER_TYPE\n",
    ")\n",
    "\n",
    "print(f\"The model has {sum(p.numel() for p in model.parameters() if p.requires_grad):,} trainable parameters\")\n",
    "print(f\"GPU Memory allocated: {torch.cuda.memory_allocated()/1024**2:.2f} MB\")\n",
    "print(f\"GPU Memory reserved: {torch.cuda.memory_reserved()/1024**2:.2f} MB\")\n",
    "print(\"Model architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvubkk67\u001b[0m (\u001b[33mvubkk67-hanoi-university-of-science-and-technology\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/vuda/Text-Summarization/Abstractive/wandb/run-20250529_201754-elhc6sm9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vubkk67-hanoi-university-of-science-and-technology/Seq2Seq-Summarization/runs/elhc6sm9' target=\"_blank\">transformer-scratch-20250529-201753</a></strong> to <a href='https://wandb.ai/vubkk67-hanoi-university-of-science-and-technology/Seq2Seq-Summarization' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vubkk67-hanoi-university-of-science-and-technology/Seq2Seq-Summarization' target=\"_blank\">https://wandb.ai/vubkk67-hanoi-university-of-science-and-technology/Seq2Seq-Summarization</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vubkk67-hanoi-university-of-science-and-technology/Seq2Seq-Summarization/runs/elhc6sm9' target=\"_blank\">https://wandb.ai/vubkk67-hanoi-university-of-science-and-technology/Seq2Seq-Summarization/runs/elhc6sm9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# W&B setup\n",
    "wandb.init(\n",
    "    project=\"Seq2Seq-Summarization\",\n",
    "    name=f\"transformer-scratch-{datetime.now().strftime('%Y%m%d-%H%M%S')}\",\n",
    "    config={\n",
    "        \"model\": \"Seq2Seq-Transformer-Scratch\",\n",
    "        \"hidden_dim\": HIDDEN_DIM,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"num_epochs\": NUM_EPOCHS,\n",
    "        \"encoder_layers\": ENC_LAYERS,\n",
    "        \"decoder_layers\": DEC_LAYERS,\n",
    "        \"encoder_heads\": ENC_HEADS,\n",
    "        \"decoder_heads\": DEC_HEADS,\n",
    "        \"encoder_pf_dim\": ENC_PF_DIM,\n",
    "        \"decoder_pf_dim\": DEC_PF_DIM,\n",
    "        \"encoder_dropout\": ENC_DROPOUT,\n",
    "        \"decoder_dropout\": DEC_DROPOUT,\n",
    "        \"weight_decay\": WEIGHT_DECAY,\n",
    "        \"clip\": CLIP,\n",
    "        \"scheduler_type\": SCHEDULER_TYPE,\n",
    "        \"warmup_steps\": warmup_steps,\n",
    "        \"num_cycles\": NUM_CYCLES, # Relevant for cosine scheduler\n",
    "        \"vocab_size\": vocab_size,\n",
    "        \"max_length_article\": MAX_LENGTH_ARTICLE,\n",
    "        \"max_length_summary\": MAX_LENGTH_SUMMARY,\n",
    "        \"teacher_forcing_ratio\": TEACHER_FORCING_RATIO # Added teacher forcing ratio\n",
    "    }\n",
    ")\n",
    "\n",
    "wandb.watch(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cee5e24bc4a743b5bc253f8e041a7c42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1 [Train]:   0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "849c2677e4ca4eccb02184aaa443330f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1 [Val]:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved best model and tokenizer to `transformer_scratch_best_model` at epoch 1\n",
      "Epoch 01 | Train Loss: 4.2372 | Val Loss: 4.4404 | LR: 0.000050 | Time: 29.65s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "154e6b9ac6da4d70b50fd9f4d552d396",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.008 MB of 0.008 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁</td></tr><tr><td>lr</td><td>▁</td></tr><tr><td>train_loss</td><td>▁</td></tr><tr><td>val_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_loss</td><td>inf</td></tr><tr><td>epoch</td><td>1</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>train_loss</td><td>4.2372</td></tr><tr><td>val_loss</td><td>4.44043</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">transformer-scratch-20250529-201753</strong> at: <a href='https://wandb.ai/vubkk67-hanoi-university-of-science-and-technology/Seq2Seq-Summarization/runs/elhc6sm9' target=\"_blank\">https://wandb.ai/vubkk67-hanoi-university-of-science-and-technology/Seq2Seq-Summarization/runs/elhc6sm9</a><br/> View project at: <a href='https://wandb.ai/vubkk67-hanoi-university-of-science-and-technology/Seq2Seq-Summarization' target=\"_blank\">https://wandb.ai/vubkk67-hanoi-university-of-science-and-technology/Seq2Seq-Summarization</a><br/>Synced 4 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250529_201754-elhc6sm9/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save best model directory\n",
    "# save_dir = os.path.join(model_dir, 'transformer_scratch_best_model')\n",
    "save_dir = 'transformer_scratch_best_model'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Training loop\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    start_time = time.time()\n",
    "\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    progress_bar_train = tqdm(train_loader, desc=f\"Epoch {epoch+1} [Train]\")\n",
    "    for batch in progress_bar_train:\n",
    "        src = batch['article'].to(device)\n",
    "        trg = batch['summary'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # The target input for the decoder should exclude the <eos> token\n",
    "        # Pass the correct masks: trg_lookahead_mask, src_padding_mask, trg_padding_mask\n",
    "        output = model(src, trg[:, :-1])\n",
    "\n",
    "        # output = [batch_size, trg_len - 1, output_dim]\n",
    "        # trg = [batch_size, trg_len]\n",
    "\n",
    "        output_dim = output.shape[-1]\n",
    "\n",
    "        # Reshape for loss calculation\n",
    "        output = output.contiguous().view(-1, output_dim)\n",
    "        trg = trg[:, 1:].contiguous().view(-1) # The target output should exclude the <sos> token\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "\n",
    "        # Check for NaN loss\n",
    "        if torch.isnan(loss):\n",
    "            print(\"NaN loss detected! Stopping training.\")\n",
    "            break # Exit the inner batch loop\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        progress_bar_train.set_postfix(loss=loss.item(), lr=optimizer.param_groups[0]['lr'])\n",
    "\n",
    "    # If NaN loss was detected in the inner loop, break the outer epoch loop as well\n",
    "    if torch.isnan(torch.tensor(train_loss)):\n",
    "        break\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        progress_bar_val = tqdm(validation_loader, desc=f\"Epoch {epoch+1} [Val]\")\n",
    "        for batch in progress_bar_val:\n",
    "            src = batch['article'].to(device)\n",
    "            trg = batch['summary'].to(device)\n",
    "\n",
    "            # The target input for the decoder should exclude the <eos> token\n",
    "            # Pass the correct masks: trg_lookahead_mask, src_padding_mask, trg_padding_mask\n",
    "            output = model(src, trg[:, :-1])\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "\n",
    "            # Reshape for loss calculation\n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            trg = trg[:, 1:].contiguous().view(-1) # The target output should exclude the <sos> token\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            progress_bar_val.set_postfix(loss=loss.item())\n",
    "\n",
    "        val_loss /= len(validation_loader)\n",
    "        current_lr = optimizer.param_groups[0]['lr'] # Get current LR after scheduler step\n",
    "\n",
    "    # W&B log\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"train_loss\": train_loss,\n",
    "        \"val_loss\": val_loss,\n",
    "        \"lr\": current_lr,\n",
    "        \"best_val_loss\": best_val_loss # Log best val loss seen so far\n",
    "    })\n",
    "\n",
    "    # Save best model and tokenizer\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), os.path.join(save_dir, 'best_transformer_model.pt'))\n",
    "        tokenizer.save_pretrained(save_dir)\n",
    "        print(f\"Saved best model and tokenizer to `{save_dir}` at epoch {epoch+1}\")\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch+1:02d} | \"\n",
    "        f\"Train Loss: {train_loss:.4f} | \"\n",
    "        f\"Val Loss: {val_loss:.4f} | \"\n",
    "        f\"LR: {current_lr:.6f} | \"\n",
    "        f\"Time: {time.time() - start_time:.2f}s\"\n",
    "    )\n",
    "\n",
    "# W&B end\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"../Model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Model/transformer_scratch_best_model\n"
     ]
    }
   ],
   "source": [
    "output_path = os.path.join(datafilter, \"test_pred_6.csv\") \n",
    "save_dir = os.path.join(model_dir, 'transformer_scratch_best_model')\n",
    "print(save_dir)\n",
    "tokenizer = BartTokenizer.from_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vuda/miniconda3/lib/python3.12/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "test_dataset = Seq2SeqDataset(test_sample['articles'].tolist(), test_sample['summaries'].tolist(), tokenizer)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "PAD_IDX = tokenizer.pad_token_id\n",
    "UNK_IDX = tokenizer.unk_token_id\n",
    "SOS_IDX = tokenizer.bos_token_id\n",
    "EOS_IDX = tokenizer.eos_token_id\n",
    "encoder = Encoder(vocab_size, HIDDEN_DIM, ENC_LAYERS, ENC_HEADS, ENC_PF_DIM, ENC_DROPOUT, device, MAX_LENGTH_ARTICLE)\n",
    "decoder = Decoder(vocab_size, HIDDEN_DIM, DEC_LAYERS, DEC_HEADS, DEC_PF_DIM, DEC_DROPOUT, device, MAX_LENGTH_SUMMARY)\n",
    "\n",
    "model = Seq2SeqTransformer(encoder, decoder, PAD_IDX, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6272/3607753624.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(os.path.join(save_dir, 'best_transformer_model.pt'), map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating summaries...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ef695fa56cb4ec5bb21f55236fbf3f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating summaries:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 42 summaries\n",
      "File has been saved at: ../dataft/test_pred_6.csv\n"
     ]
    }
   ],
   "source": [
    "# Load the model from saved state\n",
    "model.load_state_dict(torch.load(os.path.join(save_dir, 'best_transformer_model.pt'), map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"Generating summaries...\")\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Generating summaries\"):\n",
    "        input_ids = batch['article'].to(device)\n",
    "        attention_mask = batch['article_attention_mask'].to(device)\n",
    "        \n",
    "        # Instead of using beam search which has issues, use greedy search\n",
    "        # by setting num_beams=1\n",
    "        output_ids = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=MAX_LENGTH_SUMMARY,\n",
    "            num_beams=1,  # Use greedy search instead of beam search\n",
    "            length_penalty=2.0,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        batch_preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "        predictions.extend(batch_preds)\n",
    "\n",
    "# Save predictions\n",
    "print(f\"Generated {len(predictions)} summaries\")\n",
    "test_sample = test_sample.iloc[:len(predictions)].copy()\n",
    "test_sample[\"predicted_summary\"] = predictions\n",
    "test_sample.to_csv(output_path, index=False)\n",
    "print(f\"File has been saved at: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating evaluation metrics...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>articles</th>\n",
       "      <th>summaries</th>\n",
       "      <th>predicted_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Florida bus passenger was arrested for throw...</td>\n",
       "      <td>Joel Parker, 33, was riding the bus in St John...</td>\n",
       "      <td>The driver was arrested on suspicion of assaul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aston Villa may be able to sign Cordoba strike...</td>\n",
       "      <td>Aston Villa have held talks over Cordoba strik...</td>\n",
       "      <td>Manchester City midfielder has been linked wit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            articles  \\\n",
       "0  A Florida bus passenger was arrested for throw...   \n",
       "1  Aston Villa may be able to sign Cordoba strike...   \n",
       "\n",
       "                                           summaries  \\\n",
       "0  Joel Parker, 33, was riding the bus in St John...   \n",
       "1  Aston Villa have held talks over Cordoba strik...   \n",
       "\n",
       "                                   predicted_summary  \n",
       "0  The driver was arrested on suspicion of assaul...  \n",
       "1  Manchester City midfielder has been linked wit...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE scores:\n",
      "ROUGE-1: 0.1690\n",
      "ROUGE-2: 0.0234\n",
      "ROUGE-L: 0.1623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTScore:\n",
      "Precision: 0.8243\n",
      "Recall:    0.8181\n",
      "F1:        0.8210\n",
      "METEOR Score (average):\n",
      "METEOR: 0.1245\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Calculate metrics\n",
    "print(\"Calculating evaluation metrics...\")\n",
    "test_pred = pd.read_csv(output_path)\n",
    "display(test_pred[[\"articles\",\"summaries\", \"predicted_summary\"]].head(2))\n",
    "\n",
    "# Check required columns\n",
    "if \"summaries\" in test_pred.columns and \"predicted_summary\" in test_pred.columns:\n",
    "    references = test_pred[\"summaries\"].fillna(\"<empty>\").astype(str).tolist()\n",
    "    predictions = test_pred[\"predicted_summary\"].fillna(\"<empty>\").astype(str).tolist()\n",
    "\n",
    "    # Filter valid pairs\n",
    "    valid_pairs = [\n",
    "        (pred, ref) for pred, ref in zip(predictions, references)\n",
    "        if pred.strip() and pred != \"<empty>\" and ref.strip()\n",
    "    ]\n",
    "    \n",
    "    if not valid_pairs:\n",
    "        print(\"No valid pairs found for metric calculation.\")\n",
    "    else:\n",
    "        filtered_preds, filtered_refs = zip(*valid_pairs)\n",
    "\n",
    "        # ROUGE\n",
    "        rouge = Rouge()\n",
    "        rouge_scores = rouge.get_scores(filtered_preds, filtered_refs, avg=True)\n",
    "        print(\"ROUGE scores:\")\n",
    "        print(f\"ROUGE-1: {rouge_scores['rouge-1']['f']:.4f}\")\n",
    "        print(f\"ROUGE-2: {rouge_scores['rouge-2']['f']:.4f}\")\n",
    "        print(f\"ROUGE-L: {rouge_scores['rouge-l']['f']:.4f}\")\n",
    "\n",
    "        # BERTScore\n",
    "        P, R, F1 = bert_score(filtered_preds, filtered_refs, lang=\"en\", verbose=False)\n",
    "        print(\"BERTScore:\")\n",
    "        print(f\"Precision: {P.mean().item():.4f}\")\n",
    "        print(f\"Recall:    {R.mean().item():.4f}\")\n",
    "        print(f\"F1:        {F1.mean().item():.4f}\")\n",
    "\n",
    "        # METEOR\n",
    "        print(\"METEOR Score (average):\")\n",
    "        meteor_scores = [single_meteor_score(ref.split(), pred.split()) \n",
    "                        for pred, ref in zip(filtered_preds, filtered_refs)]\n",
    "        print(f\"METEOR: {sum(meteor_scores)/len(meteor_scores):.4f}\")\n",
    "\n",
    "else:\n",
    "    print(\"Could not find required columns 'summaries' and 'predicted_summary' for metric calculation.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
