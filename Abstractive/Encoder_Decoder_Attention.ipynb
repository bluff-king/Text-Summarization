{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "import wandb\n",
    "import time\n",
    "import os\n",
    "\n",
    "from rouge import Rouge\n",
    "from bert_score import score as bert_score\n",
    "from nltk.translate.meteor_score import single_meteor_score\n",
    "# Import Hugging Face tokenizer\n",
    "from transformers import BartTokenizer\n",
    "\n",
    "# nltk.download('punkt') # No longer needed with HF tokenizer\n",
    "\n",
    "NUM_EPOCHS = 1\n",
    "BATCH_SIZE = 4\n",
    "FRAC_SAMPLE = 0.2\n",
    "MAX_LENGTH_ARTICLE = 512\n",
    "MIN_LENGTH_ARTICLE = 50 # Keep for data filtering, but tokenizer handles max length\n",
    "MAX_LENGTH_SUMMARY = 128\n",
    "MIN_LENGTH_SUMMARY = 20 # Keep for data filtering\n",
    "HIDDEN_DIM = 512\n",
    "LEARNING_RATE = 0.00005 # Reduced learning rate\n",
    "NUM_CYCLES = 3\n",
    "MAX_PLATEAU_COUNT = 5\n",
    "WEIGHT_DECAY = 1e-4\n",
    "CLIP = 1\n",
    "USE_PRETRAINED_EMB = False # Not using pre-trained GloVe embeddings anymore\n",
    "USE_SCHEDULER = True\n",
    "SCHEDULER_TYPE = \"warmup_cosine_with_restarts\"\n",
    "TEACHER_FORCING_RATIO = 0.75\n",
    "NUM_CYCLES = 3\n",
    "MAX_PLATEAU_COUNT = 5\n",
    "\n",
    "\n",
    "# model_dir = \"../Model\"\n",
    "datafilter = \"../dataft\"\n",
    "os.makedirs(datafilter, exist_ok=True)\n",
    "# os.makedirs(model_dir, exist_ok=True)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"../dataset/train.csv\")\n",
    "validation_data = pd.read_csv(\"../dataset/validation.csv\")\n",
    "test_data = pd.read_csv(\"../dataset/test.csv\")\n",
    "\n",
    "# add col\n",
    "train_data.rename(columns={\"highlights\": \"summaries\", \"article\":\"articles\"}, inplace=True)\n",
    "validation_data.rename(columns={\"highlights\": \"summaries\",\"article\":\"articles\"}, inplace=True)\n",
    "test_data.rename(columns={\"highlights\": \"summaries\", \"article\":\"articles\"}, inplace=True)\n",
    "\n",
    "train_data[\"article_word_count\"] = train_data[\"articles\"].astype(str).apply(lambda x: len(x.split()))\n",
    "train_data[\"summary_word_count\"] = train_data[\"summaries\"].astype(str).apply(lambda x: len(x.split()))\n",
    "\n",
    "validation_data[\"article_word_count\"] = validation_data[\"articles\"].astype(str).apply(lambda x: len(x.split()))\n",
    "validation_data[\"summary_word_count\"] = validation_data[\"summaries\"].astype(str).apply(lambda x: len(x.split()))\n",
    "\n",
    "test_data[\"article_word_count\"] = test_data[\"articles\"].astype(str).apply(lambda x: len(x.split()))\n",
    "test_data[\"summary_word_count\"] = test_data[\"summaries\"].astype(str).apply(lambda x: len(x.split()))\n",
    "\n",
    "# filter range\n",
    "train_data = train_data[\n",
    "    (train_data[\"article_word_count\"] <= MAX_LENGTH_ARTICLE) &\n",
    "    (train_data[\"article_word_count\"] >= MIN_LENGTH_ARTICLE) &\n",
    "    (train_data[\"summary_word_count\"] <= MAX_LENGTH_SUMMARY) &\n",
    "    (train_data[\"summary_word_count\"] >= MIN_LENGTH_SUMMARY)\n",
    "]\n",
    "\n",
    "validation_data = validation_data[\n",
    "    (validation_data[\"article_word_count\"] <= MAX_LENGTH_ARTICLE) &\n",
    "    (validation_data[\"article_word_count\"] >= MIN_LENGTH_ARTICLE) &\n",
    "    (validation_data[\"summary_word_count\"] <= MAX_LENGTH_SUMMARY) &\n",
    "    (validation_data[\"summary_word_count\"] >= MIN_LENGTH_SUMMARY)\n",
    "]\n",
    "test_data = test_data[\n",
    "    (test_data[\"article_word_count\"] <= MAX_LENGTH_ARTICLE) &\n",
    "    (test_data[\"article_word_count\"] >= MIN_LENGTH_ARTICLE) &\n",
    "    (test_data[\"summary_word_count\"] <= MAX_LENGTH_SUMMARY) &\n",
    "    (test_data[\"summary_word_count\"] >= MIN_LENGTH_SUMMARY)\n",
    "]\n",
    "\n",
    "train_sample = train_data.sample(frac=FRAC_SAMPLE, random_state=1)\n",
    "validation_sample = validation_data.sample(frac=FRAC_SAMPLE, random_state=1)\n",
    "test_sample = test_data.sample(frac=1, random_state=1)\n",
    "\n",
    "train_sample.to_csv(os.path.join(datafilter,\"train_sample.csv\"), index=False)\n",
    "test_sample.to_csv(os.path.join(datafilter,\"test_sample.csv\"), index=False)\n",
    "validation_sample.to_csv(os.path.join(datafilter,\"validation_sample.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 19198 entries, 144417 to 201560\n",
      "Data columns (total 5 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   id                  19198 non-null  object\n",
      " 1   articles            19198 non-null  object\n",
      " 2   summaries           19198 non-null  object\n",
      " 3   article_word_count  19198 non-null  int64 \n",
      " 4   summary_word_count  19198 non-null  int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 899.9+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 4224 entries, 9204 to 591\n",
      "Data columns (total 5 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   id                  4224 non-null   object\n",
      " 1   articles            4224 non-null   object\n",
      " 2   summaries           4224 non-null   object\n",
      " 3   article_word_count  4224 non-null   int64 \n",
      " 4   summary_word_count  4224 non-null   int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 198.0+ KB\n"
     ]
    }
   ],
   "source": [
    "train_sample.info()\n",
    "test_sample.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained tokenizer\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
    "\n",
    "vocab_size = len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 50265\n",
      "PAD token ID: 1\n",
      "UNK token ID: 3\n",
      "SOS token ID: 0\n",
      "EOS token ID: 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "unk_token_id = tokenizer.unk_token_id\n",
    "sos_token_id = tokenizer.bos_token_id\n",
    "eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"PAD token ID: {pad_token_id}\")\n",
    "print(f\"UNK token ID: {unk_token_id}\")\n",
    "print(f\"SOS token ID: {sos_token_id}\")\n",
    "print(f\"EOS token ID: {eos_token_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqDataset(Dataset):\n",
    "    def __init__(self, articles, summaries, tokenizer, max_len_article=MAX_LENGTH_ARTICLE, max_len_summary=MAX_LENGTH_SUMMARY):\n",
    "        self.articles = articles\n",
    "        self.summaries = summaries\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len_article = max_len_article\n",
    "        self.max_len_summary = max_len_summary\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.articles)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        article = str(self.articles[idx])\n",
    "        summary = str(self.summaries[idx])\n",
    "\n",
    "        # Tokenize and encode article\n",
    "        article_encoding = self.tokenizer(\n",
    "            article,\n",
    "            max_length=self.max_len_article,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # Tokenize and encode summary\n",
    "        summary_encoding = self.tokenizer(\n",
    "            summary,\n",
    "            max_length=self.max_len_summary,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'article': article_encoding['input_ids'].squeeze(0), # Remove batch dimension\n",
    "            'article_attention_mask': article_encoding['attention_mask'].squeeze(0), # Remove batch dimension\n",
    "            'summary': summary_encoding['input_ids'].squeeze(0), # Remove batch dimension\n",
    "            'summary_attention_mask': summary_encoding['attention_mask'].squeeze(0) # Remove batch dimension\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    # Batch is a list of dictionaries from __getitem__\n",
    "    return {\n",
    "        'article': torch.stack([item['article'] for item in batch]),\n",
    "        'article_attention_mask': torch.stack([item['article_attention_mask'] for item in batch]),\n",
    "        'summary': torch.stack([item['summary'] for item in batch]),\n",
    "        'summary_attention_mask': torch.stack([item['summary_attention_mask'] for item in batch])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader setup\n",
    "\n",
    "train_dataset = Seq2SeqDataset(train_sample['articles'].tolist(), train_sample['summaries'].tolist(), tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "validation_dataset= Seq2SeqDataset(validation_sample['articles'].tolist(), validation_sample['summaries'].tolist(), tokenizer)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = [seq_len, batch_size, d_model]\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hid_dim, n_layers, n_heads, pf_dim, dropout, device, max_length=512):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, hid_dim)\n",
    "        self.pos_encoder = PositionalEncoding(hid_dim, dropout, max_length)\n",
    "\n",
    "        encoder_layers = nn.TransformerEncoderLayer(\n",
    "            d_model=hid_dim, \n",
    "            nhead=n_heads, \n",
    "            dim_feedforward=pf_dim, \n",
    "            dropout=dropout,\n",
    "            batch_first=False\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, n_layers)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "\n",
    "    def forward(self, src, src_mask=None):\n",
    "        # src = [batch_size, src_len]\n",
    "        # src_mask = [batch_size, src_len] (boolean mask, True for padding positions)\n",
    "        \n",
    "        # Convert to [src_len, batch_size]\n",
    "        src = src.transpose(0, 1)  \n",
    "        \n",
    "        # Embedding and positional encoding\n",
    "        # [src_len, batch_size, hid_dim]\n",
    "        embedded = self.dropout(self.embedding(src) * self.scale)\n",
    "        src = self.pos_encoder(embedded)\n",
    "        \n",
    "        # Create src_key_padding_mask for transformer\n",
    "        # It should be a boolean mask where True indicates padding positions\n",
    "        src_key_padding_mask = src_mask if src_mask is not None else None\n",
    "        \n",
    "        # Pass through transformer encoder\n",
    "        # [src_len, batch_size, hid_dim]\n",
    "        encoder_output = self.transformer_encoder(src, src_key_padding_mask=src_key_padding_mask)\n",
    "        \n",
    "        # Return to [batch_size, src_len, hid_dim]\n",
    "        return encoder_output.transpose(0, 1)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hid_dim, n_layers, n_heads, pf_dim, dropout, device, max_length=128):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, hid_dim)\n",
    "        self.pos_encoder = PositionalEncoding(hid_dim, dropout, max_length)\n",
    "\n",
    "        decoder_layers = nn.TransformerDecoderLayer(\n",
    "            d_model=hid_dim, \n",
    "            nhead=n_heads, \n",
    "            dim_feedforward=pf_dim, \n",
    "            dropout=dropout,\n",
    "            batch_first=False\n",
    "        )\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layers, n_layers)\n",
    "\n",
    "        self.fc_out = nn.Linear(hid_dim, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "\n",
    "    def forward(self, trg, memory, tgt_mask=None, memory_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "        # trg = [batch_size, trg_len]\n",
    "        # memory = [batch_size, src_len, hid_dim]\n",
    "        \n",
    "        # Convert to [trg_len, batch_size]\n",
    "        trg = trg.transpose(0, 1)\n",
    "        \n",
    "        # Convert memory to [src_len, batch_size, hid_dim]\n",
    "        memory = memory.transpose(0, 1)\n",
    "        \n",
    "        # Embedding and positional encoding\n",
    "        # [trg_len, batch_size, hid_dim]\n",
    "        embedded = self.dropout(self.embedding(trg) * self.scale)\n",
    "        trg = self.pos_encoder(embedded)\n",
    "        \n",
    "        # Pass through transformer decoder\n",
    "        # [trg_len, batch_size, hid_dim]\n",
    "        decoder_output = self.transformer_decoder(\n",
    "            trg, \n",
    "            memory, \n",
    "            tgt_mask=tgt_mask,\n",
    "            memory_mask=memory_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "            memory_key_padding_mask=memory_key_padding_mask\n",
    "        )\n",
    "        \n",
    "        # Output projection\n",
    "        # [trg_len, batch_size, vocab_size]\n",
    "        output = self.fc_out(decoder_output)\n",
    "        \n",
    "        # Return to [batch_size, trg_len, vocab_size]\n",
    "        return output.transpose(0, 1)\n",
    "\n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self, encoder, decoder, pad_idx, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.pad_idx = pad_idx\n",
    "        self.device = device\n",
    "        self.sos_idx = tokenizer.bos_token_id\n",
    "        self.eos_idx = tokenizer.eos_token_id\n",
    "\n",
    "    def make_src_mask(self, src):\n",
    "        src_mask = (src == self.pad_idx)\n",
    "        return src_mask\n",
    "    \n",
    "    def make_trg_mask(self, trg):\n",
    "        trg_key_padding_mask = (trg == self.pad_idx)\n",
    "        trg_len = trg.shape[1]\n",
    "        tgt_mask = torch.triu(torch.ones((trg_len, trg_len), device=self.device), diagonal=1).bool()\n",
    "        return tgt_mask, trg_key_padding_mask\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        tgt_mask, tgt_key_padding_mask = self.make_trg_mask(trg)\n",
    "        \n",
    "        encoder_output = self.encoder(src, src_mask)\n",
    "        output = self.decoder(\n",
    "            trg=trg, \n",
    "            memory=encoder_output, \n",
    "            tgt_mask=tgt_mask, \n",
    "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "            memory_key_padding_mask=src_mask\n",
    "        )\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def generate(self, input_ids, attention_mask=None, max_length=128, num_beams=4, length_penalty=2.0, early_stopping=True):\n",
    "        batch_size = input_ids.shape[0]\n",
    "        \n",
    "        # Encoder pass\n",
    "        encoder_output = self.encoder(input_ids, (input_ids == self.pad_idx))\n",
    "        \n",
    "        # Initialize decoder input with SOS tokens\n",
    "        decoder_input = torch.ones((batch_size, 1), dtype=torch.long, device=self.device) * self.sos_idx\n",
    "        \n",
    "        # Use beam search for generation\n",
    "        if num_beams > 1:\n",
    "            return self._generate_beam_search(\n",
    "                encoder_output=encoder_output,\n",
    "                encoder_mask=(input_ids == self.pad_idx),\n",
    "                start_token_id=self.sos_idx,\n",
    "                end_token_id=self.eos_idx,\n",
    "                max_length=max_length,\n",
    "                num_beams=num_beams,\n",
    "                length_penalty=length_penalty,\n",
    "                early_stopping=early_stopping\n",
    "            )\n",
    "        \n",
    "        # Greedy search as fallback\n",
    "        return self._generate_greedy(\n",
    "            encoder_output=encoder_output,\n",
    "            encoder_mask=(input_ids == self.pad_idx),\n",
    "            start_token_id=self.sos_idx,\n",
    "            end_token_id=self.eos_idx,\n",
    "            max_length=max_length\n",
    "        )\n",
    "    \n",
    "    def _generate_greedy(self, encoder_output, encoder_mask, start_token_id, end_token_id, max_length):\n",
    "        batch_size = encoder_output.shape[0]\n",
    "        \n",
    "        # Initialize decoder input with start token\n",
    "        decoder_input = torch.ones((batch_size, 1), dtype=torch.long, device=self.device) * start_token_id\n",
    "        \n",
    "        # Track which sequences are completed\n",
    "        completed_sequences = torch.zeros(batch_size, dtype=torch.bool, device=self.device)\n",
    "        \n",
    "        for _ in range(max_length - 1):\n",
    "            # Make target mask for current decoder input\n",
    "            tgt_mask, tgt_key_padding_mask = self.make_trg_mask(decoder_input)\n",
    "            \n",
    "            # Decode one step\n",
    "            decoder_output = self.decoder(\n",
    "                trg=decoder_input,\n",
    "                memory=encoder_output,\n",
    "                tgt_mask=tgt_mask,\n",
    "                tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                memory_key_padding_mask=encoder_mask\n",
    "            )\n",
    "            \n",
    "            # Get next token prediction\n",
    "            next_token_logits = decoder_output[:, -1, :]\n",
    "            next_token = next_token_logits.argmax(dim=-1, keepdim=True)\n",
    "            \n",
    "            # Concatenate to decoder input\n",
    "            decoder_input = torch.cat([decoder_input, next_token], dim=1)\n",
    "            \n",
    "            # Check for EOS\n",
    "            completed_sequences = completed_sequences | (next_token.squeeze(-1) == end_token_id)\n",
    "            if completed_sequences.all():\n",
    "                break\n",
    "        \n",
    "        return decoder_input\n",
    "    \n",
    "    def _generate_beam_search(self, encoder_output, encoder_mask, start_token_id, end_token_id, max_length, num_beams, length_penalty, early_stopping):\n",
    "        batch_size = encoder_output.shape[0]\n",
    "        \n",
    "        # Expand encoder output for beam search\n",
    "        # [batch_size, seq_len, hidden] -> [batch_size * num_beams, seq_len, hidden]\n",
    "        encoder_output = encoder_output.unsqueeze(1).expand(-1, num_beams, -1, -1).reshape(batch_size * num_beams, -1, encoder_output.shape[-1])\n",
    "        \n",
    "        # Expand encoder mask\n",
    "        # [batch_size, seq_len] -> [batch_size * num_beams, seq_len]\n",
    "        encoder_mask = encoder_mask.unsqueeze(1).expand(-1, num_beams, -1).reshape(batch_size * num_beams, -1)\n",
    "        \n",
    "        # Start with beams of SOS tokens\n",
    "        current_tokens = torch.ones((batch_size * num_beams, 1), dtype=torch.long, device=self.device) * start_token_id\n",
    "        \n",
    "        # Track beam scores\n",
    "        beam_scores = torch.zeros((batch_size, num_beams), device=self.device)\n",
    "        beam_scores[:, 1:] = float('-inf')  # Only first beam is active initially\n",
    "        beam_scores = beam_scores.view(-1)  # [batch_size * num_beams]\n",
    "        \n",
    "        # Track completed sequences and scores\n",
    "        done_sequences = []\n",
    "        done_scores = []\n",
    "        done = [False for _ in range(batch_size)]\n",
    "        \n",
    "        for step in range(max_length - 1):\n",
    "            # Make target mask for current decoder input\n",
    "            tgt_mask, tgt_key_padding_mask = self.make_trg_mask(current_tokens)\n",
    "            \n",
    "            # Get decoder output\n",
    "            decoder_output = self.decoder(\n",
    "                trg=current_tokens,\n",
    "                memory=encoder_output,\n",
    "                tgt_mask=tgt_mask,\n",
    "                tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                memory_key_padding_mask=encoder_mask\n",
    "            )\n",
    "            \n",
    "            # Get log probabilities for next token\n",
    "            vocab_size = decoder_output.shape[-1]\n",
    "            next_token_logits = decoder_output[:, -1, :]\n",
    "            logprobs = torch.nn.functional.log_softmax(next_token_logits, dim=-1)  # [batch_size * num_beams, vocab_size]\n",
    "            \n",
    "            # Calculate new scores\n",
    "            # [batch_size * num_beams, vocab_size]\n",
    "            next_scores = beam_scores.unsqueeze(1) + logprobs\n",
    "            next_scores = next_scores.view(batch_size, num_beams * vocab_size)\n",
    "            \n",
    "            # Get the best 2*num_beams candidates\n",
    "            # [batch_size, 2*num_beams]\n",
    "            topk_scores, topk_indices = next_scores.topk(2 * num_beams, dim=1, largest=True, sorted=True)\n",
    "            \n",
    "            # Process each batch item\n",
    "            next_tokens = []\n",
    "            next_scores = []\n",
    "            \n",
    "            for batch_idx in range(batch_size):\n",
    "                # Skip if this batch item is already done\n",
    "                if done[batch_idx]:\n",
    "                    next_tokens.extend([current_tokens[batch_idx * num_beams] for _ in range(num_beams)])\n",
    "                    next_scores.extend([beam_scores[batch_idx * num_beams] for _ in range(num_beams)])\n",
    "                    continue\n",
    "                \n",
    "                # Find which beams and tokens to keep\n",
    "                beam_indices = []\n",
    "                token_indices = []\n",
    "                \n",
    "                for score_idx, (token_idx, score) in enumerate(zip(topk_indices[batch_idx], topk_scores[batch_idx])):\n",
    "                    # Convert to beam index and token index\n",
    "                    beam_idx = token_idx // vocab_size\n",
    "                    token = token_idx % vocab_size\n",
    "                    \n",
    "                    # Add to candidates if not already filled\n",
    "                    if len(beam_indices) < num_beams:\n",
    "                        beam_indices.append(beam_idx)\n",
    "                        token_indices.append(token)\n",
    "                \n",
    "                # Create new token sequences\n",
    "                for beam_idx, token in zip(beam_indices, token_indices):\n",
    "                    # Get current token sequence for this beam\n",
    "                    token_seq = current_tokens[batch_idx * num_beams + beam_idx].clone()\n",
    "                    # Add new token\n",
    "                    new_seq = torch.cat([token_seq, token.unsqueeze(0)], dim=0)\n",
    "                    next_tokens.append(new_seq)\n",
    "                    \n",
    "                    # Apply length penalty\n",
    "                    lp = ((5 + len(new_seq)) / 6) ** length_penalty\n",
    "                    next_scores.append(beam_scores[batch_idx * num_beams + beam_idx] + logprobs[batch_idx * num_beams + beam_idx, token] / lp)\n",
    "                    \n",
    "                    # Check if sequence is done\n",
    "                    if token == end_token_id:\n",
    "                        done_sequences.append(new_seq)\n",
    "                        done_scores.append(next_scores[-1])\n",
    "                        \n",
    "                # Check if all sequences for this batch are done\n",
    "                if len(done_sequences) >= num_beams and early_stopping:\n",
    "                    done[batch_idx] = True\n",
    "            \n",
    "            # Update current tokens and beam scores\n",
    "            current_tokens = torch.stack(next_tokens).view(batch_size * num_beams, -1)\n",
    "            beam_scores = torch.tensor(next_scores, device=self.device).view(batch_size * num_beams)\n",
    "            \n",
    "            # Check if all batches are done\n",
    "            if all(done):\n",
    "                break\n",
    "        \n",
    "        # If no sequences completed, use the current ones\n",
    "        if not done_sequences:\n",
    "            done_sequences = [current_tokens[i] for i in range(batch_size * num_beams)]\n",
    "            done_scores = beam_scores.tolist()\n",
    "        \n",
    "        # Get best sequence for each batch\n",
    "        result = []\n",
    "        for i in range(batch_size):\n",
    "            # Find best sequence for this batch\n",
    "            best_seq = None\n",
    "            best_score = float('-inf')\n",
    "            \n",
    "            for seq, score in zip(done_sequences, done_scores):\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_seq = seq\n",
    "            \n",
    "            result.append(best_seq)\n",
    "        \n",
    "        return torch.stack(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vuda/miniconda3/lib/python3.12/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 89,880,153 trainable parameters\n",
      "GPU Memory allocated: 344.12 MB\n",
      "GPU Memory reserved: 364.00 MB\n",
      "Model architecture:\n",
      "Seq2SeqTransformer(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(50265, 512)\n",
      "    (pos_encoder): PositionalEncoding(\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer_encoder): TransformerEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-2): 3 x TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (embedding): Embedding(50265, 512)\n",
      "    (pos_encoder): PositionalEncoding(\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer_decoder): TransformerDecoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-2): 3 x TransformerDecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fc_out): Linear(in_features=512, out_features=50265, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "PAD_IDX = tokenizer.pad_token_id\n",
    "UNK_IDX = tokenizer.unk_token_id\n",
    "SOS_IDX = tokenizer.bos_token_id\n",
    "EOS_IDX = tokenizer.eos_token_id\n",
    "\n",
    "# Model Hyperparameters (can be adjusted)\n",
    "ENC_LAYERS = 3\n",
    "DEC_LAYERS = 3\n",
    "ENC_HEADS = 8\n",
    "DEC_HEADS = 8\n",
    "ENC_PF_DIM = 512 \n",
    "DEC_PF_DIM = 512\n",
    "ENC_DROPOUT = 0.1\n",
    "DEC_DROPOUT = 0.1\n",
    "\n",
    "# Instantiate the Transformer model\n",
    "encoder = Encoder(vocab_size, HIDDEN_DIM, ENC_LAYERS, ENC_HEADS, ENC_PF_DIM, ENC_DROPOUT, device, MAX_LENGTH_ARTICLE)\n",
    "decoder = Decoder(vocab_size, HIDDEN_DIM, DEC_LAYERS, DEC_HEADS, DEC_PF_DIM, DEC_DROPOUT, device, MAX_LENGTH_SUMMARY)\n",
    "\n",
    "model = Seq2SeqTransformer(encoder, decoder, PAD_IDX, device).to(device)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX)\n",
    "\n",
    "# Define scheduler\n",
    "# Calculate total training steps for the scheduler\n",
    "total_steps = len(train_loader) * NUM_EPOCHS\n",
    "warmup_steps = int(0.1 * total_steps) # 10% of total steps for warmup\n",
    "\n",
    "def linear_warmup_decay(step, warmup_steps, total_steps):\n",
    "    if step < warmup_steps:\n",
    "        return (step + 1) / (warmup_steps + 1)\n",
    "    else:\n",
    "        return max(1e-7, (total_steps - step) / (total_steps - warmup_steps))\n",
    "\n",
    "def warmup_cosine_with_restarts(step, warmup_steps, total_steps, num_cycles=1):\n",
    "    if step < warmup_steps:\n",
    "        return (step + 1) / (warmup_steps + 1)\n",
    "    else:\n",
    "        progress = (step - warmup_steps) / (total_steps - warmup_steps)\n",
    "        cycle_progress = progress * num_cycles % 1\n",
    "        return max(1e-7, 0.5 * (1 + math.cos(math.pi * cycle_progress)))\n",
    "\n",
    "def get_scheduler(optimizer, total_steps, warmup_steps, num_cycles=None, types='warmup_cosine_with_restarts'):\n",
    "    if types == 'warmup_cosine_with_restarts':\n",
    "        assert num_cycles != None, 'must specify num_cycles when types=\"warmup_cosine_with_restarts\"'\n",
    "        return torch.optim.lr_scheduler.LambdaLR(\n",
    "            optimizer,\n",
    "            lr_lambda=lambda step: warmup_cosine_with_restarts(\n",
    "                step, warmup_steps, total_steps, num_cycles=num_cycles)\n",
    "        )\n",
    "    elif types == 'linear_warmup_decay':\n",
    "        return torch.optim.lr_scheduler.LambdaLR(\n",
    "            optimizer,\n",
    "            lr_lambda=lambda step: linear_warmup_decay(step, warmup_steps, total_steps)\n",
    "        )\n",
    "    else:\n",
    "        raise Exception('not implemented')\n",
    "\n",
    "scheduler = get_scheduler(\n",
    "    optimizer,\n",
    "    total_steps=total_steps,\n",
    "    warmup_steps=warmup_steps,\n",
    "    num_cycles=NUM_CYCLES, # Used for warmup_cosine_with_restarts\n",
    "    types=SCHEDULER_TYPE\n",
    ")\n",
    "\n",
    "print(f\"The model has {sum(p.numel() for p in model.parameters() if p.requires_grad):,} trainable parameters\")\n",
    "print(f\"GPU Memory allocated: {torch.cuda.memory_allocated()/1024**2:.2f} MB\")\n",
    "print(f\"GPU Memory reserved: {torch.cuda.memory_reserved()/1024**2:.2f} MB\")\n",
    "print(\"Model architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvubkk67\u001b[0m (\u001b[33mvubkk67-hanoi-university-of-science-and-technology\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/vuda/Text-Summarization/Abstractive/wandb/run-20250519_033649-clzndw97</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vubkk67-hanoi-university-of-science-and-technology/Seq2Seq-Summarization/runs/clzndw97' target=\"_blank\">transformer-scratch-20250519-033648</a></strong> to <a href='https://wandb.ai/vubkk67-hanoi-university-of-science-and-technology/Seq2Seq-Summarization' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vubkk67-hanoi-university-of-science-and-technology/Seq2Seq-Summarization' target=\"_blank\">https://wandb.ai/vubkk67-hanoi-university-of-science-and-technology/Seq2Seq-Summarization</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vubkk67-hanoi-university-of-science-and-technology/Seq2Seq-Summarization/runs/clzndw97' target=\"_blank\">https://wandb.ai/vubkk67-hanoi-university-of-science-and-technology/Seq2Seq-Summarization/runs/clzndw97</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # W&B setup\n",
    "# wandb.init(\n",
    "#     project=\"Seq2Seq-Summarization\",\n",
    "#     name=f\"transformer-scratch-{datetime.now().strftime('%Y%m%d-%H%M%S')}\",\n",
    "#     config={\n",
    "#         \"model\": \"Seq2Seq-Transformer-Scratch\",\n",
    "#         \"hidden_dim\": HIDDEN_DIM,\n",
    "#         \"batch_size\": BATCH_SIZE,\n",
    "#         \"learning_rate\": LEARNING_RATE,\n",
    "#         \"num_epochs\": NUM_EPOCHS,\n",
    "#         \"encoder_layers\": ENC_LAYERS,\n",
    "#         \"decoder_layers\": DEC_LAYERS,\n",
    "#         \"encoder_heads\": ENC_HEADS,\n",
    "#         \"decoder_heads\": DEC_HEADS,\n",
    "#         \"encoder_pf_dim\": ENC_PF_DIM,\n",
    "#         \"decoder_pf_dim\": DEC_PF_DIM,\n",
    "#         \"encoder_dropout\": ENC_DROPOUT,\n",
    "#         \"decoder_dropout\": DEC_DROPOUT,\n",
    "#         \"weight_decay\": WEIGHT_DECAY,\n",
    "#         \"clip\": CLIP,\n",
    "#         \"scheduler_type\": SCHEDULER_TYPE,\n",
    "#         \"warmup_steps\": warmup_steps,\n",
    "#         \"num_cycles\": NUM_CYCLES, # Relevant for cosine scheduler\n",
    "#         \"vocab_size\": vocab_size,\n",
    "#         \"max_length_article\": MAX_LENGTH_ARTICLE,\n",
    "#         \"max_length_summary\": MAX_LENGTH_SUMMARY,\n",
    "#         \"teacher_forcing_ratio\": TEACHER_FORCING_RATIO # Added teacher forcing ratio\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# wandb.watch(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27ffa45bb2714de2a4faf472e9af98b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1 [Train]:   0%|          | 0/4800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 42\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaN loss detected! Stopping training.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m \u001b[38;5;66;03m# Exit the inner batch loop\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), CLIP)\n\u001b[1;32m     46\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# # Save best model directory\n",
    "# # save_dir = os.path.join(model_dir, 'transformer_scratch_best_model')\n",
    "# save_dir = 'transformer_scratch_best_model'\n",
    "# os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# # Training loop\n",
    "# best_val_loss = float(\"inf\")\n",
    "\n",
    "# for epoch in range(NUM_EPOCHS):\n",
    "#     start_time = time.time()\n",
    "\n",
    "#     model.train()\n",
    "#     train_loss = 0.0\n",
    "\n",
    "#     progress_bar_train = tqdm(train_loader, desc=f\"Epoch {epoch+1} [Train]\")\n",
    "#     for batch in progress_bar_train:\n",
    "#         src = batch['article'].to(device)\n",
    "#         trg = batch['summary'].to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # The target input for the decoder should exclude the <eos> token\n",
    "#         # Pass the correct masks: trg_lookahead_mask, src_padding_mask, trg_padding_mask\n",
    "#         output = model(src, trg[:, :-1])\n",
    "\n",
    "#         # output = [batch_size, trg_len - 1, output_dim]\n",
    "#         # trg = [batch_size, trg_len]\n",
    "\n",
    "#         output_dim = output.shape[-1]\n",
    "\n",
    "#         # Reshape for loss calculation\n",
    "#         output = output.contiguous().view(-1, output_dim)\n",
    "#         trg = trg[:, 1:].contiguous().view(-1) # The target output should exclude the <sos> token\n",
    "\n",
    "#         loss = criterion(output, trg)\n",
    "\n",
    "#         # Check for NaN loss\n",
    "#         if torch.isnan(loss):\n",
    "#             print(\"NaN loss detected! Stopping training.\")\n",
    "#             break # Exit the inner batch loop\n",
    "\n",
    "#         loss.backward()\n",
    "\n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
    "\n",
    "#         optimizer.step()\n",
    "#         scheduler.step()\n",
    "\n",
    "#         train_loss += loss.item()\n",
    "#         progress_bar_train.set_postfix(loss=loss.item(), lr=optimizer.param_groups[0]['lr'])\n",
    "\n",
    "#     # If NaN loss was detected in the inner loop, break the outer epoch loop as well\n",
    "#     if torch.isnan(torch.tensor(train_loss)):\n",
    "#         break\n",
    "\n",
    "#     train_loss /= len(train_loader)\n",
    "\n",
    "#     # Validation\n",
    "#     model.eval()\n",
    "#     val_loss = 0.0\n",
    "#     with torch.no_grad():\n",
    "#         progress_bar_val = tqdm(validation_loader, desc=f\"Epoch {epoch+1} [Val]\")\n",
    "#         for batch in progress_bar_val:\n",
    "#             src = batch['article'].to(device)\n",
    "#             trg = batch['summary'].to(device)\n",
    "\n",
    "#             # The target input for the decoder should exclude the <eos> token\n",
    "#             # Pass the correct masks: trg_lookahead_mask, src_padding_mask, trg_padding_mask\n",
    "#             output = model(src, trg[:, :-1])\n",
    "\n",
    "#             output_dim = output.shape[-1]\n",
    "\n",
    "#             # Reshape for loss calculation\n",
    "#             output = output.contiguous().view(-1, output_dim)\n",
    "#             trg = trg[:, 1:].contiguous().view(-1) # The target output should exclude the <sos> token\n",
    "\n",
    "#             loss = criterion(output, trg)\n",
    "\n",
    "#             val_loss += loss.item()\n",
    "#             progress_bar_val.set_postfix(loss=loss.item())\n",
    "\n",
    "#         val_loss /= len(validation_loader)\n",
    "#         current_lr = optimizer.param_groups[0]['lr'] # Get current LR after scheduler step\n",
    "\n",
    "#     # W&B log\n",
    "#     wandb.log({\n",
    "#         \"epoch\": epoch + 1,\n",
    "#         \"train_loss\": train_loss,\n",
    "#         \"val_loss\": val_loss,\n",
    "#         \"lr\": current_lr,\n",
    "#         \"best_val_loss\": best_val_loss # Log best val loss seen so far\n",
    "#     })\n",
    "\n",
    "#     # Save best model and tokenizer\n",
    "#     if val_loss < best_val_loss:\n",
    "#         best_val_loss = val_loss\n",
    "#         torch.save(model.state_dict(), os.path.join(save_dir, 'best_transformer_model.pt'))\n",
    "#         tokenizer.save_pretrained(save_dir)\n",
    "#         print(f\"Saved best model and tokenizer to `{save_dir}` at epoch {epoch+1}\")\n",
    "\n",
    "#     print(\n",
    "#         f\"Epoch {epoch+1:02d} | \"\n",
    "#         f\"Train Loss: {train_loss:.4f} | \"\n",
    "#         f\"Val Loss: {val_loss:.4f} | \"\n",
    "#         f\"LR: {current_lr:.6f} | \"\n",
    "#         f\"Time: {time.time() - start_time:.2f}s\"\n",
    "#     )\n",
    "\n",
    "# # W&B end\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"../Model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Model/transformer_scratch_best_model\n"
     ]
    }
   ],
   "source": [
    "output_path = os.path.join(datafilter, \"test_pred_6.csv\") \n",
    "save_dir = os.path.join(model_dir, 'transformer_scratch_best_model')\n",
    "print(save_dir)\n",
    "tokenizer = BartTokenizer.from_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vuda/miniconda3/lib/python3.12/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "test_dataset = Seq2SeqDataset(test_sample['articles'].tolist(), test_sample['summaries'].tolist(), tokenizer)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "PAD_IDX = tokenizer.pad_token_id\n",
    "UNK_IDX = tokenizer.unk_token_id\n",
    "SOS_IDX = tokenizer.bos_token_id\n",
    "EOS_IDX = tokenizer.eos_token_id\n",
    "encoder = Encoder(vocab_size, HIDDEN_DIM, ENC_LAYERS, ENC_HEADS, ENC_PF_DIM, ENC_DROPOUT, device, MAX_LENGTH_ARTICLE)\n",
    "decoder = Decoder(vocab_size, HIDDEN_DIM, DEC_LAYERS, DEC_HEADS, DEC_PF_DIM, DEC_DROPOUT, device, MAX_LENGTH_SUMMARY)\n",
    "\n",
    "model = Seq2SeqTransformer(encoder, decoder, PAD_IDX, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19516/4178991672.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(os.path.join(save_dir, 'best_transformer_model.pt'), map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating summaries...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e257f6845fbb4eb9a3be385591e0df5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating summaries:   0%|          | 0/2112 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 4224 summaries\n",
      " File has been saved at: ../dataft/test_pred_6.csv\n"
     ]
    }
   ],
   "source": [
    "# Load the model from saved state\n",
    "model.load_state_dict(torch.load(os.path.join(save_dir, 'best_transformer_model.pt'), map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"Generating summaries...\")\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Generating summaries\"):\n",
    "        input_ids = batch['article'].to(device)\n",
    "        attention_mask = batch['article_attention_mask'].to(device)\n",
    "        \n",
    "        # Instead of using beam search which has issues, use greedy search\n",
    "        # by setting num_beams=1\n",
    "        output_ids = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=MAX_LENGTH_SUMMARY,\n",
    "            num_beams=1,  # Use greedy search instead of beam search\n",
    "            length_penalty=2.0,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        batch_preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "        predictions.extend(batch_preds)\n",
    "\n",
    "# Save predictions\n",
    "print(f\"Generated {len(predictions)} summaries\")\n",
    "test_sample = test_sample.iloc[:len(predictions)].copy()\n",
    "test_sample[\"predicted_summary\"] = predictions\n",
    "test_sample.to_csv(output_path, index=False)\n",
    "print(f\" File has been saved at: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating evaluation metrics...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>articles</th>\n",
       "      <th>summaries</th>\n",
       "      <th>predicted_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Florida bus passenger was arrested for throw...</td>\n",
       "      <td>Joel Parker, 33, was riding the bus in St John...</td>\n",
       "      <td>The driver was arrested on suspicion of assaul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aston Villa may be able to sign Cordoba strike...</td>\n",
       "      <td>Aston Villa have held talks over Cordoba strik...</td>\n",
       "      <td>Manchester City midfielder has been linked wit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            articles  \\\n",
       "0  A Florida bus passenger was arrested for throw...   \n",
       "1  Aston Villa may be able to sign Cordoba strike...   \n",
       "\n",
       "                                           summaries  \\\n",
       "0  Joel Parker, 33, was riding the bus in St John...   \n",
       "1  Aston Villa have held talks over Cordoba strik...   \n",
       "\n",
       "                                   predicted_summary  \n",
       "0  The driver was arrested on suspicion of assaul...  \n",
       "1  Manchester City midfielder has been linked wit...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE scores:\n",
      "ROUGE-1: 0.1815\n",
      "ROUGE-2: 0.0280\n",
      "ROUGE-L: 0.1741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 03:53:04.319459: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-19 03:53:04.336711: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747601584.355542   19516 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747601584.361095   19516 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747601584.374852   19516 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747601584.374981   19516 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747601584.374983   19516 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747601584.374984   19516 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-19 03:53:04.379165: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTScore:\n",
      "Precision: 0.8236\n",
      "Recall:    0.8188\n",
      "F1:        0.8209\n",
      "METEOR Score (average):\n",
      "METEOR: 0.1318\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Calculate metrics\n",
    "print(\"Calculating evaluation metrics...\")\n",
    "test_pred = pd.read_csv(output_path)\n",
    "display(test_pred[[\"articles\",\"summaries\", \"predicted_summary\"]].head(2))\n",
    "\n",
    "# Check required columns\n",
    "if \"summaries\" in test_pred.columns and \"predicted_summary\" in test_pred.columns:\n",
    "    references = test_pred[\"summaries\"].fillna(\"<empty>\").astype(str).tolist()\n",
    "    predictions = test_pred[\"predicted_summary\"].fillna(\"<empty>\").astype(str).tolist()\n",
    "\n",
    "    # Filter valid pairs\n",
    "    valid_pairs = [\n",
    "        (pred, ref) for pred, ref in zip(predictions, references)\n",
    "        if pred.strip() and pred != \"<empty>\" and ref.strip()\n",
    "    ]\n",
    "    \n",
    "    if not valid_pairs:\n",
    "        print(\"No valid pairs found for metric calculation.\")\n",
    "    else:\n",
    "        filtered_preds, filtered_refs = zip(*valid_pairs)\n",
    "\n",
    "        # ROUGE\n",
    "        rouge = Rouge()\n",
    "        rouge_scores = rouge.get_scores(filtered_preds, filtered_refs, avg=True)\n",
    "        print(\"ROUGE scores:\")\n",
    "        print(f\"ROUGE-1: {rouge_scores['rouge-1']['f']:.4f}\")\n",
    "        print(f\"ROUGE-2: {rouge_scores['rouge-2']['f']:.4f}\")\n",
    "        print(f\"ROUGE-L: {rouge_scores['rouge-l']['f']:.4f}\")\n",
    "\n",
    "        # BERTScore\n",
    "        P, R, F1 = bert_score(filtered_preds, filtered_refs, lang=\"en\", verbose=False)\n",
    "        print(\"BERTScore:\")\n",
    "        print(f\"Precision: {P.mean().item():.4f}\")\n",
    "        print(f\"Recall:    {R.mean().item():.4f}\")\n",
    "        print(f\"F1:        {F1.mean().item():.4f}\")\n",
    "\n",
    "        # METEOR\n",
    "        print(\"METEOR Score (average):\")\n",
    "        meteor_scores = [single_meteor_score(ref.split(), pred.split()) \n",
    "                        for pred, ref in zip(filtered_preds, filtered_refs)]\n",
    "        print(f\"METEOR: {sum(meteor_scores)/len(meteor_scores):.4f}\")\n",
    "\n",
    "else:\n",
    "    print(\"Could not find required columns 'summaries' and 'predicted_summary' for metric calculation.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
