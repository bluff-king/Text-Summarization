{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "import random \n",
    "from tqdm import tqdm\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "import wandb\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"../dataset/train.csv\")\n",
    "validation_data = pd.read_csv(\"../dataset/validation.csv\")\n",
    "test_data = pd.read_csv(\"../dataset/test.csv\")\n",
    "\n",
    "train_data.rename(columns={\"highlights\": \"summaries\", \"article\":\"articles\"}, inplace=True)\n",
    "validation_data.rename(columns={\"highlights\": \"summaries\",\"article\":\"articles\"}, inplace=True)\n",
    "test_data.rename(columns={\"highlights\": \"summaries\", \"article\":\"articles\"}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"article_word_count\"] = train_data[\"articles\"].astype(str).apply(lambda x: len(x.split()))\n",
    "train_data[\"summary_word_count\"] = train_data[\"summaries\"].astype(str).apply(lambda x: len(x.split()))\n",
    "\n",
    "validation_data[\"article_word_count\"] = validation_data[\"articles\"].astype(str).apply(lambda x: len(x.split()))\n",
    "validation_data[\"summary_word_count\"] = validation_data[\"summaries\"].astype(str).apply(lambda x: len(x.split()))\n",
    "\n",
    "test_data[\"article_word_count\"] = test_data[\"articles\"].astype(str).apply(lambda x: len(x.split()))\n",
    "test_data[\"summary_word_count\"] = test_data[\"summaries\"].astype(str).apply(lambda x: len(x.split()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data[train_data[\"article_word_count\"] < 512]\n",
    "validation_data = validation_data[validation_data[\"article_word_count\"] < 512]\n",
    "test_data = test_data[test_data[\"article_word_count\"] < 1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 97959 entries, 282281 to 227701\n",
      "Data columns (total 5 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   id                  97959 non-null  object\n",
      " 1   articles            97959 non-null  object\n",
      " 2   summaries           97959 non-null  object\n",
      " 3   article_word_count  97959 non-null  int64 \n",
      " 4   summary_word_count  97959 non-null  int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 4.5+ MB\n"
     ]
    }
   ],
   "source": [
    "train_sample = train_data.sample(frac=1, random_state=1)\n",
    "validation_sample = validation_data.sample(frac=1, random_state=1)\n",
    "test_sample = test_data.sample(frac=0.01, random_state=1)\n",
    "train_sample.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "511\n",
      "1185\n"
     ]
    }
   ],
   "source": [
    "max_len_article = train_sample[\"article_word_count\"].max()\n",
    "print(max_len_article)\n",
    "max_len_summary = train_sample[\"summary_word_count\"].max()\n",
    "print(max_len_summary)\n",
    "# if max_len_article is None:\n",
    "#     self.max_len_article = max(len(numericalize(art)) for art in articles) + 2  # +2 cho SOS/EOS\n",
    "# else:\n",
    "#     self.max_len_article = max_len_article\n",
    "    \n",
    "# if max_len_summary is None:\n",
    "#     self.max_len_summary = max(len(numericalize(summ)) for summ in summaries) + 2\n",
    "# else:\n",
    "#     self.max_len_summary = max_len_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25004\n",
      "Embedding shape: (25004, 104)\n",
      "<PAD> embedding last 4 dims: [1.0, 0.0, 0.0, 0.0]\n",
      "<SOS> embedding last 4 dims: [0.0, 1.0, 0.0, 0.0]\n",
      "Word 'the' embedding last 4 dims: [0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_FILE = \"../Embedding/glove-wiki-gigaword-100.txt\"\n",
    "vocab, embeddings = [], []\n",
    "with open(EMBEDDING_FILE, 'rt', encoding='utf-8') as ef:\n",
    "    full_content = ef.read().strip().split('\\n')\n",
    "for i in range(len(full_content)):\n",
    "    i_word = full_content[i].split(' ')[0]\n",
    "    i_embeddings = [float(val) for val in full_content[i].split(' ')[1:]]\n",
    "    i_embeddings.extend([0.0, 0.0, 0.0, 0.0])\n",
    "    vocab.append(i_word)\n",
    "    embeddings.append(i_embeddings)\n",
    "\n",
    "embs_npa = np.array(embeddings)\n",
    "\n",
    "unk_embedding = np.mean(embs_npa, axis=0).tolist()\n",
    "\n",
    "dim = embs_npa.shape[1]\n",
    "sos_embedding = [0.0] * dim\n",
    "sos_embedding[-3] = 1.0\n",
    "eos_embedding = [0.0] * dim\n",
    "eos_embedding[-2] = 1.0\n",
    "pad_embedding = [0.0] * dim\n",
    "pad_embedding[-4] = 1.0\n",
    "# unk_embedding = [0.0] * dim\n",
    "# unk_embedding[-1] = 1.0\n",
    "\n",
    "# Update vocab and embeddings\n",
    "vocab = [\"<PAD>\", \"<SOS>\", \"<EOS>\", \"<UNK>\"] + vocab\n",
    "embeddings = [pad_embedding, sos_embedding,\n",
    "              eos_embedding, unk_embedding] + embeddings\n",
    "\n",
    "vocab_npa = np.array(vocab)\n",
    "embs_npa = np.array(embeddings)\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    return text.lower().strip().split()\n",
    "\n",
    "\n",
    "stoi_dict = {word: idx for idx, word in enumerate(vocab_npa)}\n",
    "_unk_idx = stoi_dict[\"<UNK>\"]\n",
    "\n",
    "\n",
    "def stoi(string, stoi_dict=stoi_dict):\n",
    "    return stoi_dict.get(string, _unk_idx)\n",
    "\n",
    "\n",
    "def numericalize(text):\n",
    "    tokenized_text = tokenize(text)\n",
    "    return [\n",
    "        stoi(token)\n",
    "        for token in tokenized_text\n",
    "    ]\n",
    "\n",
    "print(embs_npa.shape[0])\n",
    "embedding_layer = torch.nn.Embedding.from_pretrained(torch.FloatTensor(embeddings),\n",
    "                                                     freeze=False,\n",
    "                                                     padding_idx=stoi(\"<PAD>\"))\n",
    "embedding_layer.to(device)\n",
    "print(\"Embedding shape:\", np.array(embeddings).shape) \n",
    "print(\"<PAD> embedding last 4 dims:\", embeddings[stoi(\"<PAD>\")][-4:])\n",
    "print(\"<SOS> embedding last 4 dims:\", embeddings[stoi(\"<SOS>\")][-4:])\n",
    "print(\"Word 'the' embedding last 4 dims:\", embeddings[stoi(\"the\")][-4:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad_idx = stoi(\"<PAD>\")\n",
    "# sos_idx = stoi(\"<SOS>\")\n",
    "# eos_idx = stoi(\"<EOS>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqDataset(Dataset):\n",
    "    def __init__(self, articles, summaries, stoi, max_len_article=512, max_len_summary=256):\n",
    "        self.articles = articles  # List of articles\n",
    "        self.summaries = summaries  # List of summaries\n",
    "        self.stoi = stoi  # String-to-index dictionary\n",
    "        self.pad_idx = stoi(\"<PAD>\")\n",
    "        self.sos_idx = stoi(\"<SOS>\")\n",
    "        self.eos_idx = stoi(\"<EOS>\")\n",
    "        \n",
    "        # Determine max lengths if not provided\n",
    "        self.max_len_article = max_len_article or max(len(a.split()) for a in articles) + 2\n",
    "        self.max_len_summary = max_len_summary or max(len(s.split()) for s in summaries) + 2\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.articles)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        def process_text(text, max_len):\n",
    "            tokens = [self.sos_idx] + [self.stoi(w) for w in text.split()] + [self.eos_idx]  # Tokenize and add SOS/EOS\n",
    "            tokens = tokens[:max_len] + [self.pad_idx] * (max_len - len(tokens))  # Pad to max length\n",
    "            return torch.tensor(tokens), len(tokens)\n",
    "\n",
    "        article_tokens, article_len = process_text(self.articles[idx], self.max_len_article)\n",
    "        summary_tokens, summary_len = process_text(self.summaries[idx], self.max_len_summary)\n",
    "        \n",
    "        return {\n",
    "            'article': article_tokens,  # Encoded article\n",
    "            'article_len': torch.tensor(article_len),\n",
    "            'summary': summary_tokens,  # Encoded summary\n",
    "            'summary_len': torch.tensor(summary_len)\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Batch là list các dict {'article': ..., 'summary': ...}\n",
    "    return {\n",
    "        'article': torch.stack([item['article'] for item in batch]),\n",
    "        'article_len': torch.tensor([item['article_len'] for item in batch]),\n",
    "        'summary': torch.stack([item['summary'] for item in batch]),\n",
    "        'summary_len': torch.tensor([item['summary_len'] for item in batch])\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    1,     3,    69,     3,     3,     3,   460,  4355,     8,  8090,\n",
      "            3,     9,    52,     7,    75,   997,     3,     3,    18,    88,\n",
      "            3,     3,   974,     3, 10960,     3,     3,    11,  2871,    10,\n",
      "            4,  1139,     7,    11,  1617,  3123,     7,     3,     3,     3,\n",
      "            3,     3,     3,     3,    11,   304,    90,     3,    11,   791,\n",
      "           35,     4,   252,     8,    92,     3,     3,     3,  2248,     4,\n",
      "         3123,   139,    60,    77,     3,   204,     8,     3,   117,    24,\n",
      "         5272,     4,     3,     3,     3,  2599,    11,  3999,    29,     3,\n",
      "           16,   214,     4,  3123,     3,  1820,     4,     3,   505,     3,\n",
      "           46,     3,     3,   242,  3565, 16584,    50,  7368,  2772,    47,\n",
      "           34,  5423,     9,   396,  1126,     8,     3,   111,    34,     3,\n",
      "            3,  6216,    16,     4,     3,  3986,     7,    51,   531,    18,\n",
      "            3,     3,    11,   304,  4256,    11,  3123,     7,    30,     3,\n",
      "            3,     3,    38,    40,    11,     3,     3,    57,    10,     3,\n",
      "            3,    35,    40,  3211,     8,  4964,    14,     3,     3,  6287,\n",
      "            7,   270,   522,     4,   252,     8,   246,     3,    17,   663,\n",
      "          497,    19,  4730,    25,     3,     3,  1135,     7,  3830,     3,\n",
      "            3,     9,     3,     3,    10,     3,    65,    71,  2599,  2674,\n",
      "            8,     3,     7,  2942,  3530,     3,    10,     3,     3,     3,\n",
      "            3,     3,    14,    51, 16584,   531,    10,     4,     3,     3,\n",
      "           10,     3,    38,    40,    17,     3,     9,     3,     3,     3,\n",
      "          890,    85,   257,     3,     3,    35,  1900,   851,    16,     3,\n",
      "           18,   321, 14965,    14,    75,     3,     3,  2599,    11,  3123,\n",
      "            7,    75,     3,     3,     8,     4,   580,     3,   389,    21,\n",
      "           11,  9652,     3,     3,    90,   118,   600,   196,  3880,     9,\n",
      "          196, 16536,    17,     3,    95,   115,   196,  5205,    37,     4,\n",
      "          252,     8,  3012,   106,    43,     3,     3, 12542,     3,     6,\n",
      "            2,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0])\n"
     ]
    }
   ],
   "source": [
    "# DataLoader setup\n",
    "# torch.set_printoptions(profile=\"full\")\n",
    "torch.set_printoptions(profile=\"default\")\n",
    "train_dataset = Seq2SeqDataset(train_sample['articles'].tolist(), train_sample['summaries'].tolist(), stoi)\n",
    "print(train_dataset[268][\"article\"])\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "valid_dataset= Seq2SeqDataset(validation_sample['articles'].tolist(), validation_sample['summaries'].tolist(), stoi)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def collate_fn(batch):\n",
    "#     return {\n",
    "#         'article': torch.stack([item['article'] for item in batch]),\n",
    "#         'article_len': torch.stack([item['article_len'] for item in batch]),\n",
    "#         'summary': torch.stack([item['summary'] for item in batch]),\n",
    "#         'summary_len': torch.stack([item['summary_len'] for item in batch])\n",
    "#     }\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Architecture Seq2Seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleEncoder(nn.Module):\n",
    "    def __init__(self, embedding_layer, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = embedding_layer  # Sử dụng embedding có sẵn\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.embedding.embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            batch_first=True,\n",
    "            bidirectional=False  # Unidirectional\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, seq_lens):  # x = batch input sequences\n",
    "        # Bước 1: Embedding\n",
    "        x = self.embedding(x)  # [batch_size, max_len, emb_dim]\n",
    "        \n",
    "        # Bước 2: Pack để bỏ qua padding tokens\n",
    "        packed = pack_padded_sequence(\n",
    "            input=x,\n",
    "            lengths=seq_lens.cpu(),  # Chuyển sang CPU tensor\n",
    "            batch_first=True,\n",
    "            enforce_sorted=False  # Không cần sắp xếp theo độ dài\n",
    "        )\n",
    "        \n",
    "        # Bước 3: LSTM (chỉ xử lý độ dài thực) \n",
    "        '''\n",
    "        packed_output: dữ liệu thực sự được xử lý\n",
    "        hidden/cell lưu trạng thái cuối cùng của mỗi sequence\n",
    "        Input (padded):       Packed LSTM:         Output (unpacked):\n",
    "        [1,2,3,0,0]    -->   [1,2,3,4,5,6,7,8]       --> [h1_t1,h1_t2,h1_t3,0,0]\n",
    "        [4,5,6,7,8]     hidden_dim, hd,... hd(8cai)  --> [h2_t1,h2_t2,h2_t3,h2_t4,h2_t5]\n",
    "        '''\n",
    "        packed_output, (hidden, cell) = self.lstm(packed)\n",
    "        # Output: (batch_size, seq_len, hidden_dim)\n",
    "        # Bước 4: Unpack nếu cần dùng attention\n",
    "        \n",
    "        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "        \n",
    "        return output, (hidden, cell)  # hidden shape: [1, batch_size, hidden_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        # Kết hợp cả encoder outputs và decoder hidden state\n",
    "        self.energy = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),  # Nhận input là [encoder_output + decoder_hidden]\n",
    "            nn.Tanh(),                              # thêm chút phi tuyến\n",
    "            nn.Linear(hidden_dim, 1, bias=False)    #  Trả về 1 chiều attention scores\n",
    "        )\n",
    "    \n",
    "    def forward(self, decoder_hidden, encoder_outputs, mask=None):\n",
    "        # Bước 1: Chuẩn bị decoder_hidden để cộng với encoder_outputs\n",
    "        # decoder_hidden: [batch_size, hidden_dim]\n",
    "        # encoder_outputs: [batch_size, seq_len, hidden_dim]\n",
    "        # Copy dọc theo seq_len\n",
    "        #########  decoder_hidden = decoder_hidden.unsqueeze(1).expand_as(encoder_outputs)  # [batch_size, seq_len, hidden_dim]\n",
    "        decoder_hidden = decoder_hidden.unsqueeze(1)  # [batch_size, 1, hidden_dim]\n",
    "        decoder_hidden = decoder_hidden.repeat(1, encoder_outputs.size(1), 1)\n",
    "        # Bước 2: Tính energy từ sự kết hợp encoder-decoder\n",
    "        combined = torch.cat([encoder_outputs, decoder_hidden], dim=2)  # [batch_size, seq_len, hidden_dim * 2]\n",
    "        scores = self.energy(combined).squeeze(2)  # [batch_size, seq_len]\n",
    "        \n",
    "        # Bước 3: Áp dụng mask và softmax\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e10)\n",
    "        attn_weights = F.softmax(scores, dim=1)    # [batch_size, seq_len]\n",
    "        \n",
    "        # Bước 4: Tính context vector: # [batch_size,1, hidden_dim] x [batch_size, seq_len, hidden_dim]\n",
    "                #  context = [batch_size, 1, hidden_dim]squeeze(1) loại chiều 1 -> [batch_size, hidden_dim]\n",
    "        context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs).squeeze(1) \n",
    "     \n",
    "        # debug  \n",
    "        '''\n",
    "        Encoder Outputs:  \"<SOS>\"   \"cat\"    \"sat\"   \"<EOS>\"  \"<PAD>\"\n",
    "                        [0.1,0.2] [0.3,0.4] [0.5,0.6] [0.7,0.8] [0.0,0.0]\n",
    "        Attention Weights:  0.12      0.53      0.29      0.06      0.0\n",
    "                        ↓         ↓         ↓         ↓         ↓\n",
    "        Context Vector:  = 0.12*[0.1,0.2] + 0.53*[0.3,0.4] + ... = [0.35, 0.45]\n",
    "        '''\n",
    "        '''\n",
    "        attn_weights (unsqueezed):   encoder_outputs:       context:\n",
    "        [ [ [0.2, 0.5, 0.3] ]    @  [[0.1,0.2],        =  [ [0.32, 0.42] ]\n",
    "        [ [0.1, 0.7, 0.2] ]        [0.3,0.4],            [0.92, 1.02] ]\n",
    "                                    [0.5,0.6] ]\n",
    "        '''\n",
    "        # print(\"Decoder hidden:\", decoder_hidden.shape)\n",
    "        # print(\"Encoder outputs:\", encoder_outputs.shape)\n",
    "        # context, attn_weights = self.attention(decoder_hidden, encoder_outputs)\n",
    "        # print(\"Attention weights:\", attn_weights)  # Xem model đang tập trung vào đâu\n",
    "        return context, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDecoder(nn.Module):\n",
    "    def __init__(self, embedding_layer, hidden_dim, vocab_size):\n",
    "        super().__init__()\n",
    "        self.embedding = embedding_layer\n",
    "        self.lstm = nn.LSTMCell(\n",
    "            input_size=self.embedding.embedding_dim + hidden_dim,  # Thay đổi 1: Thêm context_dim\n",
    "            hidden_size=hidden_dim\n",
    "        )\n",
    "        self.attention = SimpleAttention(hidden_dim)\n",
    "        self.fc_out = nn.Linear(hidden_dim * 2, vocab_size)  # Thay đổi 2: Kết hợp hidden + context\n",
    "\n",
    "    def forward(self, x, prev_hidden, prev_cell, encoder_outputs, mask=None):\n",
    "        # Bước 1: Embedding\n",
    "        x = self.embedding(x)  # [batch_size] -> [batch_size, emb_dim]\n",
    "        \n",
    "        if prev_hidden.dim() == 3:  # If it's [num_layers, batch_size, hidden_dim]\n",
    "            prev_hidden = prev_hidden[-1]  # Take last layer's hidden state\n",
    "        \n",
    "        context, attn_weights = self.attention(prev_hidden, encoder_outputs, mask)\n",
    "        \n",
    "        # Bước 3: Kết hợp embedding và context làm input cho LSTM\n",
    "        lstm_input = torch.cat([x, context], dim=1)  # [batch_size, emb_dim + hidden_dim]\n",
    "        \n",
    "        # Bước 4: LSTM step\n",
    "        hidden, cell = self.lstm(lstm_input, (prev_hidden, prev_cell))\n",
    "        \n",
    "        # Bước 5: Kết hợp hidden và context để dự đoán từ\n",
    "        output_input = torch.cat([hidden, context], dim=1)\n",
    "        output = self.fc_out(output_input)\n",
    "        \n",
    "        return output, hidden, cell, attn_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqModel(nn.Module):\n",
    "    def __init__(self, embedding_layer, hidden_dim, vocab_size):\n",
    "        super().__init__()\n",
    "        self.encoder = SimpleEncoder(embedding_layer, hidden_dim)\n",
    "        self.decoder = SimpleDecoder(embedding_layer, hidden_dim, vocab_size)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.start_id = stoi(\"<SOS>\")  # Thêm start token ID\n",
    "\n",
    "    def forward(self, src, src_lens, trg=None, max_len=256, teacher_forcing_ratio=0.5):\n",
    "        # Encoder forward\n",
    "        enc_outputs, (hidden, cell) = self.encoder(src, src_lens)\n",
    "        \n",
    "        # Chuẩn bị decoder\n",
    "        batch_size = src.size(0)\n",
    "        if trg is None:  # Inference mode\n",
    "            max_len = max_len\n",
    "            trg = torch.full((batch_size,), self.start_id, dtype=torch.long, device=src.device)\n",
    "        else:  # Training mode\n",
    "            max_len = trg.size(1)\n",
    "        \n",
    "        # Tensor lưu outputs\n",
    "        outputs = torch.zeros(batch_size, max_len, self.vocab_size).to(src.device)\n",
    "        \n",
    "        # Khởi tạo input đầu tiên\n",
    "        x = torch.full((batch_size,), self.start_id, dtype=torch.long, device=src.device)  # Đổi tên x_t -> x\n",
    "        \n",
    "        # Squeeze the encoder hidden states for the decoder\n",
    "        hidden = hidden.squeeze(0)  # [1, batch_size, hidden_dim] -> [batch_size, hidden_dim]\n",
    "        cell = cell.squeeze(0)\n",
    "        \n",
    "        for t in range(max_len):\n",
    "            output, hidden, cell, _ = self.decoder(\n",
    "                x=x,\n",
    "                prev_hidden=hidden,\n",
    "                prev_cell=cell,\n",
    "                encoder_outputs=enc_outputs\n",
    "            )\n",
    "            outputs[:, t] = output\n",
    "            \n",
    "            if trg is not None and random.random() < teacher_forcing_ratio:\n",
    "                x = trg[:, t]\n",
    "            else:\n",
    "                x = output.argmax(1)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory allocated: 9.92 MB\n",
      "GPU Memory reserved: 20.00 MB\n"
     ]
    }
   ],
   "source": [
    "print(f\"GPU Memory allocated: {torch.cuda.memory_allocated()/1024**2:.2f} MB\")\n",
    "print(f\"GPU Memory reserved: {torch.cuda.memory_reserved()/1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 2. Tạo embedding layer (như bạn đã làm)\n",
    "# embedding_layer = torch.nn.Embedding.from_pretrained(\n",
    "#     torch.FloatTensor(embeddings),\n",
    "#     freeze=False,\n",
    "#     padding_idx=stoi(\"<PAD>\")\n",
    "# ).to(device)\n",
    "\n",
    "# # 3. Khởi tạo model\n",
    "# model = Seq2SeqModel(\n",
    "#     embedding_layer=embedding_layer,\n",
    "#     hidden_dim=128,\n",
    "#     vocab_size=len(vocab)\n",
    "# ).to(device)\n",
    "\n",
    "# # 4. Train loop\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "# criterion = nn.CrossEntropyLoss(ignore_index=stoi(\"<PAD>\"))\n",
    "\n",
    "# for epoch in range(1):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "    \n",
    "#     for batch in train_loader:\n",
    "#         src = batch['article'].to(device)\n",
    "#         src_lens = batch['article_len'].to(device)\n",
    "#         trg = batch['summary'].to(device)\n",
    "        \n",
    "#         outputs = model(src, src_lens, trg=trg, teacher_forcing_ratio=0.5)\n",
    "#         loss = criterion(outputs[:, 1:].reshape(-1, outputs.size(-1)), \n",
    "#                          trg[:, 1:].reshape(-1))\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         total_loss += loss.item()\n",
    "    \n",
    "#     print(f\"Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: torch.Size([25004, 104])\n",
      "Vocab size: 25004\n",
      "Model architecture:\n",
      "Seq2SeqModel(\n",
      "  (encoder): SimpleEncoder(\n",
      "    (embedding): Embedding(25004, 104, padding_idx=0)\n",
      "    (lstm): LSTM(104, 128, batch_first=True)\n",
      "  )\n",
      "  (decoder): SimpleDecoder(\n",
      "    (embedding): Embedding(25004, 104, padding_idx=0)\n",
      "    (lstm): LSTMCell(232, 128)\n",
      "    (attention): SimpleAttention(\n",
      "      (energy): Sequential(\n",
      "        (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Linear(in_features=128, out_features=1, bias=False)\n",
      "      )\n",
      "    )\n",
      "    (fc_out): Linear(in_features=256, out_features=25004, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "embedding_layer = torch.nn.Embedding.from_pretrained(\n",
    "    torch.FloatTensor(embeddings),\n",
    "    freeze=False,  # Cho phép fine-tune embedding\n",
    "    padding_idx=stoi(\"<PAD>\")  # Index của token padding\n",
    ").to(device)\n",
    "\n",
    "# Khởi tạo\n",
    "hidden_dim = 128\n",
    "vocab_size = len(vocab)\n",
    "model = Seq2SeqModel(\n",
    "    embedding_layer=embedding_layer,\n",
    "    hidden_dim=hidden_dim,\n",
    "    vocab_size=vocab_size\n",
    ").to(device)\n",
    "print(\"Embedding shape:\", torch.FloatTensor(embeddings).shape)\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "print(\"Model architecture:\")\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvubkk67\u001b[0m (\u001b[33mvubkk67-hanoi-university-of-science-and-technology\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/vuda/Text-Summarization/Abstractive/wandb/run-20250403_150310-coq8r0c3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vubkk67-hanoi-university-of-science-and-technology/Seq2Seq-Summarization/runs/coq8r0c3' target=\"_blank\">seq2seq-20250403-150309</a></strong> to <a href='https://wandb.ai/vubkk67-hanoi-university-of-science-and-technology/Seq2Seq-Summarization' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vubkk67-hanoi-university-of-science-and-technology/Seq2Seq-Summarization' target=\"_blank\">https://wandb.ai/vubkk67-hanoi-university-of-science-and-technology/Seq2Seq-Summarization</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vubkk67-hanoi-university-of-science-and-technology/Seq2Seq-Summarization/runs/coq8r0c3' target=\"_blank\">https://wandb.ai/vubkk67-hanoi-university-of-science-and-technology/Seq2Seq-Summarization/runs/coq8r0c3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/vubkk67-hanoi-university-of-science-and-technology/Seq2Seq-Summarization/runs/coq8r0c3?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fce443a8d70>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project=\"Seq2Seq-Summarization\",\n",
    "    name=f\"seq2seq-{datetime.now().strftime('%Y%m%d-%H%M%S')}\",\n",
    "    config={\n",
    "        \"model\": \"Seq2Seq-LSTM\",\n",
    "        \"hidden_dim\": 128,\n",
    "        \"batch_size\": 16,\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"teacher_forcing_ratio\": 0.5,\n",
    "        \"vocab_size\": len(vocab)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=\"Training\", leave=False)\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        src = batch['article'].to(device)\n",
    "        src_lens = batch['article_len'].to(device)\n",
    "        trg = batch['summary'].to(device)\n",
    "        \n",
    "        outputs = model(src, src_lens, trg=trg, teacher_forcing_ratio=0.5)\n",
    "        loss = criterion(\n",
    "            outputs[:, 1:].reshape(-1, outputs.size(-1)),\n",
    "            trg[:, 1:].reshape(-1)\n",
    "        )\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "    \n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "# 3. Hàm eval với progress bar\n",
    "def evaluate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(val_loader, desc=\"Evaluating\", leave=False)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in progress_bar:\n",
    "            src = batch['article'].to(device)\n",
    "            src_lens = batch['article_len'].to(device)\n",
    "            trg = batch['summary'].to(device)\n",
    "            \n",
    "            outputs = model(src, src_lens, trg=trg, teacher_forcing_ratio=0)\n",
    "            loss = criterion(\n",
    "                outputs[:, 1:].reshape(-1, outputs.size(-1)),\n",
    "                trg[:, 1:].reshape(-1)\n",
    "            )\n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "    \n",
    "    return total_loss / len(val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cf08ae1ef4e4725b32eef12753514be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/6123 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Tạo thư mục nếu chưa tồn tại\n",
    "model_dir = \"../Model\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Lưu mô hình tốt nhất\n",
    "best_model_path = os.path.join(model_dir, \"best_model.pth\")\n",
    "embedding_layer = torch.nn.Embedding.from_pretrained(\n",
    "    torch.FloatTensor(embeddings),\n",
    "    freeze=False,\n",
    "    padding_idx=stoi(\"<PAD>\")\n",
    ").to(device)\n",
    "\n",
    "# 3. Khởi tạo model\n",
    "model = Seq2SeqModel(\n",
    "    embedding_layer=embedding_layer,\n",
    "    hidden_dim=128,\n",
    "    vocab_size=len(vocab)\n",
    ").to(device)\n",
    "\n",
    "# 4. Train loop\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=stoi(\"<PAD>\"))\n",
    "best_val_loss = float('inf')\n",
    "wandb.watch(model)\n",
    "for epoch in range(3):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Train\n",
    "    train_loss = train_model(model, train_loader, optimizer, criterion, device)\n",
    "    \n",
    "    # Eval\n",
    "    val_loss = evaluate(model, valid_loader, criterion, device)\n",
    "    \n",
    "    # Log metrics\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch+1,\n",
    "        \"train_loss\": train_loss,\n",
    "        \"val_loss\": val_loss,\n",
    "        \"lr\": optimizer.param_groups[0]['lr']\n",
    "    })\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "        }, best_model_path)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1:02d} | \"\n",
    "          f\"Train Loss: {train_loss:.4f} | \"\n",
    "          f\"Val Loss: {val_loss:.4f} | \"\n",
    "          f\"Time: {time.time()-start_time:.2f}s\")\n",
    "\n",
    "# Kết thúc W&B\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
