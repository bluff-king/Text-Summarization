{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "import random \n",
    "from tqdm import tqdm\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "import wandb\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"../dataset/train.csv\")\n",
    "validation_data = pd.read_csv(\"../dataset/validation.csv\")\n",
    "test_data = pd.read_csv(\"../dataset/test.csv\")\n",
    "\n",
    "train_data.rename(columns={\"highlights\": \"summaries\", \"article\":\"articles\"}, inplace=True)\n",
    "validation_data.rename(columns={\"highlights\": \"summaries\",\"article\":\"articles\"}, inplace=True)\n",
    "test_data.rename(columns={\"highlights\": \"summaries\", \"article\":\"articles\"}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"article_word_count\"] = train_data[\"articles\"].astype(str).apply(lambda x: len(x.split()))\n",
    "train_data[\"summary_word_count\"] = train_data[\"summaries\"].astype(str).apply(lambda x: len(x.split()))\n",
    "\n",
    "validation_data[\"article_word_count\"] = validation_data[\"articles\"].astype(str).apply(lambda x: len(x.split()))\n",
    "validation_data[\"summary_word_count\"] = validation_data[\"summaries\"].astype(str).apply(lambda x: len(x.split()))\n",
    "\n",
    "test_data[\"article_word_count\"] = test_data[\"articles\"].astype(str).apply(lambda x: len(x.split()))\n",
    "test_data[\"summary_word_count\"] = test_data[\"summaries\"].astype(str).apply(lambda x: len(x.split()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data[train_data[\"article_word_count\"] < 1022]\n",
    "validation_data = validation_data[validation_data[\"article_word_count\"] < 1022]\n",
    "test_data = test_data[test_data[\"article_word_count\"] < 1022]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2415 entries, 258339 to 71845\n",
      "Data columns (total 5 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   id                  2415 non-null   object\n",
      " 1   articles            2415 non-null   object\n",
      " 2   summaries           2415 non-null   object\n",
      " 3   article_word_count  2415 non-null   int64 \n",
      " 4   summary_word_count  2415 non-null   int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 113.2+ KB\n"
     ]
    }
   ],
   "source": [
    "train_sample = train_data.sample(frac=0.01, random_state=1)\n",
    "validation_sample = validation_data.sample(frac=0.01, random_state=1)\n",
    "test_sample = test_data.sample(frac=0.01, random_state=1)\n",
    "train_sample.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1021\n",
      "159\n"
     ]
    }
   ],
   "source": [
    "max_len_article = train_sample[\"article_word_count\"].max()\n",
    "print(max_len_article)\n",
    "max_len_summary = train_sample[\"summary_word_count\"].max()\n",
    "print(max_len_summary)\n",
    "# if max_len_article is None:\n",
    "#     self.max_len_article = max(len(numericalize(art)) for art in articles) + 2  # +2 cho SOS/EOS\n",
    "# else:\n",
    "#     self.max_len_article = max_len_article\n",
    "    \n",
    "# if max_len_summary is None:\n",
    "#     self.max_len_summary = max(len(numericalize(summ)) for summ in summaries) + 2\n",
    "# else:\n",
    "#     self.max_len_summary = max_len_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25004\n",
      "Embedding shape: (25004, 104)\n",
      "<PAD> embedding last 4 dims: [1.0, 0.0, 0.0, 0.0]\n",
      "<SOS> embedding last 4 dims: [0.0, 1.0, 0.0, 0.0]\n",
      "Word 'the' embedding last 4 dims: [0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_FILE = \"../Embedding/glove-wiki-gigaword-100.txt\"\n",
    "vocab, embeddings = [], []\n",
    "with open(EMBEDDING_FILE, 'rt', encoding='utf-8') as ef:\n",
    "    full_content = ef.read().strip().split('\\n')\n",
    "for i in range(len(full_content)):\n",
    "    i_word = full_content[i].split(' ')[0]\n",
    "    i_embeddings = [float(val) for val in full_content[i].split(' ')[1:]]\n",
    "    i_embeddings.extend([0.0, 0.0, 0.0, 0.0])\n",
    "    vocab.append(i_word)\n",
    "    embeddings.append(i_embeddings)\n",
    "\n",
    "embs_npa = np.array(embeddings)\n",
    "\n",
    "unk_embedding = np.mean(embs_npa, axis=0).tolist()\n",
    "\n",
    "dim = embs_npa.shape[1]\n",
    "sos_embedding = [0.0] * dim\n",
    "sos_embedding[-3] = 1.0\n",
    "eos_embedding = [0.0] * dim\n",
    "eos_embedding[-2] = 1.0\n",
    "pad_embedding = [0.0] * dim\n",
    "pad_embedding[-4] = 1.0\n",
    "# unk_embedding = [0.0] * dim\n",
    "# unk_embedding[-1] = 1.0\n",
    "\n",
    "# Update vocab and embeddings\n",
    "vocab = [\"<PAD>\", \"<SOS>\", \"<EOS>\", \"<UNK>\"] + vocab\n",
    "embeddings = [pad_embedding, sos_embedding,\n",
    "              eos_embedding, unk_embedding] + embeddings\n",
    "\n",
    "vocab_npa = np.array(vocab)\n",
    "embs_npa = np.array(embeddings)\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    return text.lower().strip().split()\n",
    "\n",
    "\n",
    "stoi_dict = {word: idx for idx, word in enumerate(vocab_npa)}\n",
    "_unk_idx = stoi_dict[\"<UNK>\"]\n",
    "\n",
    "\n",
    "def stoi(string, stoi_dict=stoi_dict):\n",
    "    return stoi_dict.get(string, _unk_idx)\n",
    "\n",
    "\n",
    "def numericalize(text):\n",
    "    tokenized_text = tokenize(text)\n",
    "    return [\n",
    "        stoi(token)\n",
    "        for token in tokenized_text\n",
    "    ]\n",
    "\n",
    "print(embs_npa.shape[0])\n",
    "embedding_layer = torch.nn.Embedding.from_pretrained(torch.FloatTensor(embeddings),\n",
    "                                                     freeze=False,\n",
    "                                                     padding_idx=stoi(\"<PAD>\"))\n",
    "embedding_layer.to(device)\n",
    "print(\"Embedding shape:\", np.array(embeddings).shape) \n",
    "print(\"<PAD> embedding last 4 dims:\", embeddings[stoi(\"<PAD>\")][-4:])\n",
    "print(\"<SOS> embedding last 4 dims:\", embeddings[stoi(\"<SOS>\")][-4:])\n",
    "print(\"Word 'the' embedding last 4 dims:\", embeddings[stoi(\"the\")][-4:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad_idx = stoi(\"<PAD>\")\n",
    "# sos_idx = stoi(\"<SOS>\")\n",
    "# eos_idx = stoi(\"<EOS>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqDataset(Dataset):\n",
    "    def __init__(self, articles, summaries, stoi, max_len_article=None, max_len_summary=None):\n",
    "        self.articles = articles  # List of articles\n",
    "        self.summaries = summaries  # List of summaries\n",
    "        self.stoi = stoi  # String-to-index dictionary\n",
    "        self.pad_idx = stoi(\"<PAD>\")\n",
    "        self.sos_idx = stoi(\"<SOS>\")\n",
    "        self.eos_idx = stoi(\"<EOS>\")\n",
    "        \n",
    "        # Determine max lengths if not provided\n",
    "        self.max_len_article = max_len_article or max(len(a.split()) for a in articles) + 2\n",
    "        self.max_len_summary = max_len_summary or max(len(s.split()) for s in summaries) + 2\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.articles)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        def process_text(text, max_len):\n",
    "            tokens = [self.sos_idx] + [self.stoi(w) for w in text.split()] + [self.eos_idx]  # Tokenize and add SOS/EOS\n",
    "            tokens = tokens[:max_len] + [self.pad_idx] * (max_len - len(tokens))  # Pad to max length\n",
    "            return torch.tensor(tokens), len(tokens)\n",
    "\n",
    "        article_tokens, article_len = process_text(self.articles[idx], self.max_len_article)\n",
    "        summary_tokens, summary_len = process_text(self.summaries[idx], self.max_len_summary)\n",
    "        \n",
    "        return {\n",
    "            'article': article_tokens,  # Encoded article\n",
    "            'article_len': torch.tensor(article_len),\n",
    "            'summary': summary_tokens,  # Encoded summary\n",
    "            'summary_len': torch.tensor(summary_len)\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Batch là list các dict {'article': ..., 'summary': ...}\n",
    "    return {\n",
    "        'article': torch.stack([item['article'] for item in batch]),\n",
    "        'article_len': torch.tensor([item['article_len'] for item in batch]),\n",
    "        'summary': torch.stack([item['summary'] for item in batch]),\n",
    "        'summary_len': torch.tensor([item['summary_len'] for item in batch])\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 3, 3,  ..., 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "# DataLoader setup\n",
    "# torch.set_printoptions(profile=\"full\")\n",
    "torch.set_printoptions(profile=\"default\")\n",
    "train_dataset = Seq2SeqDataset(train_sample['articles'].tolist(), train_sample['summaries'].tolist(), stoi)\n",
    "print(train_dataset[268][\"article\"])\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "valid_dataset= Seq2SeqDataset(validation_sample['articles'].tolist(), validation_sample['summaries'].tolist(), stoi)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def collate_fn(batch):\n",
    "#     return {\n",
    "#         'article': torch.stack([item['article'] for item in batch]),\n",
    "#         'article_len': torch.stack([item['article_len'] for item in batch]),\n",
    "#         'summary': torch.stack([item['summary'] for item in batch]),\n",
    "#         'summary_len': torch.stack([item['summary_len'] for item in batch])\n",
    "#     }\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Architecture Seq2Seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleEncoder(nn.Module):\n",
    "    def __init__(self, embedding_layer, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = embedding_layer  # Sử dụng embedding có sẵn\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.embedding.embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            batch_first=True,\n",
    "            bidirectional=False  # Unidirectional\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, seq_lens):  # x = batch input sequences\n",
    "        # Bước 1: Embedding\n",
    "        x = self.embedding(x)  # [batch_size, max_len, emb_dim]\n",
    "        \n",
    "        # Bước 2: Pack để bỏ qua padding tokens\n",
    "        packed = pack_padded_sequence(\n",
    "            input=x,\n",
    "            lengths=seq_lens.cpu(),  # Chuyển sang CPU tensor\n",
    "            batch_first=True,\n",
    "            enforce_sorted=False  # Không cần sắp xếp theo độ dài\n",
    "        )\n",
    "        \n",
    "        # Bước 3: LSTM (chỉ xử lý độ dài thực) \n",
    "        '''\n",
    "        packed_output: dữ liệu thực sự được xử lý\n",
    "        hidden/cell lưu trạng thái cuối cùng của mỗi sequence\n",
    "        Input (padded):       Packed LSTM:         Output (unpacked):\n",
    "        [1,2,3,0,0]    -->   [1,2,3,4,5,6,7,8]       --> [h1_t1,h1_t2,h1_t3,0,0]\n",
    "        [4,5,6,7,8]     hidden_dim, hd,... hd(8cai)  --> [h2_t1,h2_t2,h2_t3,h2_t4,h2_t5]\n",
    "        '''\n",
    "        packed_output, (hidden, cell) = self.lstm(packed)\n",
    "        # Output: (batch_size, seq_len, hidden_dim)\n",
    "        # Bước 4: Unpack nếu cần dùng attention\n",
    "        \n",
    "        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "        \n",
    "        return output, (hidden, cell)  # hidden shape: [1, batch_size, hidden_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        # Kết hợp cả encoder outputs và decoder hidden state\n",
    "        self.energy = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),  # Nhận input là [encoder_output + decoder_hidden]\n",
    "            nn.Tanh(),                              # thêm chút phi tuyến\n",
    "            nn.Linear(hidden_dim, 1, bias=False)    #  Trả về 1 chiều attention scores\n",
    "        )\n",
    "    \n",
    "    def forward(self, decoder_hidden, encoder_outputs, mask=None):\n",
    "        # Bước 1: Chuẩn bị decoder_hidden để cộng với encoder_outputs\n",
    "        # decoder_hidden: [batch_size, hidden_dim]\n",
    "        # encoder_outputs: [batch_size, seq_len, hidden_dim]\n",
    "        # Copy dọc theo seq_len\n",
    "        decoder_hidden = decoder_hidden.unsqueeze(1).expand_as(encoder_outputs)  # [batch_size, seq_len, hidden_dim]\n",
    "        \n",
    "        # Bước 2: Tính energy từ sự kết hợp encoder-decoder\n",
    "        combined = torch.cat([encoder_outputs, decoder_hidden], dim=2)  # [batch_size, seq_len, hidden_dim * 2]\n",
    "        scores = self.energy(combined).squeeze(2)  # [batch_size, seq_len]\n",
    "        \n",
    "        # Bước 3: Áp dụng mask và softmax\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e10)\n",
    "        attn_weights = F.softmax(scores, dim=1)    # [batch_size, seq_len]\n",
    "        \n",
    "        # Bước 4: Tính context vector: # [batch_size,1, hidden_dim] x [batch_size, seq_len, hidden_dim]\n",
    "                #  context = [batch_size, 1, hidden_dim]squeeze(1) loại chiều 1 -> [batch_size, hidden_dim]\n",
    "        context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs).squeeze(1) \n",
    "     \n",
    "        # debug  \n",
    "        '''\n",
    "        Encoder Outputs:  \"<SOS>\"   \"cat\"    \"sat\"   \"<EOS>\"  \"<PAD>\"\n",
    "                        [0.1,0.2] [0.3,0.4] [0.5,0.6] [0.7,0.8] [0.0,0.0]\n",
    "        Attention Weights:  0.12      0.53      0.29      0.06      0.0\n",
    "                        ↓         ↓         ↓         ↓         ↓\n",
    "        Context Vector:  = 0.12*[0.1,0.2] + 0.53*[0.3,0.4] + ... = [0.35, 0.45]\n",
    "        '''\n",
    "        '''\n",
    "        attn_weights (unsqueezed):   encoder_outputs:       context:\n",
    "        [ [ [0.2, 0.5, 0.3] ]    @  [[0.1,0.2],        =  [ [0.32, 0.42] ]\n",
    "        [ [0.1, 0.7, 0.2] ]        [0.3,0.4],            [0.92, 1.02] ]\n",
    "                                    [0.5,0.6] ]\n",
    "        '''\n",
    "        # print(\"Decoder hidden:\", decoder_hidden.shape)\n",
    "        # print(\"Encoder outputs:\", encoder_outputs.shape)\n",
    "        # context, attn_weights = self.attention(decoder_hidden, encoder_outputs)\n",
    "        # print(\"Attention weights:\", attn_weights)  # Xem model đang tập trung vào đâu\n",
    "        return context, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDecoder(nn.Module):\n",
    "    def __init__(self, embedding_layer, hidden_dim, vocab_size):\n",
    "        super().__init__()\n",
    "        self.embedding = embedding_layer\n",
    "        self.lstm = nn.LSTMCell(\n",
    "            input_size=self.embedding.embedding_dim + hidden_dim,  # Thay đổi 1: Thêm context_dim\n",
    "            hidden_size=hidden_dim\n",
    "        )\n",
    "        self.attention = SimpleAttention(hidden_dim)\n",
    "        self.fc_out = nn.Linear(hidden_dim * 2, vocab_size)  # Thay đổi 2: Kết hợp hidden + context\n",
    "\n",
    "    def forward(self, x, prev_hidden, prev_cell, encoder_outputs, mask=None):\n",
    "        # Bước 1: Embedding\n",
    "        x = self.embedding(x)  # [batch_size] -> [batch_size, emb_dim]\n",
    "        \n",
    "        # Bước 2: Tính context vector từ Attention\n",
    "        context, attn_weights = self.attention(prev_hidden, encoder_outputs, mask)  # Sửa: dùng prev_hidden thay vì hidden\n",
    "        \n",
    "        # Bước 3: Kết hợp embedding và context làm input cho LSTM\n",
    "        # cat batch_size, emb_dim], [batch_size, hidden_dim]\n",
    "        lstm_input = torch.cat([x, context], dim=1)  # [batch_size, emb_dim + hidden_dim]\n",
    "        \n",
    "        # Bước 4: LSTM step\n",
    "        hidden, cell = self.lstm(lstm_input, (prev_hidden, prev_cell))\n",
    "        \n",
    "        # Bước 5: Kết hợp hidden và context để dự đoán từ\n",
    "        output_input = torch.cat([hidden, context], dim=1)  # [batch_size, hidden_dim * 2]\n",
    "        output = self.fc_out(output_input)  # [batch_size, vocab_size]\n",
    "        \n",
    "        return output, hidden, cell, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqModel(nn.Module):\n",
    "    def __init__(self, embedding_layer, hidden_dim, vocab_size):\n",
    "        super().__init__()\n",
    "        self.encoder = SimpleEncoder(embedding_layer, hidden_dim)\n",
    "        self.decoder = SimpleDecoder(embedding_layer, hidden_dim, vocab_size)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.start_id = stoi(\"<SOS>\")  # Thêm start token ID\n",
    "\n",
    "    def forward(self, src, src_lens, trg=None, max_len=256, teacher_forcing_ratio=0.5):\n",
    "        # Encoder forward\n",
    "        enc_outputs, (hidden, cell) = self.encoder(src, src_lens)\n",
    "        \n",
    "        # Chuẩn bị decoder\n",
    "        batch_size = src.size(0)\n",
    "        if trg is None:  # Inference mode\n",
    "            max_len = max_len\n",
    "            trg = torch.full((batch_size,), self.start_id, dtype=torch.long, device=src.device)\n",
    "        else:  # Training mode\n",
    "            max_len = trg.size(1)\n",
    "        \n",
    "        # Tensor lưu outputs\n",
    "        outputs = torch.zeros(batch_size, max_len, self.vocab_size).to(src.device)\n",
    "        \n",
    "        # Khởi tạo input đầu tiên\n",
    "        x = torch.full((batch_size,), self.start_id, dtype=torch.long, device=src.device)  # Đổi tên x_t -> x\n",
    "        \n",
    "        for t in range(max_len):\n",
    "            output, hidden, cell, _ = self.decoder(\n",
    "                x=x,  # Đổi thành x thay vì x_t\n",
    "                prev_hidden=hidden.squeeze(0),\n",
    "                prev_cell=cell.squeeze(0),\n",
    "                encoder_outputs=enc_outputs\n",
    "            )\n",
    "            outputs[:, t] = output\n",
    "            \n",
    "            # Teacher forcing\n",
    "            if trg is not None and random.random() < teacher_forcing_ratio:\n",
    "                x = trg[:, t]  # Ground truth\n",
    "            else:\n",
    "                x = output.argmax(1)  # Prediction\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory allocated: 9.92 MB\n",
      "GPU Memory reserved: 20.00 MB\n"
     ]
    }
   ],
   "source": [
    "print(f\"GPU Memory allocated: {torch.cuda.memory_allocated()/1024**2:.2f} MB\")\n",
    "print(f\"GPU Memory reserved: {torch.cuda.memory_reserved()/1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 5.7680\n",
      "Epoch 2, Loss: 5.2853\n",
      "Epoch 3, Loss: 5.1788\n",
      "Epoch 4, Loss: 5.0784\n",
      "Epoch 5, Loss: 4.9679\n",
      "Epoch 6, Loss: 4.8746\n",
      "Epoch 7, Loss: 4.7827\n",
      "Epoch 8, Loss: 4.6977\n",
      "Epoch 9, Loss: 4.6204\n",
      "Epoch 10, Loss: 4.5356\n"
     ]
    }
   ],
   "source": [
    "# # 2. Tạo embedding layer (như bạn đã làm)\n",
    "# embedding_layer = torch.nn.Embedding.from_pretrained(\n",
    "#     torch.FloatTensor(embeddings),\n",
    "#     freeze=False,\n",
    "#     padding_idx=stoi(\"<PAD>\")\n",
    "# ).to(device)\n",
    "\n",
    "# # 3. Khởi tạo model\n",
    "# model = Seq2SeqModel(\n",
    "#     embedding_layer=embedding_layer,\n",
    "#     hidden_dim=128,\n",
    "#     vocab_size=len(vocab)\n",
    "# ).to(device)\n",
    "\n",
    "# # 4. Train loop\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "# criterion = nn.CrossEntropyLoss(ignore_index=stoi(\"<PAD>\"))\n",
    "\n",
    "# for epoch in range(1):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "    \n",
    "#     for batch in train_loader:\n",
    "#         src = batch['article'].to(device)\n",
    "#         src_lens = batch['article_len'].to(device)\n",
    "#         trg = batch['summary'].to(device)\n",
    "        \n",
    "#         outputs = model(src, src_lens, trg=trg, teacher_forcing_ratio=0.5)\n",
    "#         loss = criterion(outputs[:, 1:].reshape(-1, outputs.size(-1)), \n",
    "#                          trg[:, 1:].reshape(-1))\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         total_loss += loss.item()\n",
    "    \n",
    "#     print(f\"Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: torch.Size([25004, 104])\n",
      "Vocab size: 25004\n",
      "Model architecture:\n",
      "Seq2SeqModel(\n",
      "  (encoder): SimpleEncoder(\n",
      "    (embedding): Embedding(25004, 104, padding_idx=0)\n",
      "    (lstm): LSTM(104, 128, batch_first=True)\n",
      "  )\n",
      "  (decoder): SimpleDecoder(\n",
      "    (embedding): Embedding(25004, 104, padding_idx=0)\n",
      "    (lstm): LSTMCell(232, 128)\n",
      "    (attention): SimpleAttention(\n",
      "      (energy): Sequential(\n",
      "        (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Linear(in_features=128, out_features=1, bias=False)\n",
      "      )\n",
      "    )\n",
      "    (fc_out): Linear(in_features=256, out_features=25004, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "embedding_layer = torch.nn.Embedding.from_pretrained(\n",
    "    torch.FloatTensor(embeddings),\n",
    "    freeze=False,  # Cho phép fine-tune embedding\n",
    "    padding_idx=stoi(\"<PAD>\")  # Index của token padding\n",
    ").to(device)\n",
    "\n",
    "# Khởi tạo\n",
    "hidden_dim = 128\n",
    "vocab_size = len(vocab)\n",
    "model = Seq2SeqModel(\n",
    "    embedding_layer=embedding_layer,\n",
    "    hidden_dim=hidden_dim,\n",
    "    vocab_size=vocab_size\n",
    ").to(device)\n",
    "print(\"Embedding shape:\", torch.FloatTensor(embeddings).shape)\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "print(\"Model architecture:\")\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvubkk67\u001b[0m (\u001b[33mvubkk67-hanoi-university-of-science-and-technology\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/vuda/Text-Summarization/Abstractive/wandb/run-20250403_054324-hsv4xlqe</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vubkk67-hanoi-university-of-science-and-technology/Seq2Seq-Summarization/runs/hsv4xlqe' target=\"_blank\">seq2seq-20250403-054323</a></strong> to <a href='https://wandb.ai/vubkk67-hanoi-university-of-science-and-technology/Seq2Seq-Summarization' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vubkk67-hanoi-university-of-science-and-technology/Seq2Seq-Summarization' target=\"_blank\">https://wandb.ai/vubkk67-hanoi-university-of-science-and-technology/Seq2Seq-Summarization</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vubkk67-hanoi-university-of-science-and-technology/Seq2Seq-Summarization/runs/hsv4xlqe' target=\"_blank\">https://wandb.ai/vubkk67-hanoi-university-of-science-and-technology/Seq2Seq-Summarization/runs/hsv4xlqe</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/vubkk67-hanoi-university-of-science-and-technology/Seq2Seq-Summarization/runs/hsv4xlqe?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f4af9ab79b0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Khởi tạo W&B\n",
    "# wandb.init(\n",
    "#     project=\"Seq2Seq-Summarization\",\n",
    "#     name=f\"seq2seq-{datetime.now().strftime('%Y%m%d-%H%M%S')}\",\n",
    "#     config={\n",
    "#         \"model\": \"Seq2Seq-LSTM\",\n",
    "#         \"hidden_dim\": 128,\n",
    "#         \"batch_size\": 8,\n",
    "#         \"learning_rate\": 0.001,\n",
    "#         \"teacher_forcing_ratio\": 0.5,\n",
    "#         \"vocab_size\": len(vocab)\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=\"Training\", leave=False)\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        src = batch['article'].to(device)\n",
    "        src_lens = batch['article_len'].to(device)\n",
    "        trg = batch['summary'].to(device)\n",
    "        \n",
    "        outputs = model(src, src_lens, trg=trg, teacher_forcing_ratio=0.5)\n",
    "        loss = criterion(\n",
    "            outputs[:, 1:].reshape(-1, outputs.size(-1)),\n",
    "            trg[:, 1:].reshape(-1)\n",
    "        )\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "    \n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "# 3. Hàm eval với progress bar\n",
    "def evaluate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(val_loader, desc=\"Evaluating\", leave=False)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in progress_bar:\n",
    "            src = batch['article'].to(device)\n",
    "            src_lens = batch['article_len'].to(device)\n",
    "            trg = batch['summary'].to(device)\n",
    "            \n",
    "            outputs = model(src, src_lens, trg=trg, teacher_forcing_ratio=0)\n",
    "            loss = criterion(\n",
    "                outputs[:, 1:].reshape(-1, outputs.size(-1)),\n",
    "                trg[:, 1:].reshape(-1)\n",
    "            )\n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "    \n",
    "    return total_loss / len(val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de2b48bb53a24437ac88ae76c1c29adb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/302 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6e6a3de047a44a0a8485ddebe4bfa65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (974) must match the existing size (128) at non-singleton dimension 1.  Target sizes: [1, 974, 128].  Tensor sizes: [128, 1]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m train_model(model, train_loader, optimizer, criterion, device)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Eval\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Log metrics\u001b[39;00m\n\u001b[1;32m     28\u001b[0m wandb\u001b[38;5;241m.\u001b[39mlog({\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m: epoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: train_loss,\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: val_loss,\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m: optimizer\u001b[38;5;241m.\u001b[39mparam_groups[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     33\u001b[0m })\n",
      "Cell \u001b[0;32mIn[24], line 39\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(model, val_loader, criterion, device)\u001b[0m\n\u001b[1;32m     36\u001b[0m src_lens \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marticle_len\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     37\u001b[0m trg \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msummary\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 39\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_lens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mteacher_forcing_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(\n\u001b[1;32m     41\u001b[0m     outputs[:, \u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, outputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)),\n\u001b[1;32m     42\u001b[0m     trg[:, \u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     43\u001b[0m )\n\u001b[1;32m     44\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[15], line 28\u001b[0m, in \u001b[0;36mSeq2SeqModel.forward\u001b[0;34m(self, src, src_lens, trg, max_len, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m     25\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfull((batch_size,), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_id, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39msrc\u001b[38;5;241m.\u001b[39mdevice)  \u001b[38;5;66;03m# Đổi tên x_t -> x\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_len):\n\u001b[0;32m---> 28\u001b[0m     output, hidden, cell, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Đổi thành x thay vì x_t\u001b[39;49;00m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprev_hidden\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprev_cell\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcell\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menc_outputs\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     outputs[:, t] \u001b[38;5;241m=\u001b[39m output\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m# Teacher forcing\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[14], line 17\u001b[0m, in \u001b[0;36mSimpleDecoder.forward\u001b[0;34m(self, x, prev_hidden, prev_cell, encoder_outputs, mask)\u001b[0m\n\u001b[1;32m     14\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(x)  \u001b[38;5;66;03m# [batch_size] -> [batch_size, emb_dim]\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Bước 2: Tính context vector từ Attention\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m context, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprev_hidden\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Sửa: dùng prev_hidden thay vì hidden\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Bước 3: Kết hợp embedding và context làm input cho LSTM\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# cat batch_size, emb_dim], [batch_size, hidden_dim]\u001b[39;00m\n\u001b[1;32m     21\u001b[0m lstm_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x, context], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# [batch_size, emb_dim + hidden_dim]\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[13], line 16\u001b[0m, in \u001b[0;36mSimpleAttention.forward\u001b[0;34m(self, decoder_hidden, encoder_outputs, mask)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, decoder_hidden, encoder_outputs, mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Bước 1: Chuẩn bị decoder_hidden để cộng với encoder_outputs\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# decoder_hidden: [batch_size, hidden_dim]\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# encoder_outputs: [batch_size, seq_len, hidden_dim]\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# Copy dọc theo seq_len\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m     decoder_hidden \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_hidden\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand_as\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch_size, seq_len, hidden_dim]\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# Bước 2: Tính energy từ sự kết hợp encoder-decoder\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     combined \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([encoder_outputs, decoder_hidden], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# [batch_size, seq_len, hidden_dim * 2]\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (974) must match the existing size (128) at non-singleton dimension 1.  Target sizes: [1, 974, 128].  Tensor sizes: [128, 1]"
     ]
    }
   ],
   "source": [
    "embedding_layer = torch.nn.Embedding.from_pretrained(\n",
    "    torch.FloatTensor(embeddings),\n",
    "    freeze=False,\n",
    "    padding_idx=stoi(\"<PAD>\")\n",
    ").to(device)\n",
    "\n",
    "# 3. Khởi tạo model\n",
    "model = Seq2SeqModel(\n",
    "    embedding_layer=embedding_layer,\n",
    "    hidden_dim=128,\n",
    "    vocab_size=len(vocab)\n",
    ").to(device)\n",
    "\n",
    "# 4. Train loop\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=stoi(\"<PAD>\"))\n",
    "best_val_loss = float('inf')\n",
    "wandb.watch(model)\n",
    "for epoch in range(3):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Train\n",
    "    train_loss = train_model(model, train_loader, optimizer, criterion, device)\n",
    "    \n",
    "    # Eval\n",
    "    val_loss = evaluate(model, valid_loader, criterion, device)\n",
    "    \n",
    "    # Log metrics\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch+1,\n",
    "        \"train_loss\": train_loss,\n",
    "        \"val_loss\": val_loss,\n",
    "        \"lr\": optimizer.param_groups[0]['lr']\n",
    "    })\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "        }, 'best_model.pth')\n",
    "    \n",
    "    print(f\"Epoch {epoch+1:02d} | \"\n",
    "          f\"Train Loss: {train_loss:.4f} | \"\n",
    "          f\"Val Loss: {val_loss:.4f} | \"\n",
    "          f\"Time: {time.time()-start_time:.2f}s\")\n",
    "\n",
    "# Kết thúc W&B\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
