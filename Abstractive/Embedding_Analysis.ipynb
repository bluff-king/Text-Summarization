{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/vuda/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "import random \n",
    "from tqdm import tqdm\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "import wandb\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 30\n",
    "BATCH_SIZE = 4\n",
    "FRAC_SAMPLE = 0.2\n",
    "MAX_LENGTH_ARTICLE = 512\n",
    "MIN_LENGTH_ARTICLE = 50\n",
    "MAX_LENGTH_SUMMARY = 128\n",
    "MIN_LENGTH_SUMMARY = 20\n",
    "HIDDEN_DIM = 128\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_CYCLES = 3\n",
    "MAX_PLATEAU_COUNT = 5\n",
    "WEIGHT_DECAY = 1e-4\n",
    "CLIP = 1\n",
    "USE_PRETRAINED_EMB = True\n",
    "USE_SCHEDULER = True\n",
    "SCHEDULER_TYPE = \"plateau\"  # hoặc cosine, linear\n",
    "TEACHER_FORCING_RATIO = 0.75\n",
    "NUM_CYCLES = 3\n",
    "MAX_PLATEAU_COUNT = 5\n",
    "\n",
    "\n",
    "model_dir = \"../Model\"\n",
    "datafilter = \"../dataft\"\n",
    "os.makedirs(datafilter, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 19198 entries, 144417 to 201560\n",
      "Data columns (total 5 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   id                  19198 non-null  object\n",
      " 1   articles            19198 non-null  object\n",
      " 2   summaries           19198 non-null  object\n",
      " 3   article_word_count  19198 non-null  int64 \n",
      " 4   summary_word_count  19198 non-null  int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 899.9+ KB\n",
      "\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 994 entries, 8901 to 8365\n",
      "Data columns (total 5 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   id                  994 non-null    object\n",
      " 1   articles            994 non-null    object\n",
      " 2   summaries           994 non-null    object\n",
      " 3   article_word_count  994 non-null    int64 \n",
      " 4   summary_word_count  994 non-null    int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 46.6+ KB\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv(\"../dataset/train.csv\")\n",
    "validation_data = pd.read_csv(\"../dataset/validation.csv\")\n",
    "test_data = pd.read_csv(\"../dataset/test.csv\")\n",
    "\n",
    "# add col\n",
    "train_data.rename(columns={\"highlights\": \"summaries\", \"article\":\"articles\"}, inplace=True)\n",
    "validation_data.rename(columns={\"highlights\": \"summaries\",\"article\":\"articles\"}, inplace=True)\n",
    "test_data.rename(columns={\"highlights\": \"summaries\", \"article\":\"articles\"}, inplace=True)\n",
    "\n",
    "train_data[\"article_word_count\"] = train_data[\"articles\"].astype(str).apply(lambda x: len(x.split()))\n",
    "train_data[\"summary_word_count\"] = train_data[\"summaries\"].astype(str).apply(lambda x: len(x.split()))\n",
    "\n",
    "validation_data[\"article_word_count\"] = validation_data[\"articles\"].astype(str).apply(lambda x: len(x.split()))\n",
    "validation_data[\"summary_word_count\"] = validation_data[\"summaries\"].astype(str).apply(lambda x: len(x.split()))\n",
    "\n",
    "test_data[\"article_word_count\"] = test_data[\"articles\"].astype(str).apply(lambda x: len(x.split()))\n",
    "test_data[\"summary_word_count\"] = test_data[\"summaries\"].astype(str).apply(lambda x: len(x.split()))\n",
    "\n",
    "# filter range\n",
    "train_data = train_data[\n",
    "    (train_data[\"article_word_count\"] <= MAX_LENGTH_ARTICLE) & \n",
    "    (train_data[\"article_word_count\"] >= MIN_LENGTH_ARTICLE) &\n",
    "    (train_data[\"summary_word_count\"] <= MAX_LENGTH_SUMMARY) &\n",
    "    (train_data[\"summary_word_count\"] >= MIN_LENGTH_SUMMARY)\n",
    "]\n",
    "\n",
    "validation_data = validation_data[\n",
    "    (validation_data[\"article_word_count\"] <= MAX_LENGTH_ARTICLE) & \n",
    "    (validation_data[\"article_word_count\"] >= MIN_LENGTH_ARTICLE) &\n",
    "    (validation_data[\"summary_word_count\"] <= MAX_LENGTH_SUMMARY) &\n",
    "    (validation_data[\"summary_word_count\"] >= MIN_LENGTH_SUMMARY)\n",
    "]\n",
    "test_data = test_data[\n",
    "    (test_data[\"article_word_count\"] <= MAX_LENGTH_ARTICLE) & \n",
    "    (test_data[\"article_word_count\"] >= MIN_LENGTH_ARTICLE) &\n",
    "    (test_data[\"summary_word_count\"] <= MAX_LENGTH_SUMMARY) &\n",
    "    (test_data[\"summary_word_count\"] >= MIN_LENGTH_SUMMARY)\n",
    "]\n",
    "\n",
    "train_sample = train_data.sample(frac=FRAC_SAMPLE, random_state=1)\n",
    "validation_sample = validation_data.sample(frac=FRAC_SAMPLE, random_state=1)\n",
    "test_sample = test_data.sample(frac=1, random_state=1)\n",
    "train_sample.info()\n",
    "print(\"\\n\")\n",
    "validation_sample.info()\n",
    "train_sample.to_csv(os.path.join(datafilter,\"train_sample.csv\"), index=False)\n",
    "test_sample.to_csv(os.path.join(datafilter,\"test_sample.csv\"), index=False)\n",
    "validation_sample.to_csv(os.path.join(datafilter,\"validation_sample.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tokenize(\"A dog. in a 'tree with 5.3% rate drop\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19198 entries, 0 to 19197\n",
      "Data columns (total 5 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   id                  19198 non-null  object\n",
      " 1   articles            19198 non-null  object\n",
      " 2   summaries           19198 non-null  object\n",
      " 3   article_word_count  19198 non-null  int64 \n",
      " 4   summary_word_count  19198 non-null  int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 750.1+ KB\n"
     ]
    }
   ],
   "source": [
    "train_sample = pd.read_csv(\"../dataft/train_sample.csv\")\n",
    "validation_sample = pd.read_csv(\"../dataft/validation_sample.csv\")\n",
    "test_sample = pd.read_csv(\"../dataft/test_sample.csv\")\n",
    "train_sample.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400004\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(400004, 54, padding_idx=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOP_K = 400500\n",
    "EMBEDDING_FILE = \"../Embedding/glove.6B.50d.txt\"\n",
    "# EMBEDDING_FILE = \"../Embedding/glove-wiki-gigaword-100.txt\"\n",
    "\n",
    "vocab, embeddings = [], []\n",
    "\n",
    "with open(EMBEDDING_FILE, 'rt', encoding='utf-8') as ef:\n",
    "    for i, line in enumerate(ef):\n",
    "        if i >= TOP_K:\n",
    "            break\n",
    "        split_line = line.strip().split(' ')\n",
    "        i_word = split_line[0]\n",
    "        i_embeddings = [float(val) for val in split_line[1:]]\n",
    "        i_embeddings.extend([0.0, 0.0, 0.0, 0.0])  # để dành cho token đặc biệt\n",
    "        vocab.append(i_word)\n",
    "        embeddings.append(i_embeddings)\n",
    "\n",
    "\n",
    "embs_npa = np.array(embeddings)\n",
    "\n",
    "unk_embedding = np.mean(embs_npa, axis=0).tolist()\n",
    "\n",
    "dim = embs_npa.shape[1]\n",
    "sos_embedding = [0.0] * dim\n",
    "sos_embedding[-3] = 1.0\n",
    "eos_embedding = [0.0] * dim\n",
    "eos_embedding[-2] = 1.0\n",
    "pad_embedding = [0.0] * dim\n",
    "pad_embedding[-4] = 1.0\n",
    "# unk_embedding = [0.0] * dim\n",
    "# unk_embedding[-1] = 1.0\n",
    "\n",
    "# Update vocab and embeddings\n",
    "vocab = [\"<PAD>\", \"<SOS>\", \"<EOS>\", \"<UNK>\"] + vocab\n",
    "embeddings = [pad_embedding, sos_embedding,\n",
    "              eos_embedding, unk_embedding] + embeddings\n",
    "\n",
    "vocab_npa = np.array(vocab)\n",
    "embs_npa = np.array(embeddings)\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    return word_tokenize(text)\n",
    "    return (text.split())\n",
    "\n",
    "def preclean_text(text):\n",
    "    text = re.sub(r\"\\s'([a-zA-Z])\", r\" '\\1\", text)\n",
    "\n",
    "    return word_tokenize(text)\n",
    "    # return text\n",
    "\n",
    "\n",
    "stoi_dict = {word: idx for idx, word in enumerate(vocab_npa)}\n",
    "_unk_idx = stoi_dict[\"<UNK>\"]\n",
    "itos_dict = {idx: word for idx, word in enumerate(vocab_npa)}\n",
    "\n",
    "def stoi(string, stoi_dict=stoi_dict):\n",
    "    return stoi_dict.get(string, _unk_idx)\n",
    "\n",
    "def itos(idx, itos_dict=itos_dict):\n",
    "    return itos_dict.get(idx)\n",
    "\n",
    "def revert_to_text(lst):\n",
    "    if hasattr(lst, 'tolist'):  # works for both torch.Tensor and np.ndarray\n",
    "        lst = lst.tolist()\n",
    "    return [str(itos(int(token))) for token in lst] \n",
    "\n",
    "\n",
    "def numericalize(text):\n",
    "    tokenized_text = tokenize(text)\n",
    "    return [\n",
    "        stoi(token)\n",
    "        for token in tokenized_text\n",
    "    ]\n",
    "\n",
    "print(embs_npa.shape[0])\n",
    "embedding_layer = torch.nn.Embedding.from_pretrained(torch.FloatTensor(embeddings),\n",
    "                                                     freeze=False,\n",
    "                                                     padding_idx=stoi(\"<PAD>\"))\n",
    "embedding_layer.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: (400004, 54)\n",
      "<PAD> embedding last 4 dims: [1.0, 0.0, 0.0, 0.0]\n",
      "<SOS> embedding last 4 dims: [0.0, 1.0, 0.0, 0.0]\n",
      "Word 'the' embedding last 4 dims: [-0.12920060864176852, -0.28866239452097603, -0.012248941299726332, -0.056766888363689434, -0.202111085482792, -0.08389026443356357, 0.3335973726965789, 0.1604514588494143, 0.03867495354970917, 0.17833092082704793, 0.0469662038533105, -0.0028577895152307304, 0.29099850796744287, 0.046137231761455566, -0.20923841559858444, -0.06613100298669164, -0.06822448421043388, 0.07665884568148376, 0.31339918388268906, 0.17848512473276362, -0.12257719082558292, -0.09916927562478682, -0.07495972834085389, 0.06413205706058327, 0.1444125551281154, 0.6089459982604638, 0.17463101054296204, 0.05335403311654184, -0.012738255533159106, 0.034741076886942744, -0.8123956655755472, -0.04688727359339901, 0.2019353311723676, 0.20311115159355098, -0.03935654449686459, 0.06967517803561558, -0.015536553796198381, -0.034052746766077585, -0.0652802475349671, 0.12250091921016126, 0.13992004933389163, -0.1744630454565621, -0.08011841031916592, 0.08495219260330641, -0.010416448746240264, -0.13704901119491045, 0.20127087898234736, 0.10069294277050474, 0.006530070028596603, 0.01685149976465394, 0.0, 0.0, 0.0, 0.0]\n",
      "['<PAD>', '<SOS>', '<EOS>', '<UNK>']\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab_npa)\n",
    "print(\"Embedding shape:\", np.array(embeddings).shape) \n",
    "print(\"<PAD> embedding last 4 dims:\", embeddings[stoi(\"<PAD>\")][-4:])\n",
    "print(\"<SOS> embedding last 4 dims:\", embeddings[stoi(\"<SOS>\")][-4:])\n",
    "print(\"Word 'the' embedding last 4 dims:\", embeddings[stoi(\"5.3%\")])\n",
    "print(revert_to_text(torch.tensor([0, 1, 2, 3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A word not in dict:  people.This\n",
      "\n",
      "Train Vocabulary Coverage:\n",
      "- Unique words: 169676\n",
      "- Exist in dict: 52627\n",
      "- Outside the dict: 117049\n",
      "- Coverage rate: 31.02%\n",
      "A word not in dict:  Edie\n",
      "\n",
      "Validation Vocabulary Coverage:\n",
      "- Unique words: 29321\n",
      "- Exist in dict: 15786\n",
      "- Outside the dict: 13535\n",
      "- Coverage rate: 53.84%\n",
      "A word not in dict:  Rawson.Siem\n",
      "\n",
      "Test Vocabulary Coverage:\n",
      "- Unique words: 65420\n",
      "- Exist in dict: 28522\n",
      "- Outside the dict: 36898\n",
      "- Coverage rate: 43.60%\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def analyze_vocab_coverage(sample_data, stoi_dict):\n",
    "    # Đếm tần suất từ duy nhất\n",
    "    word_freq = defaultdict(int)\n",
    "\n",
    "    for text in sample_data['articles'] + sample_data['summaries']:\n",
    "        tokens = tokenize(text)\n",
    "        for token in tokens:\n",
    "            word_freq[token] += 1\n",
    "\n",
    "    # Phân loại từ vào known / unknown\n",
    "    known_words = set()\n",
    "    unknown_words = set()\n",
    "\n",
    "    for word in word_freq:\n",
    "        if word in stoi_dict:\n",
    "            known_words.add(word)\n",
    "        else:\n",
    "            unknown_words.add(word)\n",
    "\n",
    "    total_unique_words = len(known_words) + len(unknown_words)\n",
    "    coverage = len(known_words) / total_unique_words * 100 if total_unique_words > 0 else 0.0\n",
    "    print(\"A word not in dict: \", random.choice(list(unknown_words)))\n",
    "    return {\n",
    "        'total_unique_words': total_unique_words,\n",
    "        'known_unique_words': len(known_words),\n",
    "        'unknown_unique_words': len(unknown_words),\n",
    "        'coverage_percentage': coverage,\n",
    "    }\n",
    "def print_vocab_stats(name, stats):\n",
    "    print(f\"\\n{name} Vocabulary Coverage:\")\n",
    "    print(f\"- Unique words: {stats['total_unique_words']}\")\n",
    "    print(f\"- Exist in dict: {stats['known_unique_words']}\")\n",
    "    print(f\"- Outside the dict: {stats['unknown_unique_words']}\")\n",
    "    print(f\"- Coverage rate: {stats['coverage_percentage']:.2f}%\")\n",
    "\n",
    "print_vocab_stats(\"Train\", analyze_vocab_coverage(train_sample, stoi_dict))\n",
    "print_vocab_stats(\"Validation\", analyze_vocab_coverage(validation_sample, stoi_dict))\n",
    "print_vocab_stats(\"Test\", analyze_vocab_coverage(test_sample, stoi_dict))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
